{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZQ3yrZuZsXx"
      },
      "source": [
        "# Initialization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yt-OTzpxBk3q"
      },
      "source": [
        "### Installation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-v_0rzg3CG5"
      },
      "source": [
        "Version used for run notebook\n",
        "\n",
        "* gdown: 5.1.0\n",
        "* tqdm: 4.66.4\n",
        "* torchmetrics: 1.4.0.post0\n",
        "* scikit-learn: Version: 1.2.2*\n",
        "* pykan: 0.1.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "pqZNQdxgZe3v"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install gdown\n",
        "!pip install tqdm\n",
        "!pip install torchvision --quiet\n",
        "!pip install pykan==0.1.1\n",
        "!pip install torchmetrics\n",
        "!pip install scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecVBSRTQBmdb"
      },
      "source": [
        "### Import of libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ODbC4xF6Zym4"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import ToPILImage\n",
        "from torchvision.io import read_image\n",
        "from torch.utils.data import random_split, DataLoader, Dataset\n",
        "\n",
        "from torchmetrics.classification import BinaryAccuracy\n",
        "from torchmetrics.classification import BinaryPrecision\n",
        "from torchmetrics.classification import BinaryRecall\n",
        "from torchmetrics.classification import BinaryAUROC\n",
        "from torchmetrics.classification import BinaryF1Score\n",
        "\n",
        "from kan import MultKAN as KAN\n",
        "\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "import os\n",
        "import gdown\n",
        "import zipfile\n",
        "import shutil\n",
        "import gc\n",
        "from tqdm.notebook import tqdm\n",
        "import time\n",
        "from google.colab import drive\n",
        "import json\n",
        "\n",
        "import random\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86RET5RaaNKi"
      },
      "source": [
        "### GLOBAL DEFINITIONS\n",
        "\n",
        "Global variables where there are stored hyperparameters for the training and link, paths for the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "YswtWHEPaLP7"
      },
      "outputs": [],
      "source": [
        "global_var = {\n",
        "    # Dataset\n",
        "    'link_download_prefix' : 'https://drive.google.com/uc?export=download&id=YOUR_FILE_ID', # Generic link to download from Drive the file with id \"YOUR_FILE_ID\"\n",
        "    \"link_dataset\": \"https://drive.google.com/file/d/10un_W7teXQy9LOB1uqd0e6VJ46UFNRza/view?usp=drive_link0\", # The ID is \"10un_W7teXQy9LOB1uqd0e6VJ46UFNRza\" of the file\n",
        "    \"path_download_dataset\": \"/content/dataset.zip\", # Path of the runtime directory of Colab with the zipped dataset\n",
        "    \"extract_directory\": \"/content\", # Directory where we want to extract the zipped dataset\n",
        "    \"dataset_name\": \"chest_xray\", # Name of the dataset\n",
        "    \"dataset_name_resized\": \"chest_xray_resized\", # Name of the dataset resized\n",
        "    \"dataset_name_resized_augmented\": \"chest_xray_resized_augmented\", # Name of the dataset resized\n",
        "    \"dataset_name_gaussian\": \"chest_xray_gaussian\",\n",
        "    \"dataset_name_gaussian_he\": \"chest_xray_gaussian_he\",\n",
        "    \"dataset_name_gaussian_clahe\": \"chest_xray_gaussian_clahe\",\n",
        "    \"sub_folders_ttv\": ['train', 'test', 'val'], # at least must exist one between {'train', 'test', 'val'}\n",
        "\n",
        "    # Preprocessing parameters\n",
        "    \"resizing_dim\": (256,256),\n",
        "    \"smoothing_value\": 3,\n",
        "    \"augment_performed\": False,  # if want to perform augmentation\n",
        "    \"augmentation_train_dimension\": 1125,\n",
        "    \"augmentation_test_dimension\": 156,\n",
        "\n",
        "\n",
        "    # Subset parameters\n",
        "    \"use_subset\": True, # if want to train on a subset\n",
        "    \"subset_suffix\" : \"_subset\",\n",
        "    \"split_value\": [0.7, 0.2, 0.1], # train, test, val splitting values\n",
        "    \"subset_dim_percentage\": 0.1, # from 0.1 to 1\n",
        "\n",
        "    # Train\n",
        "    'batch_size': 48,\n",
        "    \"train_epochs\": 20,\n",
        "    'early_stop_threshold': 1e-4,\n",
        "    'early_stop_patience': 2,\n",
        "\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDA_BRDlugNi"
      },
      "source": [
        "###REPRODUCIBILITY\n",
        "\n",
        "This part sets the seed for various random number generators of torch, numpy and random libraries, to ensure that the results are the same each time the code is run.\n",
        "\n",
        "CuBLAS is a GPU-accelerated library for dense linear algebra operations, such as matrix multiplications, that is part of the CUDA toolkit provided by NVIDIA: we have set the environment variable of the workspace of CuBLAS to \"_:4096:8_\" to ensure deterministic behaviour each run.\n",
        "\n",
        "1. **4096** => Size in byte of the workspace used by CuBLAS for the operation\n",
        "2. **8** => Parameter that ensure to use deterministic algorithm\n",
        "\n",
        "To ensure reproducibility we have also set torch to use deterministic algorithm\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "4IUB3lR18JwU"
      },
      "outputs": [],
      "source": [
        "seed = 0\n",
        "\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "\n",
        "# Set the environment variable for deterministic CuBLAS operations\n",
        "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
        "\n",
        "# Enable deterministic algorithms in PyTorch\n",
        "torch.use_deterministic_algorithms(False)\n",
        "# torch.backends.cudnn.benchmark = False # maybe cancell warning: UserWarning: Plan failed with a cudnnException, not tested\n",
        "\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztmhJrCtndIN"
      },
      "source": [
        "### Color Class\n",
        "This class is used for printing text in different color. In particular each attribute in the Color class corresponds to a specific text color or formatting style."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "-Q5lxLgy3E-z"
      },
      "outputs": [],
      "source": [
        "class Color:\n",
        "    RED = '\\033[91m'\n",
        "    GREEN = '\\033[92m'\n",
        "    YELLOW = '\\033[93m'\n",
        "    BLUE = '\\033[94m'\n",
        "    MAGENTA = '\\033[95m'\n",
        "    CYAN = '\\033[96m'\n",
        "    WHITE = '\\033[97m'\n",
        "    RESET = '\\033[0m' #RESET: Resets all text formatting and color to default\n",
        "    BOLD = '\\033[1m' #BOLD: Makes the text bold\n",
        "    UNDERLINE = '\\033[4m' #UNDERLINE: Underlines the text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y80hfJRBqsLM"
      },
      "source": [
        "Random Seed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgQdo_c94IG5"
      },
      "source": [
        "# Dataset Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09Nrq6xD9viW"
      },
      "source": [
        "## Initial setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBFYnapNrZVx"
      },
      "source": [
        "\n",
        "The function, given the link of the dataset in google drive, download it.\n",
        "\n",
        "Args:\n",
        "        \n",
        "1.   **link_dataset** = link to zip files\n",
        "2.   **delete_zip_file** = {True/False}, use to remove the zip file once extracted the content\n",
        "\n",
        "Note: the link my be public or gdown can't download it\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Mk4TdLIY5CkF"
      },
      "outputs": [],
      "source": [
        "def download_dataset(link_dataset, delete_zip_file=False):\n",
        "\n",
        "    # Creating string for download\n",
        "\n",
        "    link_download_base =  global_var['link_download_prefix'] # General link for the download from Drive (with \"YOUR_FILE_ID\")\n",
        "    id_dataset = link_dataset.split(\"/view\")[0].split(\"/\")[-1]\n",
        "    dataset_download_link = link_download_base.replace(\"YOUR_FILE_ID\", id_dataset) # Replace the \"YOUR_FILE_ID\" part with the extracted id_dataset\n",
        "\n",
        "    path_download_dataset = global_var['path_download_dataset'] # Path of the zip file that contain the dataset\n",
        "    dataset_name = global_var['dataset_name'] # Dataset's Name\n",
        "    extract_directory = global_var['extract_directory'] # Directory of the GColab Runtime where we want to extract the dataset\n",
        "\n",
        "    # Downloading zip file\n",
        "    try:\n",
        "        if not( os.path.exists(dataset_name) ): # check is the dataset is already downloaded\n",
        "            gdown.download(\n",
        "                url=dataset_download_link, # Link that we extract with the file ID\n",
        "                output=path_download_dataset, # Path of the runtime Colab to save the zip file\n",
        "                quiet=False # Output to check the download\n",
        "            )\n",
        "            print(f\"{Color.GREEN}\\nZip file downloaded{Color.RESET}\")\n",
        "        else:\n",
        "            print(f\"{Color.GREEN}Zip file already downloaded{Color.RESET} \")\n",
        "    except Exception as error:\n",
        "        print(f\"{Color.RED}Download went wrong!\\nerror:{Color.RESET}\")\n",
        "        print(error)\n",
        "\n",
        "\n",
        "    # Unzipping the file\n",
        "    '''\n",
        "    In this part we want to unzip the zip file contain the dataset\n",
        "    '''\n",
        "    try:\n",
        "        if not( os.path.exists(dataset_name) ):\n",
        "\n",
        "          # This open the file zip in path_download_dataset in 'r' = read mode.\n",
        "          # The file is created from the ZipFile class contained in zipfile module\n",
        "          # and the name of the instance will be zipfile\n",
        "\n",
        "            with zipfile.ZipFile(path_download_dataset, 'r') as zip_file:\n",
        "\n",
        "                n_files = len(zip_file.namelist()) # number of files and directories inside the zip\n",
        "\n",
        "                with tqdm(total=n_files, desc='Unzipping files') as pbar: # It's a bar to track the unzip process, we pass the number of total files\n",
        "                    for file_name in zip_file.namelist():\n",
        "                        zip_file.extract(member=file_name, path=extract_directory) # Extract the file_name (iteration over all the files) in the path extract_directory\n",
        "                        pbar.update(1) # increment the progress bar of 1 unit for each extraction\n",
        "\n",
        "            print(f\"{Color.GREEN}Dataset {dataset_name} unzipped{Color.RESET}\")\n",
        "        else:\n",
        "            print(f\"{Color.GREEN}Dataset {dataset_name} already unzipped{Color.RESET} \")\n",
        "    except Exception as error:\n",
        "        print(f\"{Color.RED}Unzip went wrong!\\nerror:{Color.RESET}\")\n",
        "        print(error)\n",
        "\n",
        "    # Removing zip file\n",
        "    '''\n",
        "    In this part we remove the zip file if the flag\n",
        "    delete_zip_file is True (args of the function)\n",
        "    '''\n",
        "    try:\n",
        "        if delete_zip_file == True:\n",
        "            ! rm {path_download_dataset}\n",
        "            print(f\"{Color.GREEN}File {path_download_dataset} removed{Color.RESET}\")\n",
        "    except Exception as error:\n",
        "        print(f\"{Color.RED}Could not remove zip file, pass!\\nerror:{Color.RESET}\")\n",
        "        print(error)\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYJ7AV4CyyF5"
      },
      "source": [
        "**Merge Dataset**\n",
        "\n",
        "Given the dataset, it merge and shuffle it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ASW5hS0Tvg-0"
      },
      "outputs": [],
      "source": [
        "def merge_dataset():\n",
        "\n",
        "    dataset_name = global_var['dataset_name'] # Name of the directory\n",
        "    extract_directory = global_var['extract_directory'] # Directory of destination where we put the shuffled dataset\n",
        "    current_path = os.path.join(extract_directory, dataset_name) # Create the path \"/content/chest_xray\"\n",
        "    origin_dataset = os.path.join(current_path,dataset_name) # Create the path \"/content/chest_xray/chest_xray\"\n",
        "\n",
        "    # Create a list of this type -> ['/content/chest_xray/train', '/content/chest_xray/test', '/content/chest_xray/val']:\n",
        "    sub_directory = [ os.path.join(current_path, s) for s in ['train', 'test', 'val']]\n",
        "    classes = ['PNEUMONIA', 'NORMAL']\n",
        "\n",
        "    try:\n",
        "        for sub_dir in sub_directory:\n",
        "\n",
        "            for c in classes:\n",
        "                sub_dir_c = os.path.join(sub_dir, c) # Create a path of the type '/content/chest_xray/train/PNEUMONIA' and '/content/chest_xray/train/NORMAL' and so on...\n",
        "                counter = 0 # This is used for the name of each image\n",
        "\n",
        "                for img_name in os.listdir(sub_dir_c):\n",
        "                    counter += 1\n",
        "                    # Example of img_name:  IM-0001-0001.jpeg\n",
        "\n",
        "                    # sub_dir.split(\"/\")[-1]: Take the part \"train\", \"test\" or \"val\"\n",
        "                    # c.lower(): Take the name of the class in uppercase and transform in lowercase\n",
        "                    # f\"_{counter:05d}\": put the number counter starting with a zero sequence until 5 numbers sequence\n",
        "                    # current_name.split(\".\")[-1]: put the jpeg word at the end\n",
        "                    # Example of new_name: train_pneumonia_00001.jpeg\n",
        "\n",
        "                    new_name = sub_dir.split(\"/\")[-1] + \"_\" + c.lower() + f\"_{counter:05d}\" + \".\" + img_name.split(\".\")[-1]\n",
        "\n",
        "                    final_path = sub_dir + \"/\" + new_name # Create a path of the type: /content/chest_xray/train/train_pneumonia_00001.jpeg\n",
        "\n",
        "                    # Move the file from '/content/chest_xray/train/PNEUMONIA/IM-0001-0001.jpeg' to '/content/chest_xray/train/train_pneumonia_00001.jpeg'\n",
        "                    # this result in changin the name of the file\n",
        "                    os.rename( os.path.join(sub_dir_c, img_name), final_path )\n",
        "\n",
        "                ! rmdir {sub_dir_c} # Remove the empty directory \"PNEUMONIA\" and \"NORMAL\" after all rename\n",
        "        !rm -rf {origin_dataset} # Remove a useless directory\n",
        "\n",
        "        print(f\"{Color.GREEN}Dataset reordered and labelled{Color.RESET}\")\n",
        "    except Exception as error:\n",
        "        print(f\"Dataset already merged\\n\")\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GaUnYpNGxwXj"
      },
      "source": [
        "## Counter Function\n",
        "This function is used in two options:\n",
        "\n",
        "1. **labels** = *True* : in this case the counter will count the number of \"normal\" or \"pneumonia\" images in the \"path_dir\"\n",
        "\n",
        "1. **labels** = *False* : in this case the counter will count simply the number of images in the \"path_dir\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "s73kxQ4yIrTV"
      },
      "outputs": [],
      "source": [
        "def counter (path_dir, labels = True):\n",
        "\n",
        "  file_list = os.listdir(path_dir) # Create a list of all files in the dataset directory\n",
        "\n",
        "  if labels == True:\n",
        "\n",
        "    train_count_normal = 0 # Counter of images \"normal\"\n",
        "    train_count_pneumonia = 0 # Counter of images \"pneumonia\"\n",
        "\n",
        "  # Iterate through each file in the directory\n",
        "    for filename in file_list:\n",
        "        if 'normal' in filename:\n",
        "            train_count_normal += 1\n",
        "        elif 'pneumonia' in filename:\n",
        "            train_count_pneumonia += 1\n",
        "\n",
        "    return train_count_normal, train_count_pneumonia\n",
        "\n",
        "  else:\n",
        "\n",
        "    samples_num = len(file_list);\n",
        "    return samples_num"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgiZw8VRqNqf"
      },
      "source": [
        "# Preprocessing operation on dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "xtLCbTqxBR2d"
      },
      "outputs": [],
      "source": [
        "def get_gaussian(img_original):\n",
        "\n",
        "    img_original = cv2.imread(img_original, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "    smoothing_value = global_var['smoothing_value']\n",
        "    img_smoothed = cv2.GaussianBlur( img_original, ( smoothing_value, smoothing_value), 0 )\n",
        "\n",
        "    return img_smoothed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "DEU8BafbFqpY"
      },
      "outputs": [],
      "source": [
        "def get_gaussian_clahe(img_original):\n",
        "\n",
        "    img_original = cv2.imread(img_original, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "    smoothing_value = global_var['smoothing_value']\n",
        "    img_smoothed = cv2.GaussianBlur( img_original, ( smoothing_value, smoothing_value), 0 )\n",
        "\n",
        "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
        "    img_eq = clahe.apply(img_smoothed)\n",
        "\n",
        "    return img_eq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ju51SJzSjccg"
      },
      "outputs": [],
      "source": [
        "def get_gaussian_he(img_original):\n",
        "\n",
        "    img_original = cv2.imread(img_original, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "    smoothing_value = global_var['smoothing_value']\n",
        "    img_smoothed = cv2.GaussianBlur( img_original, ( smoothing_value, smoothing_value), 0 )\n",
        "    img_eq = cv2.equalizeHist(img_smoothed )\n",
        "\n",
        "    return img_eq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "sjRur4spqNYP"
      },
      "outputs": [],
      "source": [
        "def resize_and_save(mode, size):\n",
        "\n",
        "    original_dataset = global_var['dataset_name'] # Name of the original dataset\n",
        "    resized_dataset = global_var['dataset_name_resized'] # Name of the resized dataset\n",
        "    sub_folders_ttv = global_var['sub_folders_ttv'] # sub folder 'train', 'test', 'val'\n",
        "\n",
        "    extract_directory = global_var['extract_directory'] # Directory of the GColab Runtime where we want to extract the dataset\n",
        "    original_path = os.path.join(extract_directory, original_dataset) # Create the path \"/content/chest_xray\"\n",
        "    resized_path = os.path.join(extract_directory, resized_dataset) # Create the path \"/content/chest_xray_resized\"\n",
        "\n",
        "    # Create a list of this type -> ['/content/chest_xray/train', '/content/chest_xray/test', '/content/chest_xray/val']:\n",
        "    sub_directory_original = [ os.path.join(original_path, s) for s in sub_folders_ttv]\n",
        "\n",
        "    try:\n",
        "      for sub_dir in sub_directory_original:\n",
        "        category = os.path.basename(sub_dir) # Extract the part 'train', 'test' or 'val'\n",
        "\n",
        "        # Copy from source to destination\n",
        "        source_dir = sub_dir # Source Directory (Ex. '/content/chest_xray/train')\n",
        "        destination_dir = resized_path + '/' + category # Destination Directory ('Ex. /content/chest_xray_resized/train')\n",
        "\n",
        "        if not(os.path.exists(destination_dir)):\n",
        "          # Copy the entire directory\n",
        "          shutil.copytree(source_dir, destination_dir)\n",
        "\n",
        "          # Loop for the resizing and save\n",
        "          img_files = os.listdir(sub_dir)\n",
        "          for img in tqdm(img_files, desc=f\"Resizing {category} images\", unit=\"image\"):\n",
        "\n",
        "            # Take the name and extension of the image\n",
        "            name, extension = os.path.splitext(img)\n",
        "\n",
        "            # Read the image\n",
        "            img_path = sub_dir + '/' + name + extension\n",
        "            img = read_image(img_path) # Tensor Image (Ex. torch.Size([1, 928, 1288]))\n",
        "\n",
        "            # Add a batch dimensione of 1 needed for the nn.functional.interpolate method\n",
        "            img_4d = img.unsqueeze(0) # Tensor Image (Ex. torch.Size([1, 1, 928, 1288]))\n",
        "\n",
        "            # Interpolation method\n",
        "            resized_image = nn.functional.interpolate(img_4d, size=size, mode=mode) # Tensor Image (Ex. torch.Size([1, 1, 800, 800]))\n",
        "\n",
        "            # Remove the batch dimension\n",
        "            resized_image = resized_image.squeeze(0)  # Tensor Image (Ex. torch.Size([1, 800, 800]))\n",
        "\n",
        "            # Permute the dimensions\n",
        "            resized_image = resized_image.permute(1, 2, 0) # Tensor Image (Ex. torch.Size([800, 800, 1]))\n",
        "\n",
        "            # Transform in numpy structure\n",
        "            resized_image = resized_image.numpy() # Numpy Image (Ex. (800, 800, 1))\n",
        "\n",
        "            # Convert to PIL image using torchvision.transforms\n",
        "            to_pil = ToPILImage() # Transformer Numpy -> PIL Image\n",
        "            resized_image_pil = to_pil(resized_image) # PIL Image\n",
        "\n",
        "            # Save the resulting image\n",
        "            path_to_save = destination_dir + '/' + name + extension # Path of the type '/content/chest_xray_resized/train/img_name.jpeg'\n",
        "            resized_image_pil.save(path_to_save) # Function to save the image\n",
        "        else:\n",
        "          print(f\"{Color.GREEN}Data for {category} already resized{Color.RESET} \")\n",
        "\n",
        "    except Exception as error:\n",
        "      print(f\"Problem in resizing\\n\")\n",
        "      print(error)\n",
        "      pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "REAcgrBor5cX"
      },
      "outputs": [],
      "source": [
        "def data_augmentation(transform):\n",
        "\n",
        "    tr = transform # Transform that we use for augment the data of the training set\n",
        "    original_dataset = global_var['dataset_name_resized'] # Name of the original dataset\n",
        "    new_dataset = global_var['dataset_name_resized_augmented'] # Name of the resized dataset\n",
        "    sub_folders_vtt = global_var['sub_folders_ttv'] # sub folder 'train', 'test', 'val'\n",
        "\n",
        "    extract_directory = global_var['extract_directory'] # Directory of the GColab Runtime where we want to extract the dataset\n",
        "    original_path = os.path.join(extract_directory, original_dataset) # Create the path \"/content/chest_xray_resized\"\n",
        "    new_path = os.path.join(extract_directory, new_dataset) # Create the path \"/content/chest_xray_resized_augmented\"\n",
        "\n",
        "    # Create a list of this type -> ['/content/chest_xray_resized/train', '/content/chest_xray_resized/test', '/content/chest_xray_resized/val']:\n",
        "    sub_directory_original = [ os.path.join(original_path, s) for s in sub_folders_vtt]\n",
        "    # Create a list of this type -> ['/content/chest_xray_resized_augmented/train', '/content/chest_xray_resized_augmented/test', '/content/chest_xray_resized_augmented/val']:\n",
        "    sub_directory_augmented = [ os.path.join(new_path, s) for s in sub_folders_vtt]\n",
        "\n",
        "    try:\n",
        "      for sub_dir in sub_directory_original:\n",
        "        category = os.path.basename(sub_dir) # Extract the part 'train', 'test' or 'val'\n",
        "\n",
        "        # Copy from source to destination\n",
        "        source_dir = sub_dir # Source Directory ('Ex. /content/chest_xray_resized/train')\n",
        "        destination_dir = new_path + '/' + category # Destination Directory ('Ex. /content/chest_xray_resized_augmented/train')\n",
        "\n",
        "        if not(os.path.exists(destination_dir)):\n",
        "          # Copy the entire directory\n",
        "          shutil.copytree(source_dir, destination_dir)\n",
        "        else:\n",
        "          print(f\"{Color.GREEN}Data for {category} already copied{Color.RESET} \")\n",
        "          return\n",
        "\n",
        "    except Exception as error:\n",
        "      print(f\"Problem in copying\\n\")\n",
        "      print(error)\n",
        "      pass\n",
        "\n",
        "    try:\n",
        "      dir_train = sub_directory_augmented[0]                   # '/content/chest_xray_resized_augmented/train'\n",
        "      img_files_train = os.listdir(dir_train)                  # List of all training images\n",
        "      n_normal, n_pneumonia = counter(dir_train, labels=True)  # Count the number of normal and pneumonia images\n",
        "      difference = n_pneumonia - n_normal                      # Desired value of augmented images\n",
        "      sub_path = os.path.join(dir_train, '/augmented')         # Subdirectory where we save the augmented images\n",
        "      !mkdir {dir_train + '/augmented/'}                       # Create the subdirectory\n",
        "      actual = len(os.listdir(dir_train + '/augmented/'))      # Actual value of augmented images\n",
        "\n",
        "      n = 0                                                    # Counter used to name the augmented images\n",
        "\n",
        "      with tqdm(total=difference, desc='Balancing data') as pbar:\n",
        "        while  actual < difference:\n",
        "          img = random.choice(img_files_train)           # Pick a random image file from the list\n",
        "          if ('normal' in img):\n",
        "            n += 1\n",
        "            # Take the name and extension of the image\n",
        "            name, extension = os.path.splitext(img)\n",
        "\n",
        "            # Read the image as a Tensor\n",
        "            img = read_image(dir_train + '/' + img)\n",
        "\n",
        "            # Transform the image\n",
        "            new_image = transform(img); # Transformed Tensor Image\n",
        "\n",
        "            # Permute the dimensions\n",
        "            new_image = new_image.permute(1, 2, 0) # Tensor Image (Ex. torch.Size([800, 800, 1]))\n",
        "\n",
        "            # Transform in numpy structure\n",
        "            new_image = new_image.numpy() # Numpy Image (Ex. (800, 800, 1))\n",
        "\n",
        "            # Convert to PIL image using torchvision.transforms\n",
        "            to_pil = ToPILImage() # Transformer Numpy -> PIL Image\n",
        "            new_image_pil = to_pil(new_image) # PIL Image\n",
        "\n",
        "            # Save the resulting image:  Path of the type /content/chest_xray_resized_augmented/train/augmented/train_normal_00001_augmented.jpeg\n",
        "            path_to_save = dir_train + '/augmented/' + name.split('_')[0] + '_' + name.split('_')[1] + f\"_{actual:05d}\" + '_augmented' + extension\n",
        "            new_image_pil.save(path_to_save) # Function to save the image\n",
        "            actual += 1     # Update of Actual value of agumented images\n",
        "            pbar.update(1)\n",
        "\n",
        "      for img_name in os.listdir(dir_train + '/augmented/'):\n",
        "        final_path = dir_train + '/' # Create a path of the type: /content/chest_xray_resized_augmented/train/\n",
        "        os.rename(dir_train + '/augmented/' + img_name, final_path + img_name)\n",
        "\n",
        "      !rmdir {dir_train + '/augmented/'} # Remove the empty directory \"PNEUMONIA\" and \"NORMAL\" after all rename\n",
        "\n",
        "    except Exception as error:\n",
        "        print(f\"Problem in augmenting\\n\")\n",
        "        print(error)\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "mSfyDAb52zE1"
      },
      "outputs": [],
      "source": [
        "def augment_class(class_to_augment, n_new_samples, transform, subfolder_to_augment):\n",
        "\n",
        "    if n_new_samples == 0:\n",
        "      return\n",
        "\n",
        "    resized_dataset = global_var['dataset_name_resized'] # where take imaged to augment\n",
        "    augmented_dataset = global_var['dataset_name_resized_augmented'] # where add new samples\n",
        "    dir_train = os.path.join(resized_dataset, subfolder_to_augment)\n",
        "    image_list = os.listdir( dir_train )\n",
        "\n",
        "    actual_id = len(image_list)\n",
        "\n",
        "    with tqdm(total=n_new_samples, desc=f'Augmenting data for {class_to_augment} ') as pbar:\n",
        "\n",
        "      while actual_id < n_new_samples + len(image_list) :\n",
        "        img = random.choice( image_list )\n",
        "\n",
        "        if class_to_augment in img:\n",
        "          # Take the name and extension of the image\n",
        "          name, extension = os.path.splitext(img)\n",
        "\n",
        "          # Read the image as a Tensor\n",
        "          img = read_image(dir_train + '/' + img)\n",
        "\n",
        "          # Transform the image\n",
        "          new_image = transform(img); # Transformed Tensor Image\n",
        "\n",
        "          # Permute the dimensions\n",
        "          new_image = new_image.permute(1, 2, 0) # Tensor Image (Ex. torch.Size([800, 800, 1]))\n",
        "\n",
        "          # Transform in numpy structure\n",
        "          new_image = new_image.numpy() # Numpy Image (Ex. (800, 800, 1))\n",
        "\n",
        "          # Convert to PIL image using torchvision.transforms\n",
        "          to_pil = ToPILImage() # Transformer Numpy -> PIL Image\n",
        "          new_image_pil = to_pil(new_image) # PIL Image\n",
        "\n",
        "          # Save the resulting image\n",
        "          path_to_save = augmented_dataset + f'/{subfolder_to_augment}/' + name.split('_')[0] + '_' + name.split('_')[1] + f\"_{actual_id:05d}\" + '_augmentedv2' + extension\n",
        "          new_image_pil.save(path_to_save)\n",
        "\n",
        "          pbar.update(1)\n",
        "          actual_id += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "qROIm9cA22Vr"
      },
      "outputs": [],
      "source": [
        "def create_subset(main_path, subfolder_percentage):\n",
        "\n",
        "  substring = global_var[\"subset_suffix\"]\n",
        "  classes = [\"normal\", \"pneumonia\"]\n",
        "  sub_dir = global_var['sub_folders_ttv'] # train, test, val\n",
        "  subs_paths = [ os.path.join(main_path + \"_subset\", subs) for subs in sub_dir ]\n",
        "\n",
        "  if os.path.exists(main_path + \"_subset\"):\n",
        "    print(f\"{Color.GREEN}Subset already created{Color.RESET}\")\n",
        "    return\n",
        "\n",
        "  for s in subs_paths:\n",
        "    if not os.path.exists(s):\n",
        "      os.makedirs(s)\n",
        "\n",
        "  subfolder_dimension = {}\n",
        "  for s_path in os.listdir(main_path):\n",
        "      num_files = len( os.listdir( os.path.join(main_path, s_path)) )  # compute actual dimension of subfolder\n",
        "\n",
        "      subfolder_size = round(num_files * subfolder_percentage)  # compute new dimension\n",
        "      subfolder_dimension[s_path] = subfolder_size\n",
        "\n",
        "\n",
        "  for subs_path in subs_paths:  # iterate over train, test, val on subset\n",
        "    subs_counter = 0\n",
        "    sub_dir_type = subs_path.split(\"/\")[-1] # take the type train, test, val\n",
        "    image_list = os.listdir( os.path.join(main_path, sub_dir_type) ) # list of all images\n",
        "\n",
        "    for c in classes: # iter over classes to ensure a balanced subset\n",
        "      while len(os.listdir(subs_path)) < subfolder_dimension[sub_dir_type]*(subs_counter +1):  # check on the length on the subdir\n",
        "        img = random.choice( image_list )\n",
        "\n",
        "\n",
        "        if c in img: # copying images\n",
        "          source_img_path = os.path.join(main_path, sub_dir_type, img)\n",
        "          dest_img_path = os.path.join(subs_path, img)\n",
        "          shutil.copy(source_img_path, dest_img_path)\n",
        "\n",
        "      subs_counter += 1\n",
        "\n",
        "    print(f\"{Color.GREEN}Subset {subs_path} created, {Color.MAGENTA}{subfolder_dimension[sub_dir_type]} samples {Color.GREEN}per class{Color.RESET}\")\n",
        "\n",
        "\n",
        "  total_samples = 0\n",
        "  for subs_path in subs_paths:\n",
        "    total_samples += len(os.listdir(subs_path))\n",
        "\n",
        "  print(f\"{Color.GREEN}Subset created with a total of {Color.MAGENTA}{total_samples} samples{Color.RESET}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "OKD7cvURZZxN"
      },
      "outputs": [],
      "source": [
        "def unify_folder(path_to_folder):\n",
        "\n",
        "  sub_folders = global_var['sub_folders_ttv'] # ['train', 'test', 'val']\n",
        "\n",
        "  # check if already unified\n",
        "  if len(os.listdir(path_to_folder)) != len(sub_folders):\n",
        "    print(f\"{Color.GREEN}Already unified{Color.RESET}\")\n",
        "    return\n",
        "\n",
        "  for sub_folder in sub_folders:\n",
        "\n",
        "    if not os.path.exists(os.path.join(path_to_folder, sub_folder)):\n",
        "      print(f\"{Color.RED}Subfolder {sub_folder} doesn't exist{Color.RESET}\")\n",
        "      continue\n",
        "\n",
        "    for images in os.listdir( os.path.join( path_to_folder, sub_folder) ):\n",
        "\n",
        "      path_to_img = os.path.join(path_to_folder, sub_folder, images)\n",
        "      shutil.move(path_to_img, path_to_folder)\n",
        "\n",
        "    shutil.rmtree(os.path.join(path_to_folder, sub_folder))\n",
        "\n",
        "  print(f\"{Color.GREEN}Folder {path_to_folder} unified{Color.RESET}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "uyLjQfGMbkz7"
      },
      "outputs": [],
      "source": [
        "def preprocess_image(image_global_path, mode):\n",
        "\n",
        "    # based of the selected mode, it does a preprocess on image\n",
        "    # returun None is something went wrong\n",
        "\n",
        "    if mode == 'gaussian_clahe':\n",
        "        preprocessed_img = get_gaussian_clahe( image_global_path )\n",
        "    elif mode == 'gaussian':\n",
        "        preprocessed_img = get_gaussian( image_global_path )\n",
        "    elif mode == 'gaussian_he':\n",
        "        preprocessed_img = get_gaussian_he( image_global_path )\n",
        "    else:\n",
        "        print(f\"{Color.RED}Preprocessing {mode} is not available{Color.RESET}\")\n",
        "        ! rm -rf {new_dataset_path}\n",
        "\n",
        "        preprocessed_img = None\n",
        "\n",
        "    return preprocessed_img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "KCT0M4ovvdhG"
      },
      "outputs": [],
      "source": [
        "def create_preprocessed_dataset( dataset_path, new_dataset_path, mode ):\n",
        "\n",
        "    categories = global_var['sub_folders_ttv'] #['train', 'test', 'val']\n",
        "    mode = mode.lower()\n",
        "\n",
        "    if os.path.exists(new_dataset_path):\n",
        "        print(f\"{Color.GREEN}Dataset {new_dataset_path} images already exists{Color.RESET}\")\n",
        "        return\n",
        "    else:\n",
        "        ! cp -r {dataset_path} {new_dataset_path}   # coping augmented dataset \"/content/chest_xray_{mode}\"\n",
        "\n",
        "    for c in categories:  # iterate on train, test, val\n",
        "\n",
        "        n_files = len( os.listdir( os.path.join(new_dataset_path, c ) ) )   # it is \"/content/chest_xray_{mode}/train,test,val\"\n",
        "\n",
        "        with tqdm(total=n_files, desc='Processing ' + c) as pbar:\n",
        "            for img_path in os.listdir( os.path.join(dataset_path, c) ):   # iterate on train, test, val of heatmap dataset\n",
        "\n",
        "                full_img_path = new_dataset_path + \"/\" + c + \"/\" + img_path # global path to image\n",
        "\n",
        "                preprocessed_img = preprocess_image(full_img_path, mode)    # preprocess the image according the the selected mode\n",
        "\n",
        "                '''\n",
        "                if \"augmented_v2\" in img_path:\n",
        "                    new_name = img_path.replace(\"augmented_v2\", mode)   # renaming the images\n",
        "                elif \"augmented\" in img_path:\n",
        "                    new_name = img_path.replace(\"augmented\", mode)   # renaming the images\n",
        "                else:\n",
        "                    new_name = img_path.replace(\".\", f\"_{mode}.\")\n",
        "                '''\n",
        "                new_name = img_path.replace(\".\", f\"_{mode}.\")\n",
        "\n",
        "                #print(new_name)\n",
        "                new_full_path = full_img_path.replace(img_path, new_name)  # /content/chest_xray_heatmap/train/test_normal_00001_heatmap.jpeg\n",
        "\n",
        "                cv2.imwrite(full_img_path, preprocessed_img) # overwrite the image\n",
        "                os.rename(full_img_path, new_full_path) # renaming the image\n",
        "\n",
        "                pbar.update(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XZvgZPRvK13"
      },
      "source": [
        "# Dataset and Network classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4uFWWdnvMeh"
      },
      "source": [
        "## ChestRayDataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "05zFupZ3yC5K"
      },
      "outputs": [],
      "source": [
        "class ChestRayDataset(Dataset):\n",
        "\n",
        "    def __init__(self,file_path, transform):\n",
        "        self.file_path = file_path\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(os.listdir(self.file_path))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        sample_path = self.file_path + \"/\" + os.listdir(self.file_path)[index]\n",
        "        sample = cv2.imread(sample_path)\n",
        "        sample = cv2.cvtColor(sample, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Output of the class\n",
        "        sample_tensor = self.transform(sample)\n",
        "        label = sample_path.split(\"/\")[-1].split(\"_\")[1]\n",
        "\n",
        "        if label == \"normal\":\n",
        "            label = 0\n",
        "        else:\n",
        "            label = 1\n",
        "\n",
        "        return sample_tensor,label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yOQD6ATspbh"
      },
      "source": [
        "## Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9mzIlG5MXGK"
      },
      "source": [
        "### Blocks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "4mJYzPUhKrW8"
      },
      "outputs": [],
      "source": [
        "class SeparableConv2D(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=7, padding='same', bias=True):\n",
        "        super(SeparableConv2D, self).__init__()\n",
        "\n",
        "        self.depthwise = nn.Conv2d(\n",
        "            in_channels=in_channels,\n",
        "            out_channels=out_channels,\n",
        "            kernel_size=kernel_size,\n",
        "            padding=padding,\n",
        "            groups=in_channels,\n",
        "            bias=bias\n",
        "            )\n",
        "\n",
        "        self.pointwise = nn.Conv2d(\n",
        "            in_channels=in_channels,\n",
        "            out_channels=out_channels,\n",
        "            kernel_size=1,\n",
        "            bias=bias\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.depthwise(x)\n",
        "        x = self.pointwise(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "aP5rm2XROYGW"
      },
      "outputs": [],
      "source": [
        "class CAM(nn.Module):\n",
        "    def __init__(self, channels, ratio=8, bias=False):\n",
        "        super(CAM,self).__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(\n",
        "            in_features=channels,\n",
        "            out_features=channels//ratio,\n",
        "            bias=bias\n",
        "            )\n",
        "\n",
        "        self.fc2 = nn.Linear(\n",
        "            in_features=channels//ratio,\n",
        "            out_features=channels,\n",
        "            bias=bias\n",
        "            )\n",
        "\n",
        "        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.global_max_pool = nn.AdaptiveMaxPool2d((1,1))\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # Input Tensor: torch.Size([1, 1024, 8, 8])\n",
        "        x1 = torch.mean(x, dim=(2,3)) # GAP: torch.Size([1, 1024])\n",
        "        x1 = self.fc1(x1) # GAP + Dense: torch.Size([1, 128])\n",
        "        x1 = self.relu(x1)\n",
        "        x1 = self.fc2(x1) # GAP + Dense + Dense: torch.Size([1, 1024])\n",
        "\n",
        "\n",
        "        x2 = self.global_max_pool(x) # GMP: torch.Size([1, 1024, 1, 1])\n",
        "        x2 = x2.view(x2.size(0), -1) # view: torch.size([1, 1024])\n",
        "        x2 = self.fc1(x2) # GMP + Dense: torch.Size([1, 128])\n",
        "        x2 = self.relu(x2)\n",
        "        x2 = self.fc2(x2) # GMP + Dense + Dense: torch.Size([1, 1024])\n",
        "\n",
        "        feats = x1 + x2 # Feats shape: torch.Size([1, 1024])\n",
        "        feats = self.sigmoid(feats)\n",
        "        feats = feats.view(feats.size(0), feats.size(1), 1, 1) # feats.shape: torch.Size([1, 1024, 1, 1])\n",
        "        feats = torch.mul(x, feats) # Output shape: torch.Size([1, 1024, 8, 8])\n",
        "        return feats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "5eypP1uqnZfo"
      },
      "outputs": [],
      "source": [
        "class SAM_avg (nn.Module):\n",
        "    def __init__(self, channels, padding='same'):\n",
        "\n",
        "        super(SAM_avg, self).__init__()\n",
        "\n",
        "        self.sep_conv2d_1 = SeparableConv2D(\n",
        "            in_channels=channels,\n",
        "            out_channels=channels,\n",
        "            kernel_size=1,\n",
        "            padding=padding\n",
        "        )\n",
        "\n",
        "        self.sep_conv2d_2 = SeparableConv2D(\n",
        "            in_channels=channels,\n",
        "            out_channels=channels,\n",
        "            kernel_size=3,\n",
        "            padding=padding\n",
        "        )\n",
        "\n",
        "        self.batchnorm2d = nn.BatchNorm2d(channels)\n",
        "\n",
        "        self.conv2d_3 = nn.Conv2d(\n",
        "            in_channels=1,\n",
        "            out_channels=1,\n",
        "            kernel_size=7,\n",
        "            padding=padding\n",
        "        )\n",
        "\n",
        "        self.sigm= nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x, cam):\n",
        "\n",
        "\n",
        "        # Input tensor: torch.Size([1, 1024, 8, 8])\n",
        "\n",
        "        x = self.sep_conv2d_1(x) # SepConv2d_1 shape: torch.Size([1, 1024, 8, 8])\n",
        "        x = self.sep_conv2d_2(x) # SepConv2d_2 shape: torch.Size([1, 1024, 8, 8])\n",
        "        x = self.batchnorm2d(x) # BatchNorm2d shape: torch.Size([1, 1024, 8, 8])\n",
        "        x = x*cam # X*CAM shape: torch.Size([1, 1024, 8, 8])\n",
        "\n",
        "        x1 = torch.mean(x, dim=1, keepdim=True) # Mean pooling shape: torch.Size([1, 1, 8, 8])\n",
        "\n",
        "        feats = self.conv2d_3(x1) # Conv2d shape: torch.Size([1, 1, 8, 8])\n",
        "        feats = self.sigm(feats)\n",
        "        out = x * feats # Output shape: torch.Size([1, 1024, 8, 8])\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "collapsed": true,
        "id": "uAj3XIHNh6qc"
      },
      "outputs": [],
      "source": [
        "class SAM_max (nn.Module):\n",
        "    def __init__(self, channels, padding='same'):\n",
        "\n",
        "        super(SAM_max, self).__init__()  # Initialize nn.Module\n",
        "\n",
        "        self.sep_conv2d_1 = SeparableConv2D(\n",
        "            in_channels=channels,\n",
        "            out_channels=channels,\n",
        "            kernel_size=1,\n",
        "            padding=padding\n",
        "        )\n",
        "\n",
        "        self.sep_conv2d_2 = SeparableConv2D(\n",
        "            in_channels=channels,\n",
        "            out_channels=channels,\n",
        "            kernel_size=3,\n",
        "            padding=padding\n",
        "        )\n",
        "\n",
        "        self.batchnorm2d = nn.BatchNorm2d(channels)\n",
        "\n",
        "        self.conv2d_3 = nn.Conv2d(\n",
        "            in_channels=1,\n",
        "            out_channels=1,\n",
        "            kernel_size=7,\n",
        "            padding=padding\n",
        "        )\n",
        "\n",
        "        self.sigm= nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x, cam):\n",
        "\n",
        "        # input: torch.Size([1, 1024, 8, 8])\n",
        "\n",
        "        x = self.sep_conv2d_1(x) # SepConv2d_1 shape: torch.Size([1, 1024, 8, 8])\n",
        "        x = self.sep_conv2d_2(x) # SepConv2d_2 shape: torch.Size([1, 1024, 8, 8])\n",
        "        x = self.batchnorm2d(x) # BatchNorm2d shape: torch.Size([1, 1024, 8, 8])\n",
        "        x = x*cam # X*CAM shape: torch.Size([1, 1024, 8, 8])\n",
        "\n",
        "        # Max Pooling\n",
        "        x2, _ = torch.max(x, dim=1, keepdim=True) # MAX pooling shape: torch.Size([1, 1, 8, 8])\n",
        "\n",
        "        # Convolution and Multiplication\n",
        "        feats = self.conv2d_3(x2) # Conv2d shape: torch.Size([1, 1, 8, 8])\n",
        "        feats = self.sigm(feats)\n",
        "        out = x*feats # Output shape: torch.Size([1, 1024, 8, 8])\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "U2PP05AUj3Gd"
      },
      "outputs": [],
      "source": [
        "class RichardsSigmoid(nn.Module):\n",
        "    def __init__(self, units=1, bias=True):\n",
        "        super(RichardsSigmoid, self).__init__()\n",
        "\n",
        "        self.A = nn.Parameter(torch.Tensor(units))\n",
        "        self.Q = nn.Parameter(torch.Tensor(units))\n",
        "        self.mu = nn.Parameter(torch.Tensor(units))\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        nn.init.zeros_(self.A)\n",
        "        nn.init.zeros_(self.Q)\n",
        "        nn.init.zeros_(self.mu)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Richards sigmoid function\n",
        "        return 1 / (1 + torch.exp(-self.A * torch.exp(-self.Q * (x - self.mu))))\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape[:-1] + (self.units,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "ak3hs7oFj5rf"
      },
      "outputs": [],
      "source": [
        "class ChannelDropout(nn.Module):\n",
        "    def __init__(self, channels, drop_ratio=0.2):\n",
        "        super(ChannelDropout, self).__init__()\n",
        "\n",
        "        self.drop_ratio = drop_ratio # Ratio of channels to be dropped.\n",
        "\n",
        "        # Build the Mask with ones\n",
        "        self.mask = nn.Parameter(torch.ones(1, channels, 1 , 1))\n",
        "\n",
        "        # Build the Richard Sigmoid to apply to the mask\n",
        "        self.r_sigm = RichardsSigmoid(units=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply the Richard Sigmoid to the mask\n",
        "        mask = self.r_sigm(self.mask) # MASK Shape: torch.Size([1, 3072, 1, 1])\n",
        "\n",
        "        # Repeat the mask as the batch size of the input (to apply to all the features)\n",
        "        batch_size = x.size(0)\n",
        "        mask_replicated = mask.repeat(batch_size, 1, 1, 1)\n",
        "\n",
        "        # Apply the mask to the input\n",
        "        x = x * mask_replicated # Input after Mask: torch.Size([1, 3072, 8, 8])\n",
        "\n",
        "        # New Channel dimension\n",
        "        new_n_channels = int(x.shape[1] * (1 - self.drop_ratio))\n",
        "\n",
        "        # Perform top-k operation along the channels dimension (dim=1)\n",
        "        # Keep the channels with highest values\n",
        "        _, indices = torch.topk(x, k=new_n_channels, dim=1, largest=True)\n",
        "        output = torch.gather(x, dim=1, index=indices)\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "WgzwSMXLnj6P"
      },
      "outputs": [],
      "source": [
        "class CSSAM (nn.Module):\n",
        "    def __init__(self, channels, padding='same', ratio=8, bias=False, drop_ratio=0.2):\n",
        "\n",
        "        super(CSSAM, self).__init__()  # Initialize nn.Module\n",
        "\n",
        "\n",
        "        self.SamAvg = SAM_avg(\n",
        "            channels=channels,\n",
        "            padding=padding\n",
        "            )\n",
        "\n",
        "        self.SamMax = SAM_max(\n",
        "            channels=channels,\n",
        "            padding=padding\n",
        "            )\n",
        "\n",
        "        self.cam = CAM(\n",
        "            channels=channels,\n",
        "            ratio=ratio,\n",
        "            bias=bias\n",
        "            )\n",
        "\n",
        "        self.ch_dropout = ChannelDropout(\n",
        "            channels=channels*3,\n",
        "            drop_ratio=drop_ratio\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        cam = self.cam(x)\n",
        "\n",
        "        # SAM average and SAM max\n",
        "        x_avg = self.SamAvg(x, cam) # SAM_avg shape: torch.Size([1, 1024, 8, 8])\n",
        "        x_max = self.SamMax(x, cam) # SAM_max shape: torch.Size([1, 1024, 8, 8])\n",
        "\n",
        "        # Concatenation\n",
        "        x = torch.cat((x_avg, x_max, cam), dim=1) # Concatenation shape: torch.Size([1, 3072, 8, 8])\n",
        "\n",
        "        # Channel Dropout\n",
        "        out = self.ch_dropout(x) # Channel Dropout shape: torch.Size([1, 1536, 8, 8])\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DP7tdvSgMaRV"
      },
      "source": [
        "### Bodies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "DMnIyzqTsy44"
      },
      "outputs": [],
      "source": [
        "class JAM_network(nn.Module):\n",
        "    def __init__(self, channels, padding='same', drop_ratio=0.2, dropout_rate=0.35 ):\n",
        "        super(JAM_network,self).__init__()\n",
        "\n",
        "        self.densenet169 = models.densenet169(pretrained=True).features\n",
        "\n",
        "        self.cssam = CSSAM(\n",
        "            channels=channels,\n",
        "            padding=padding,\n",
        "            drop_ratio=drop_ratio\n",
        "            )\n",
        "\n",
        "        self.dropout = nn.Dropout(\n",
        "            p=dropout_rate\n",
        "            )\n",
        "\n",
        "\n",
        "        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # Input Shape: torch.Size([1, 3, 256, 256])\n",
        "\n",
        "        # BACKBONE\n",
        "        x_dense = self.densenet169(x) # x_dense.shape: torch.Size([1, 1024, 8, 8])\n",
        "\n",
        "        # CSSAM\n",
        "        x_cssam = self.cssam(x_dense) # CSSAM shape: torch.Size([1, 2457, 8, 8])\n",
        "\n",
        "        # GAP\n",
        "        feats = self.global_avg_pool(x_cssam) # GAP shape: torch.Size([1, 2457, 1, 1])\n",
        "        feats = torch.flatten(feats, start_dim=1) # Flattening shape: torch.Size([1, 2457])\n",
        "\n",
        "        # DROPOUT\n",
        "        feats = self.dropout(feats) # dropout: torch.Size([1, 2457])\n",
        "\n",
        "        return feats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "guvsPltcgb0q",
        "outputId": "140868ef-41f0-49a5-dd4c-e4d99ab277d0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nprint(device)\\njam_try = JAM(1664, device=device, model_name=\"jam_model.pt\")\\nt = torch.rand([2, 3, 256, 256]).to(device)\\nprint(f\"input tensor: {t.shape}\")\\n\\nout = jam_try(t)\\nprint(f\"out.shape: {out.shape},\\n{out}\")\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "class JAM(nn.Module):\n",
        "    def __init__(self, channels, device, model_name, padding='same', drop_ratio=0.2, dropout_rate=0.35):\n",
        "        super().__init__()\n",
        "\n",
        "        self.body = JAM_network(\n",
        "            channels=channels,\n",
        "            padding=padding,\n",
        "            drop_ratio=drop_ratio,\n",
        "            dropout_rate=dropout_rate\n",
        "            ).to(device)\n",
        "\n",
        "        self.kan_ch = int(channels * 3 * (1 - drop_ratio))\n",
        "        self.classifier = KAN([ self.kan_ch, 1])\n",
        "        self.classifier.to(device)\n",
        "\n",
        "        self.sig = nn.Sigmoid()\n",
        "\n",
        "        self.loss_fn = nn.BCELoss()\n",
        "        self.optimizer = torch.optim.AdamW(self.parameters(), lr=0.0001) # torch.optim.SGD(self.model.parameters(), lr=0.1)\n",
        "\n",
        "        self.accuracy = BinaryAccuracy()\n",
        "        self.precision = BinaryPrecision()\n",
        "        self.recall = BinaryRecall()\n",
        "        self.auroc = BinaryAUROC()\n",
        "        self.f1score = BinaryF1Score()\n",
        "\n",
        "        self.model_name = model_name\n",
        "\n",
        "    def set_training_mode(self):\n",
        "        self.body.train()\n",
        "\n",
        "    def set_eval_mode(self):\n",
        "        self.body.eval()\n",
        "\n",
        "    def forward(self, model_input):\n",
        "        out_body = self.body(model_input)\n",
        "        out_cls = self.classifier(out_body)\n",
        "        out = self.sig(out_cls)\n",
        "        return out\n",
        "\n",
        "    def save(self, string_id, stamp=True):\n",
        "        name = self.model_name + f\"_{string_id}.pt\"\n",
        "        torch.save(self.state_dict(), name )\n",
        "\n",
        "        if stamp==True:\n",
        "            print(f\"{Color.MAGENTA}saved: {name}{Color.RESET}\")\n",
        "\n",
        "    def load(self, string_id):\n",
        "        name = self.model_name + f\"_{string_id}.pt\"\n",
        "        try:\n",
        "            self.load_state_dict(torch.load(name) )\n",
        "            print(f\"{Color.MAGENTA}loaded: {name}{Color.RESET}\")\n",
        "        except Exception as e:\n",
        "            print(f\"{Color.RED}Model not loaded: try specify 'string_id' parameter{Color.RESET}\")\n",
        "            print(e)\n",
        "\n",
        "'''\n",
        "print(device)\n",
        "jam_try = JAM(1664, device=device, model_name=\"jam_model.pt\")\n",
        "t = torch.rand([2, 3, 256, 256]).to(device)\n",
        "print(f\"input tensor: {t.shape}\")\n",
        "\n",
        "out = jam_try(t)\n",
        "print(f\"out.shape: {out.shape},\\n{out}\")\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HmvdEe24KbO"
      },
      "source": [
        "# Dataset informations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owMu7Tiqzbnc"
      },
      "source": [
        "## Download and merge of the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "hLaY4F33bKCc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205,
          "referenced_widgets": [
            "20b667a6d1154779a12bb72950baffa9",
            "f810889e570d4240930a2a9b7f439be1",
            "c67c1b1015c6432f9daf72a02fdf9f2b",
            "bc1677055b42462f844f7143dce9f959",
            "81c56c5ee7e94c178a29b9b26f67d994",
            "adeb4d702f6941cf931a0824b3eb7446",
            "22cd9fed21694334ae025f18a4ce35ba",
            "fb2f320e442c49c69967e9fd3f4a1da0",
            "888bc10fc42646f092fbf85b216c4baf",
            "ac70531a2f6a4ae981e80f6269a4e6e4",
            "f50c79101d024da79e614a0defdc0a53"
          ]
        },
        "outputId": "59e04661-3f35-4850-ff99-df741283c7c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?export=download&id=10un_W7teXQy9LOB1uqd0e6VJ46UFNRza\n",
            "From (redirected): https://drive.google.com/uc?export=download&id=10un_W7teXQy9LOB1uqd0e6VJ46UFNRza&confirm=t&uuid=229a8717-d25f-41d4-9cc9-f82669f85b64\n",
            "To: /content/dataset.zip\n",
            "100%|██████████| 2.46G/2.46G [01:10<00:00, 35.0MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[92m\n",
            "Zip file downloaded\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Unzipping files:   0%|          | 0/17591 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "20b667a6d1154779a12bb72950baffa9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[92mDataset chest_xray unzipped\u001b[0m\n",
            "\u001b[92mDataset reordered and labelled\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "download_dataset(\n",
        "     link_dataset=global_var['link_dataset'],\n",
        "     delete_zip_file=False\n",
        " )\n",
        "\n",
        "merge_dataset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "86abced6216f4d3a9854e81b0ebd523a",
            "776dd6c978dc4c0d9f3ee8b52244dbfb",
            "7ae83c71e0084ef08f5a84cb1075a79d",
            "7bf2f17e336341a4bd827485850a5eb3",
            "71d7d8ae966a457aa26bd19f8b116647",
            "7a9fcdaeb4df4b4ba6b52960e90f3ffe",
            "87be9c3a15394495be44c9bc61b81caf",
            "73615a20a3e74f8bb94ac8f461bf46db",
            "93aecdf2f13e4235a92a289918dc6340",
            "4f645a3c548648dc9f58fd89a8169060",
            "28391d760c594e4eaad89baf5318fcb7"
          ]
        },
        "id": "OVeAMibJverg",
        "outputId": "5dbd25e4-30c8-4902-c9c9-8e875a780f9a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Resizing train images:   0%|          | 0/5216 [00:00<?, ?image/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "86abced6216f4d3a9854e81b0ebd523a"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "size = global_var[\"resizing_dim\"] # Choose the size\n",
        "\n",
        "resize_and_save(\n",
        "    mode='bilinear',    # Choose the mode of interpolation ('nearest', 'bilinear', 'bicubic', ...)\n",
        "    size=size\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lg6MUQReBBLp"
      },
      "outputs": [],
      "source": [
        "probs = 0.15\n",
        "\n",
        "resize_crop_transform = transforms.RandomResizedCrop(size=size, scale=(0.8, 1.0))  # For zoom and size change\n",
        "rotation_transform = transforms.RandomRotation(degrees=(0,20))  # For random rotation\n",
        "horizontal_flip_transform = transforms.RandomHorizontalFlip(p=1.0)  # For horizontal flip\n",
        "shift_transform = transforms.RandomAffine(degrees=0, translate=(0.1, 0.1))  # for random shift beween 0 and translate value\n",
        "\n",
        "\n",
        "tr_augmentation = transforms.Compose([\n",
        "     transforms.RandomApply( [resize_crop_transform], p=probs ),\n",
        "     transforms.RandomApply( [rotation_transform], p=probs ),\n",
        "     transforms.RandomApply( [horizontal_flip_transform], p=probs ),\n",
        "     transforms.RandomApply([shift_transform], p=probs),\n",
        "])\n",
        "\n",
        "data_augmentation(tr_augmentation)\n",
        "\n",
        "if global_var[\"augment_performed\"] == False:\n",
        "    aug_train_dimension = global_var[\"augmentation_train_dimension\"]\n",
        "    aug_test_dimension = global_var[\"augmentation_test_dimension\"]\n",
        "\n",
        "    augment_class('normal', aug_train_dimension, tr_augmentation, 'train')\n",
        "    augment_class('pneumonia', aug_train_dimension, tr_augmentation, 'train')\n",
        "\n",
        "    augment_class('normal', aug_test_dimension, tr_augmentation, 'test')\n",
        "\n",
        "    global_var[\"augment_performed\"] = True"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_directory = global_var[\"extract_directory\"] + \"/\" + global_var[\"dataset_name_resized_augmented\"]\n",
        "sub_directory = global_var['sub_folders_ttv']\n",
        "\n",
        "\n",
        "for sub_dir in sub_directory:\n",
        "  pneumonia_counter = 0\n",
        "  normal_counter = 0\n",
        "  for img in os.listdir( os.path.join (dataset_directory, sub_dir) ):\n",
        "    if \"pneumonia\" in img:\n",
        "      pneumonia_counter += 1\n",
        "    else:\n",
        "      normal_counter += 1\n",
        "\n",
        "  print(f\"The number of pneumonia images in {sub_dir}:\", pneumonia_counter)\n",
        "  print(f\"The number of normal images in {sub_dir}:\", normal_counter)"
      ],
      "metadata": {
        "id": "yzAGj3IhF65y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fZ8UZCB_kJYm"
      },
      "outputs": [],
      "source": [
        "dataset_directory = global_var[\"extract_directory\"] + \"/\" + global_var[\"dataset_name_resized_augmented\"]\n",
        "gaussian_he_directory = global_var[\"extract_directory\"] + \"/\" + global_var[\"dataset_name_gaussian_he\"]\n",
        "\n",
        "create_preprocessed_dataset(\n",
        "    dataset_path = dataset_directory,\n",
        "    new_dataset_path = gaussian_he_directory,\n",
        "    mode = 'gaussian_he'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZUzk4d-liAkO"
      },
      "outputs": [],
      "source": [
        "dataset_directory = global_var[\"extract_directory\"] + \"/\" + global_var[\"dataset_name_resized_augmented\"]\n",
        "gaussian_directory = global_var[\"extract_directory\"] + \"/\" + global_var[\"dataset_name_gaussian\"]\n",
        "\n",
        "create_preprocessed_dataset(\n",
        "    dataset_path = dataset_directory,\n",
        "    new_dataset_path = gaussian_directory,\n",
        "    mode = 'gaussian'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "756DIgy_kTY9"
      },
      "outputs": [],
      "source": [
        "dataset_directory = global_var[\"extract_directory\"] + \"/\" + global_var[\"dataset_name_resized_augmented\"]\n",
        "gaussian_clahe_directory = global_var[\"extract_directory\"] + \"/\" + global_var[\"dataset_name_gaussian_clahe\"]\n",
        "\n",
        "create_preprocessed_dataset(\n",
        "    dataset_path = dataset_directory,\n",
        "    new_dataset_path = gaussian_clahe_directory,\n",
        "    mode = 'gaussian_clahe'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_sFf8oHnnS8s"
      },
      "source": [
        "## Dataset exploration, augmentation and visualization\n",
        "\n",
        "In this part we plot an:\n",
        "\n",
        "1. *Histogram Distribution* : to see if the dataset is balanced or unbalanced\n",
        "\n",
        "2. *Pie Chart* : to visualize the sizes of training, test and validation sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "YVgIgejynSRf"
      },
      "outputs": [],
      "source": [
        "# @title Initial statistic of dataset\n",
        "# Definition of the paths for training, test and validation set\n",
        "path_training_set = \"/content/chest_xray/train\"\n",
        "path_test_set = \"/content/chest_xray/test\"\n",
        "path_val_set = \"/content/chest_xray/val\"\n",
        "\n",
        "# Counting the Normal and Pneumonia images in the training set\n",
        "train_norm, train_pneu = counter(path_training_set, labels = True); # Count the number of normal images and pneumonia images\n",
        "\n",
        "train_num = counter(path_training_set, labels = False);\n",
        "test_num = counter(path_test_set, labels = False);\n",
        "val_num = counter(path_val_set, labels = False);\n",
        "\n",
        "# Histogram Data\n",
        "classes_hist = ['Normal', 'Pneumonia']\n",
        "counts_hist = [train_norm, train_pneu]\n",
        "colors_hist = ['aquamarine', 'blue']\n",
        "\n",
        "# Pie Chart Data\n",
        "classes_pie = ['Train', 'Test', 'Validation']\n",
        "counts_pie = [train_num, test_num, val_num]\n",
        "colors_pie = ['gold', 'lightcoral', 'lightskyblue']\n",
        "\n",
        "# Create a figure with specific dimensions and subplots\n",
        "fig, ax = plt.subplots(1, 2, figsize=(8, 4))  # Adjust figsize as needed\n",
        "\n",
        "# Plotting the histogram\n",
        "ax[0].bar(classes_hist, counts_hist, color=colors_hist)\n",
        "ax[0].set_title('Histogram distribution')\n",
        "ax[0].set_xlabel('Class')\n",
        "ax[0].set_ylabel('Count')\n",
        "\n",
        "# Plotting the pie chart\n",
        "ax[1].pie(counts_pie, labels=classes_pie, colors=colors_pie, autopct='%1.1f%%', startangle=90)\n",
        "ax[1].set_title('Pie Chart of training, test and validation sizes')\n",
        "\n",
        "plt.tight_layout()  # This adjusts subplot params so that the subplots fit into the figure area.\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nThe number of training images:\", train_num)\n",
        "print(\"The number of test images:\", test_num)\n",
        "print(\"The number of validation images:\", val_num)\n",
        "\n",
        "train_num_norm, train_num_pn = counter(path_training_set, labels = True);\n",
        "test_num_norm, test_num_pn= counter(path_test_set, labels = True);\n",
        "val_num_norm, val_num_pn = counter(path_val_set, labels = True);\n",
        "\n",
        "print(\"\\nThe number of normal images: \", train_num_norm + test_num_norm + val_num_norm)\n",
        "print(\"The number of pneumonia images: \", train_num_pn + test_num_pn + val_num_pn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "mc37Ky-RrrVB"
      },
      "outputs": [],
      "source": [
        "# @title Visualization of two random images\n",
        "# Define a transform element to resize the images as square images and transform in Tensor\n",
        "tr_1 = transforms.ToTensor()                # Convert the image to a tensor\n",
        "\n",
        "# Definition of the paths for training, test and validation set\n",
        "path_training_set = \"/content/chest_xray/train\"\n",
        "path_test_set = \"/content/chest_xray/test\"\n",
        "path_val_set = \"/content/chest_xray/val\"\n",
        "\n",
        "# Instance of classes for the Dataset\n",
        "train_dataset = ChestRayDataset(path_training_set, tr_1) # Create an instance of training class\n",
        "test_dataset = ChestRayDataset(path_test_set, tr_1) # Create an instance of test class\n",
        "val_dataset = ChestRayDataset(path_val_set, tr_1) # Create an instance of validation class\n",
        "\n",
        "# Generate two random numbers to select two random images from training set\n",
        "random_num_1 = np.random.randint(1, len(train_dataset))\n",
        "random_num_2 = np.random.randint(1, len(train_dataset))\n",
        "\n",
        "# Take an image + label randomly\n",
        "img_1, label_1 = train_dataset[random_num_1]\n",
        "img_2, label_2 = train_dataset[random_num_2]\n",
        "\n",
        "# Display the image using matplotlib library\n",
        "plt.figure( figsize=(6,3))\n",
        "plt.subplot(1,2,1)\n",
        "plt.imshow(img_1.permute(1, 2, 0))\n",
        "plt.axis('on')\n",
        "plt.text(0, 0, label_1, fontsize=12, color='black', backgroundcolor='white')\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.imshow(img_2.permute(1, 2, 0))\n",
        "plt.axis('on')\n",
        "plt.text(0, 0, label_2, fontsize=12, color='black', backgroundcolor='white')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "print (\"The shape of the first image is: \", img_1.permute(1, 2, 0).shape)\n",
        "print (\"The shape of the second image is: \", img_2.permute(1, 2, 0).shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "yrfs0O-G5puA"
      },
      "outputs": [],
      "source": [
        "# @title Squarring\n",
        "\n",
        "# Define a transform element to resize the images as square images and transform in Tensor\n",
        "tr_1 = transforms.ToTensor()                 # Convert the image to a tensor\n",
        "\n",
        "# Definition of the paths for training, test and validation set\n",
        "path_training_set_resized = \"/content/chest_xray_resized/train\"\n",
        "path_test_set_resized = \"/content/chest_xray_resized/test\"\n",
        "path_val_set_resized = \"/content/chest_xra_resized/val\"\n",
        "\n",
        "# Instance of classes for the Dataset\n",
        "train_dataset_resized = ChestRayDataset(path_training_set_resized, tr_1) # Create an instance of training class\n",
        "test_dataset_resized = ChestRayDataset(path_test_set_resized, tr_1) # Create an instance of test class\n",
        "val_dataset_resized = ChestRayDataset(path_val_set_resized, tr_1) # Create an instance of validation class\n",
        "\n",
        "# Take an image + label randomly\n",
        "img_1, label_1 = train_dataset_resized[random_num_1]\n",
        "img_2, label_2 = train_dataset_resized[random_num_2]\n",
        "\n",
        "# Display the image using matplotlib library\n",
        "plt.figure( figsize=(6,3))\n",
        "plt.subplot(1,2,1)\n",
        "plt.imshow(img_1.permute(1, 2, 0))\n",
        "plt.axis('on')\n",
        "plt.text(0, 0, label_1, fontsize=12, color='black', backgroundcolor='white')\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.imshow(img_2.permute(1, 2, 0))\n",
        "plt.axis('on')\n",
        "plt.text(0, 0, label_2, fontsize=12, color='black', backgroundcolor='white')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "print (\"The shape of the first image is: \", img_1.permute(1, 2, 0).shape)\n",
        "print (\"The shape of the second image is: \", img_2.permute(1, 2, 0).shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "JNNfT9nMIZCR"
      },
      "outputs": [],
      "source": [
        "# @title Statistics after data augmentation\n",
        "# Definition of the paths for training, test and validation set\n",
        "path_training_set_resized_augmented = \"/content/chest_xray_resized_augmented/train\"\n",
        "\n",
        "# Counting the Normal and Pneumonia images in the training set\n",
        "train_norm, train_pneu = counter(path_training_set_resized_augmented, labels = True); # Count the number of normal images and pneumonia images\n",
        "\n",
        "# Histogram Data\n",
        "classes_hist = ['Normal', 'Pneumonia']\n",
        "counts_hist = [train_norm, train_pneu]\n",
        "colors_hist = ['aquamarine', 'blue']\n",
        "\n",
        "\n",
        "plt.figure(figsize=(5, 3))\n",
        "plt.bar(classes_hist, counts_hist, color=colors_hist)\n",
        "plt.title('Histogram distribution')\n",
        "plt.xlabel('Class')\n",
        "plt.ylabel('Count')\n",
        "plt.show()\n",
        "\n",
        "print(\"The number of normal images:\", train_norm)\n",
        "print(\"The number of pneumonia images:\", train_pneu)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "14ofGPAecuu2"
      },
      "outputs": [],
      "source": [
        "# @title Visualization of two sample in augmented dataset\n",
        "# Instance of classes for the Dataset\n",
        "train_dataset_resized_augmented = ChestRayDataset(path_training_set_resized_augmented, tr_1) # Create an instance of training class\n",
        "\n",
        "# Generate two random numbers to select two random images from training set\n",
        "random_num_1 = np.random.randint(1, len(train_dataset_resized_augmented))\n",
        "random_num_2 = np.random.randint(1, len(train_dataset_resized_augmented))\n",
        "\n",
        "# Take an image + label randomly\n",
        "img_1, label_1 = train_dataset_resized_augmented[random_num_1]\n",
        "img_2, label_2 = train_dataset_resized_augmented[random_num_2]\n",
        "\n",
        "# Display the image using matplotlib library\n",
        "plt.figure(figsize=(6,3))\n",
        "plt.subplot(1,2,1)\n",
        "plt.imshow(img_1.permute(1, 2, 0))\n",
        "plt.axis('on')\n",
        "plt.text(0, 0, label_1, fontsize=12, color='black', backgroundcolor='white')\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.imshow(img_2.permute(1, 2, 0))\n",
        "plt.axis('on')\n",
        "plt.text(0, 0, label_2, fontsize=12, color='black', backgroundcolor='white')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "C15kRm31B3aX"
      },
      "outputs": [],
      "source": [
        "# @title Visualization of two sample in gaussian dataset\n",
        "# Instance of classes for the Dataset\n",
        "path_training_set_gaussian = \"/content/chest_xray_gaussian/train\"\n",
        "train_dataset_gaussian = ChestRayDataset(path_training_set_gaussian, tr_1) # Create an instance of training class\n",
        "\n",
        "# Generate two random numbers to select two random images from training set\n",
        "random_num_1 = np.random.randint(1, len(train_dataset_gaussian))\n",
        "random_num_2 = np.random.randint(1, len(train_dataset_gaussian))\n",
        "\n",
        "# Take an image + label randomly\n",
        "img_1, label_1 = train_dataset_gaussian[random_num_1]\n",
        "img_2, label_2 = train_dataset_gaussian[random_num_2]\n",
        "\n",
        "# Display the image using matplotlib library\n",
        "plt.figure(figsize=(6,3))\n",
        "plt.subplot(1,2,1)\n",
        "plt.imshow(img_1.permute(1, 2, 0))\n",
        "plt.axis('on')\n",
        "plt.text(0, 0, label_1, fontsize=12, color='black', backgroundcolor='white')\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.imshow(img_2.permute(1, 2, 0))\n",
        "plt.axis('on')\n",
        "plt.text(0, 0, label_2, fontsize=12, color='black', backgroundcolor='white')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "vMuVWDug0CKW"
      },
      "outputs": [],
      "source": [
        "# @title Visualization of two sample in gaussian_he dataset\n",
        "# Instance of classes for the Dataset\n",
        "path_training_set_gaussian_he = \"/content/chest_xray_gaussian_he/train\"\n",
        "train_dataset_gaussian_he = ChestRayDataset(path_training_set_gaussian_he, tr_1) # Create an instance of training class\n",
        "\n",
        "# Generate two random numbers to select two random images from training set\n",
        "random_num_1 = np.random.randint(1, len(train_dataset_gaussian_he))\n",
        "random_num_2 = np.random.randint(1, len(train_dataset_gaussian_he))\n",
        "\n",
        "# Take an image + label randomly\n",
        "img_1, label_1 = train_dataset_gaussian_he[random_num_1]\n",
        "img_2, label_2 = train_dataset_gaussian_he[random_num_2]\n",
        "\n",
        "# Display the image using matplotlib library\n",
        "plt.figure(figsize=(6,3))\n",
        "plt.subplot(1,2,1)\n",
        "plt.imshow(img_1.permute(1, 2, 0))\n",
        "plt.axis('on')\n",
        "plt.text(0, 0, label_1, fontsize=12, color='black', backgroundcolor='white')\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.imshow(img_2.permute(1, 2, 0))\n",
        "plt.axis('on')\n",
        "plt.text(0, 0, label_2, fontsize=12, color='black', backgroundcolor='white')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jk0Qf3nslAPT"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PETZ7JYemIzn"
      },
      "source": [
        "## Train functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPug6dfQhrfK"
      },
      "source": [
        "Training loop from [documentation](https://pytorch.org/tutorials/beginner/introyt/trainingyt.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q2fYnRpxSfFZ"
      },
      "outputs": [],
      "source": [
        "def save_metrics_to_csv(metrics_dict, name_dict):\n",
        "\n",
        "    with open(name_dict, 'w') as f:\n",
        "        json.dump(metrics_dict,f)\n",
        "\n",
        "    print(f\"{Color.GREEN}{name_dict} saved{Color.RESET}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "72PGUGyoSgc6"
      },
      "outputs": [],
      "source": [
        "def load_metrics_from_csv(path):\n",
        "\n",
        "    if os.path.exists(path):\n",
        "        with open(path, 'r') as f:\n",
        "            loaded_data = json.load(f)\n",
        "\n",
        "        print(f\"{Color.GREEN}{path} loaded{Color.RESET}\")\n",
        "        return loaded_data\n",
        "    else:\n",
        "        print(f\"{Color.RED}{path} not loaded, return {{}}{Color.RESET}\")\n",
        "        return {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kiho-pu6Z-W-"
      },
      "outputs": [],
      "source": [
        "def my_plot(train_dict, x_axis1, y_axis1, val_dict, x_axis2, y_axis2, title,\n",
        "            marker1='o', linestyle1='-', color1='b',\n",
        "            marker2='x', linestyle2='-', color2='r'):\n",
        "\n",
        "    x_values1 = train_dict[x_axis1]\n",
        "    y_values1 = train_dict[y_axis1]\n",
        "\n",
        "    x_values2 = val_dict[x_axis2]\n",
        "    y_values2 = val_dict[y_axis2]\n",
        "\n",
        "    plt.figure(figsize=(8, 5))\n",
        "\n",
        "    # Plot the first function\n",
        "    plt.plot(x_values1, y_values1, marker=marker1, linestyle=linestyle1, color=color1, label=\"Training \" + y_axis1)\n",
        "\n",
        "    # Plot the second function\n",
        "    plt.plot(x_values2, y_values2, marker=marker2, linestyle=linestyle2, color=color2, label=\"Validation \" + y_axis2)\n",
        "\n",
        "    plt.xlabel(f\"{x_axis1}\")\n",
        "    plt.ylabel(f\"{y_axis1}\")\n",
        "    plt.title(f\"{title}\")\n",
        "    plt.legend()\n",
        "\n",
        "    # Ensure the x-axis has integer ticks\n",
        "    x_min = min(min(x_values1), min(x_values2))\n",
        "    x_max = max(max(x_values1), max(x_values2))\n",
        "    plt.xticks(range(x_min, x_max + 1))\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FelC6-p8xMgu"
      },
      "outputs": [],
      "source": [
        "def plot_confusion_matrix(cm, class_names, normalize=False, title='Confusion Matrix', cmap='Oranges'):\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "    plt.figure(figsize=(10, 7))\n",
        "    sns.heatmap(cm, annot=True, fmt='.2f' if normalize else 'd', cmap=cmap,\n",
        "                xticklabels=class_names, yticklabels=class_names)\n",
        "\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.ylabel('True label')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MDqbhMpUtpWU"
      },
      "outputs": [],
      "source": [
        "def calculate_metrics(model, outputs, labels, metrics_dict, store_to_dict=True, stamp=True):\n",
        "\n",
        "    accuracy_value = model.accuracy(outputs, labels)\n",
        "    precision_value = model.precision(outputs, labels)\n",
        "    recall_value = model.recall(outputs, labels)\n",
        "    auroc_value = model.auroc(outputs, labels)\n",
        "    f1score_value = model.f1score(outputs, labels)\n",
        "\n",
        "    accuracy_value_item = accuracy_value.item() * 100\n",
        "    precision_value_item = precision_value.item() * 100\n",
        "    recall_value_item = recall_value.item() * 100\n",
        "    auroc_value_item = auroc_value.item() * 100\n",
        "    f1score_value_item = f1score_value.item() * 100\n",
        "\n",
        "    metrics_current_dict = {\n",
        "      'accuracy': accuracy_value_item,\n",
        "      'precision': precision_value_item,\n",
        "      'recall': recall_value_item,\n",
        "      'auroc': auroc_value_item,\n",
        "      'f1score': f1score_value_item\n",
        "    }\n",
        "\n",
        "    if store_to_dict == True:\n",
        "      metrics_dict['accuracy'].append(accuracy_value_item)\n",
        "      metrics_dict['precision'].append(precision_value_item)\n",
        "      metrics_dict['recall'].append(recall_value_item)\n",
        "      metrics_dict['auroc'].append(auroc_value_item)\n",
        "      metrics_dict['f1score'].append(f1score_value_item)\n",
        "\n",
        "\n",
        "    metrics_string = f\"\\n\\taccuracy: {accuracy_value_item:.2f}, precision: {precision_value_item:.2f}, recall: {recall_value_item:.2f}, auroc: {auroc_value_item:.2f}, f1score: {f1score_value_item:.2f}\"\n",
        "\n",
        "    if stamp == True:\n",
        "      print(metrics_string)\n",
        "    return metrics_current_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KlmSrxH8hIuV"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(model, epoch_index, training_loader, metrics_train):\n",
        "    running_loss = 0.\n",
        "\n",
        "    # Here, we use enumerate(training_loader) instead of\n",
        "    # iter(training_loader) so that we can track the batch\n",
        "    # index and do some intra-epoch reporting\n",
        "\n",
        "    n_batch = len(training_loader)  # number of batches in training loader\n",
        "    loss_epoch_array = []\n",
        "\n",
        "    with tqdm(total=n_batch, desc=f'Epoch {epoch_index} ') as pbar:\n",
        "        for i, data in enumerate(training_loader):\n",
        "\n",
        "            # Every data instance is an input + label pair\n",
        "            inputs, labels = data\n",
        "\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            labels = labels.unsqueeze(dim=1)     # reshaping as inputs, from [0, 1] to [[0], [1]]\n",
        "            labels = labels.type(torch.float32)  # preditions of network are floats\n",
        "\n",
        "            # Zero your gradients for every batch!\n",
        "            model.optimizer.zero_grad()\n",
        "\n",
        "            # Make predictions for this batch\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Compute the loss and its gradients\n",
        "            loss = model.loss_fn(outputs, labels)\n",
        "            loss_epoch_array.append(loss.item())\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            # Adjust learning weights\n",
        "            model.optimizer.step()\n",
        "\n",
        "            # Gather data and report\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            if i % batch_size == 0 and i>0:\n",
        "                last_loss = running_loss / batch_size #1000 # loss per batch\n",
        "                print(f'\\r\\tbatch {i+1} loss: {last_loss}', end='')\n",
        "                running_loss = 0.\n",
        "\n",
        "                del inputs, labels, outputs, loss\n",
        "                gc.collect()\n",
        "\n",
        "                if device != \"cpu\":\n",
        "                    torch.cuda.empty_cache()\n",
        "\n",
        "            # compute metrics in last epoch\n",
        "            if i == n_batch-1:\n",
        "                print(\"\\n\\tTraining metrics:\", end='')\n",
        "                _ = calculate_metrics(model, outputs, labels, metrics_train)\n",
        "\n",
        "            pbar.update(1)\n",
        "\n",
        "        # Svuota la cache CUDA dopo ogni epoca\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    loss_epoch_array = np.array(loss_epoch_array)\n",
        "    loss_avg = np.mean(loss_epoch_array)\n",
        "    return loss_avg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hVTkjpNugwlg"
      },
      "outputs": [],
      "source": [
        "def training(model, epochs, training_loader, validation_loader, metrics_train, metrics_validation):\n",
        "    model.set_training_mode()\n",
        "\n",
        "    best_vloss = 1000000\n",
        "    current_accuracy = 0.0\n",
        "    EARLY_STOPPING = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        # Make sure gradient tracking is on, and do a pass over the data\n",
        "        model.set_training_mode()\n",
        "        avg_loss = train_one_epoch(model, epoch, training_loader, metrics_train)\n",
        "\n",
        "        metrics_train['loss'].append(avg_loss)\n",
        "        metrics_train['epoch'].append(epoch)\n",
        "\n",
        "        running_vloss = 0.0\n",
        "\n",
        "        model.set_eval_mode()\n",
        "        validating(\n",
        "            model=model,\n",
        "            epoch=epoch,\n",
        "            validation_loader=validation_loader,\n",
        "            metrics_validation=metrics_validation\n",
        "          )\n",
        "\n",
        "        save_metrics_to_csv(metrics_train, '/content/train_metrics.json')\n",
        "        save_metrics_to_csv(metrics_validation, '/content/validation_metrics.json')\n",
        "\n",
        "        avg_vloss = metrics_validation['loss'][-1] # / ( len(validation_loader) + 1 )\n",
        "        avg_loss_epoch = metrics_train['loss'][-1] # / ( len(training_loader) + 1 )\n",
        "        print(f\"\\taverage loss epoch: {avg_loss_epoch:.4f}, average loss validation: {avg_vloss:.4f}\\n\")\n",
        "\n",
        "        if avg_vloss < best_vloss:\n",
        "            best_vloss = avg_loss\n",
        "            model_path = model.model_name\n",
        "\n",
        "            print(f\"\\t{Color.BLUE}Saving best model on validation set{Color.RESET}\")\n",
        "            #model.save(epoch, stamp=True)\n",
        "            model.save('best', stamp=True)\n",
        "\n",
        "        if epoch >= 1:\n",
        "            loss_1 = metrics_validation['loss'][-2]\n",
        "            loss_2 = metrics_validation['loss'][-1]\n",
        "\n",
        "            if abs(loss_1 - loss_2) <= global_var['early_stop_threshold']:\n",
        "                EARLY_STOPPING += 1\n",
        "                print(f\"{Color.RED}Early stop counter: {EARLY_STOPPING}/{global_var['early_stop_patience']}{Color.RESET}\")\n",
        "            else:\n",
        "                EARLY_STOPPING = 0\n",
        "\n",
        "        if EARLY_STOPPING >= global_var['early_stop_patience']:\n",
        "            print(f\"{Color.RED}Early Stopping{Color.RESET}\")\n",
        "            break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Thhm--Fm7JQa"
      },
      "outputs": [],
      "source": [
        "def validating(model, epoch, validation_loader, metrics_validation):\n",
        "\n",
        "  model.set_eval_mode()\n",
        "  print()\n",
        "  # Disable gradient computation and reduce memory consumption.\n",
        "  with torch.no_grad():\n",
        "      validation_metrics_array = []\n",
        "      vloss_array = []\n",
        "      for i, vdata in enumerate(validation_loader):\n",
        "\n",
        "          print(f\"\\r\\tValidation batch: {i+1}/{len(validation_loader)}\", end='')\n",
        "\n",
        "          vinputs, vlabels = vdata\n",
        "\n",
        "          vinputs = vinputs.to(device)\n",
        "          vlabels = vlabels.to(device)\n",
        "\n",
        "          vlabels = vlabels.unsqueeze(dim=1)    # [0, 1, 0, 1] -> [[0], [1], [0], [1]]\n",
        "          vlabels = vlabels.type(torch.float32)\n",
        "\n",
        "          voutputs = model(vinputs)\n",
        "          vloss = model.loss_fn(voutputs, vlabels)\n",
        "\n",
        "          vloss_array.append(vloss.item())\n",
        "\n",
        "          current_validation_metrics = calculate_metrics(model, voutputs, vlabels, metrics_validation, store_to_dict=False, stamp=False)\n",
        "          validation_metrics_array.append(current_validation_metrics)\n",
        "\n",
        "      #### print validation metrics\n",
        "      #print(f\"\\r\\n\\tValidation\")\n",
        "\n",
        "      sums = {key: 0.0 for key in validation_metrics_array[0].keys()}\n",
        "\n",
        "      # calculate mean values\n",
        "      for metrics_dict in validation_metrics_array:\n",
        "          for key in sums.keys():\n",
        "              sums[key] += metrics_dict[key]\n",
        "\n",
        "      num_dicts = len(validation_metrics_array)\n",
        "      averages = {key: sums[key] / num_dicts for key in sums.keys()}\n",
        "\n",
        "      # print metrics\n",
        "      metrics_string = (\n",
        "          f\"\\n\\taccuracy: {averages['accuracy']:.2f}, \"\n",
        "          f\"precision: {averages['precision']:.2f}, \"\n",
        "          f\"recall: {averages['recall']:.2f}, \"\n",
        "          f\"auroc: {averages['auroc']:.2f}, \"\n",
        "          f\"f1score: {averages['f1score']:.2f}\"\n",
        "      )\n",
        "\n",
        "      for k in averages.keys():\n",
        "        metrics_validation[k].append(averages[k])\n",
        "\n",
        "      print(metrics_string)\n",
        "\n",
        "      metrics_validation['loss'].append( np.mean(vloss_array))\n",
        "      metrics_validation['epoch'].append(epoch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-4RYAhHqjHPc"
      },
      "outputs": [],
      "source": [
        "def testing(model, test_loader, metrics_test):\n",
        "\n",
        "    model.set_eval_mode()\n",
        "    running_tloss = 0\n",
        "\n",
        "    n_batch = len(test_loader)  # number of batches in training loader\n",
        "    batch_size = global_var['batch_size']\n",
        "    n_files = n_batch*batch_size\n",
        "\n",
        "    with tqdm(total=n_files, desc=f'Testing ') as pbar:\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for i, tdata in enumerate(test_loader):\n",
        "\n",
        "                tinputs, tlabels = tdata\n",
        "\n",
        "                tinputs = tinputs.to(device)\n",
        "                tlabels = tlabels.to(device)\n",
        "\n",
        "                tlabels = tlabels.unsqueeze(dim=1)    # [0, 1, 0, 1] -> [[0], [1], [0], [1]]\n",
        "                tlabels = tlabels.type(torch.float32)\n",
        "\n",
        "                if i == 0:\n",
        "                    outputs_tensor = torch.empty(0, tlabels.shape[1], device=device)\n",
        "                    labels_tensor = torch.empty(0, tlabels.shape[1], device=device)\n",
        "\n",
        "                toutputs = model(tinputs)\n",
        "                tloss = model.loss_fn(toutputs, tlabels)\n",
        "\n",
        "                metrics_test['loss'].append(tloss.item())\n",
        "                metrics_test['iteration'].append(i)\n",
        "\n",
        "                running_tloss += tloss\n",
        "\n",
        "                outputs_tensor = torch.cat((outputs_tensor, toutputs), dim=0)\n",
        "                labels_tensor = torch.cat((labels_tensor, tlabels), dim=0)\n",
        "\n",
        "                pbar.update(tlabels.shape[0])\n",
        "\n",
        "            _ = calculate_metrics(model, outputs_tensor, labels_tensor, metrics_test)\n",
        "\n",
        "            labels_tensor = labels_tensor.cpu()\n",
        "            labels_tensor = labels_tensor.squeeze(dim=1) # [[0], [1], [0], [1]] -> [0, 1, 0, 1]\n",
        "            true_labels_np = labels_tensor.numpy().astype(int)\n",
        "\n",
        "            outputs_tensor = outputs_tensor.cpu()\n",
        "            outputs_tensor = outputs_tensor.squeeze(dim=1) # [[0], [1], [0], [1]] -> [0, 1, 0, 1]\n",
        "            predicted_labels_np = outputs_tensor.numpy()\n",
        "            threshold = 0.5\n",
        "            predicted_labels_np = (predicted_labels_np > threshold).astype(int)\n",
        "\n",
        "    return true_labels_np, predicted_labels_np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aah1c8VdYl8x"
      },
      "outputs": [],
      "source": [
        "print(device)\n",
        "batch_size = global_var[\"batch_size\"]\n",
        "train_epochs = global_var[\"train_epochs\"]\n",
        "tf_to_tensor = transforms.ToTensor()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XW7LJ74xETHq"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qgDn7HElF_XK"
      },
      "outputs": [],
      "source": [
        "use_dataset_path = os.path.join(global_var['extract_directory'], global_var['dataset_name_gaussian_clahe'])\n",
        "subset_dimension = global_var['subset_dim_percentage']\n",
        "use_subset = global_var['use_subset']\n",
        "\n",
        "create_subset(use_dataset_path , subset_dimension)\n",
        "\n",
        "if use_subset == True:\n",
        "  use_dataset_path = os.path.join(global_var['extract_directory'], global_var['dataset_name_gaussian_clahe']) + \"_subset\"\n",
        "\n",
        "unify_folder(use_dataset_path)\n",
        "\n",
        "full_dataset = ChestRayDataset(\n",
        "    file_path= use_dataset_path,\n",
        "    transform=tf_to_tensor\n",
        ")\n",
        "\n",
        "split_values = [round(len(full_dataset) * ratio) for ratio in global_var['split_value']]\n",
        "train_size, test_size, val_size = split_values[0], split_values[1], split_values[2]\n",
        "train_size = train_size + ( len(full_dataset) - sum(split_values) ) # adding value for due to round problems\n",
        "\n",
        "dt_train_dataset, dt_val_dataset, dt_test_dataset = random_split(full_dataset, [train_size, val_size, test_size])\n",
        "\n",
        "print(f\"train dt: {len(dt_train_dataset)}, samples: {train_size}\")\n",
        "print(f\"test dt: {len(dt_test_dataset)},  samples: {test_size}\")\n",
        "print(f\"val dt: {len(dt_val_dataset)},  samples: {val_size}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xgr5hKfRm_gn"
      },
      "outputs": [],
      "source": [
        "dt_train_dataloader = DataLoader(\n",
        "    dt_train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True\n",
        "    )\n",
        "\n",
        "dt_test_dataloader = DataLoader(\n",
        "    dt_test_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True\n",
        "    )\n",
        "\n",
        "dt_val_dataloader = DataLoader(\n",
        "    dt_val_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eIcYCX14F_XK"
      },
      "outputs": [],
      "source": [
        "gc.collect()\n",
        "if device != \"cpu\":\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "jam_network = JAM(\n",
        "    channels=1664,\n",
        "    device=device,\n",
        "    model_name=global_var['extract_directory'] + \"/\" + \"jam_net\"\n",
        "    )\n",
        "\n",
        "#jam_network.load() # Decomment this to load precedent weights\n",
        "jam_network.to(device)\n",
        "jam_network.set_eval_mode()\n",
        "\n",
        "gc.collect()\n",
        "if device != \"cpu\":\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "train_metrics = {}\n",
        "train_metrics['epoch'] = []\n",
        "train_metrics['loss'] = []\n",
        "train_metrics['accuracy'] = []\n",
        "train_metrics['precision'] = []\n",
        "train_metrics['recall'] = []\n",
        "train_metrics['auroc'] = []\n",
        "train_metrics['f1score'] = []\n",
        "\n",
        "\n",
        "validation_metrics = {}\n",
        "validation_metrics['epoch'] = []\n",
        "validation_metrics['loss'] = []\n",
        "validation_metrics['accuracy'] = []\n",
        "validation_metrics['precision'] = []\n",
        "validation_metrics['recall'] = []\n",
        "validation_metrics['auroc'] = []\n",
        "validation_metrics['f1score'] = []\n",
        "\n",
        "\n",
        "print(\"You are using for training: \", device)\n",
        "\n",
        "training(jam_network, train_epochs, dt_train_dataloader, dt_val_dataloader, train_metrics, validation_metrics)\n",
        "jam_network.save('last_epoch')\n",
        "\n",
        "save_metrics_to_csv(train_metrics, '/content/train_metrics.json')\n",
        "save_metrics_to_csv(validation_metrics, '/content/validation_metrics.json')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TEST: last epoch"
      ],
      "metadata": {
        "id": "YBVxKQ3YpRP_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hRN3BOTiaR_m"
      },
      "outputs": [],
      "source": [
        "test_metrics = {}\n",
        "test_metrics['iteration'] = []\n",
        "test_metrics['loss'] = []\n",
        "test_metrics['accuracy'] = []\n",
        "test_metrics['precision'] = []\n",
        "test_metrics['recall'] = []\n",
        "test_metrics['auroc'] = []\n",
        "test_metrics['f1score'] = []\n",
        "\n",
        "jam_network.load('last_epoch')\n",
        "true_labels, predicted_labels = testing(jam_network, dt_test_dataloader, test_metrics)\n",
        "save_metrics_to_csv(test_metrics, '/content/test_metrics.json')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGrdQ0RRp1oi"
      },
      "source": [
        "## PLOT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UqibyLgZYrB"
      },
      "source": [
        "**LOSS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IB8xj9eFp1MZ"
      },
      "outputs": [],
      "source": [
        "my_plot(train_metrics, 'epoch', 'loss', validation_metrics, 'epoch', 'loss', title=\"JAM-net Loss\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70-RcB3OZ8QF"
      },
      "source": [
        "**ACCURACY**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FvM3pZT4SZym"
      },
      "outputs": [],
      "source": [
        "my_plot(train_metrics, 'epoch', 'accuracy', validation_metrics, 'epoch', 'accuracy', title=\"JAM-net Accuracy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ihhEFByZbIj"
      },
      "source": [
        "**CONFUSION MATRIX**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xnJCUk2IZgcC"
      },
      "outputs": [],
      "source": [
        "class_names = ['Normal', 'Pneumonia']\n",
        "cm = confusion_matrix(true_labels, predicted_labels)\n",
        "plot_confusion_matrix(cm, class_names, normalize=False, title='FA-net Confusion Matrix', cmap='Oranges')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TEST: best model"
      ],
      "metadata": {
        "id": "X2fPPPMPpT8m"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7WnYzsDSpT8u"
      },
      "outputs": [],
      "source": [
        "test_metrics_best = {}\n",
        "test_metrics_best['iteration'] = []\n",
        "test_metrics_best['loss'] = []\n",
        "test_metrics_best['accuracy'] = []\n",
        "test_metrics_best['precision'] = []\n",
        "test_metrics_best['recall'] = []\n",
        "test_metrics_best['auroc'] = []\n",
        "test_metrics_best['f1score'] = []\n",
        "\n",
        "jam_network.load('best')\n",
        "true_labels, predicted_labels = testing(jam_network, dt_test_dataloader, test_metrics_best)\n",
        "save_metrics_to_csv(test_metrics, '/content/test_metrics_best.json')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivH8h2vmUlPm"
      },
      "source": [
        "# References\n",
        "[1]  [Arshia Rehman et al.\n",
        "Review on chest patogies detection systems using deep learning techniques\n",
        "Artificial Intelligence Review,\n",
        "2023.](https://www.researchgate.net/publication/369379105_Review_on_chest_pathogies_detection_systems_using_deep_learning_techniques)\n",
        "\n",
        "\n",
        "[2] [Ziming Liu et al.\n",
        "KAN: Kolmogorov-Arnold Networks\n",
        "2024.](https://arxiv.org/abs/2404.19756)\n",
        "\n",
        "\n",
        "[3] [Ayush Roy et al.\n",
        "FA-net: A Fuzzy Attention-aided Deep Neural Network for Pneumonia Detection in Chest X-Rays\n",
        "2024.](https://arxiv.org/pdf/2406.15117)\n",
        "\n",
        "[4] [Labeled Optical Coherence Tomography (OCT) and Chest X-Ray Images for Classification](https://data.mendeley.com/datasets/rscbjbr9sj/2)\n",
        "\n",
        "[5] [COLIN P.D. BIRCH,\n",
        "A New Generalized Logistic Sigmoid Growth Equation Compared with the Richards Growth Equation,\n",
        "Annals of Botany,\n",
        "Volume 83, Issue 6,\n",
        "1999](https://www.sciencedirect.com/science/article/pii/S0305736499908776)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "uZQ3yrZuZsXx",
        "yt-OTzpxBk3q",
        "ecVBSRTQBmdb",
        "LDA_BRDlugNi",
        "ztmhJrCtndIN",
        "GgQdo_c94IG5",
        "09Nrq6xD9viW",
        "GaUnYpNGxwXj",
        "vgiZw8VRqNqf",
        "j4uFWWdnvMeh",
        "d9mzIlG5MXGK",
        "_HmvdEe24KbO",
        "_sFf8oHnnS8s",
        "PETZ7JYemIzn",
        "GGrdQ0RRp1oi"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "20b667a6d1154779a12bb72950baffa9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f810889e570d4240930a2a9b7f439be1",
              "IPY_MODEL_c67c1b1015c6432f9daf72a02fdf9f2b",
              "IPY_MODEL_bc1677055b42462f844f7143dce9f959"
            ],
            "layout": "IPY_MODEL_81c56c5ee7e94c178a29b9b26f67d994"
          }
        },
        "f810889e570d4240930a2a9b7f439be1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_adeb4d702f6941cf931a0824b3eb7446",
            "placeholder": "​",
            "style": "IPY_MODEL_22cd9fed21694334ae025f18a4ce35ba",
            "value": "Unzipping files: 100%"
          }
        },
        "c67c1b1015c6432f9daf72a02fdf9f2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fb2f320e442c49c69967e9fd3f4a1da0",
            "max": 17591,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_888bc10fc42646f092fbf85b216c4baf",
            "value": 17591
          }
        },
        "bc1677055b42462f844f7143dce9f959": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ac70531a2f6a4ae981e80f6269a4e6e4",
            "placeholder": "​",
            "style": "IPY_MODEL_f50c79101d024da79e614a0defdc0a53",
            "value": " 17591/17591 [00:22&lt;00:00, 999.81it/s]"
          }
        },
        "81c56c5ee7e94c178a29b9b26f67d994": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "adeb4d702f6941cf931a0824b3eb7446": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "22cd9fed21694334ae025f18a4ce35ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fb2f320e442c49c69967e9fd3f4a1da0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "888bc10fc42646f092fbf85b216c4baf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ac70531a2f6a4ae981e80f6269a4e6e4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f50c79101d024da79e614a0defdc0a53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "86abced6216f4d3a9854e81b0ebd523a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_776dd6c978dc4c0d9f3ee8b52244dbfb",
              "IPY_MODEL_7ae83c71e0084ef08f5a84cb1075a79d",
              "IPY_MODEL_7bf2f17e336341a4bd827485850a5eb3"
            ],
            "layout": "IPY_MODEL_71d7d8ae966a457aa26bd19f8b116647"
          }
        },
        "776dd6c978dc4c0d9f3ee8b52244dbfb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7a9fcdaeb4df4b4ba6b52960e90f3ffe",
            "placeholder": "​",
            "style": "IPY_MODEL_87be9c3a15394495be44c9bc61b81caf",
            "value": "Resizing train images:  91%"
          }
        },
        "7ae83c71e0084ef08f5a84cb1075a79d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_73615a20a3e74f8bb94ac8f461bf46db",
            "max": 5216,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_93aecdf2f13e4235a92a289918dc6340",
            "value": 4761
          }
        },
        "7bf2f17e336341a4bd827485850a5eb3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4f645a3c548648dc9f58fd89a8169060",
            "placeholder": "​",
            "style": "IPY_MODEL_28391d760c594e4eaad89baf5318fcb7",
            "value": " 4761/5216 [00:50&lt;00:04, 103.97image/s]"
          }
        },
        "71d7d8ae966a457aa26bd19f8b116647": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a9fcdaeb4df4b4ba6b52960e90f3ffe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "87be9c3a15394495be44c9bc61b81caf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "73615a20a3e74f8bb94ac8f461bf46db": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "93aecdf2f13e4235a92a289918dc6340": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4f645a3c548648dc9f58fd89a8169060": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "28391d760c594e4eaad89baf5318fcb7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}