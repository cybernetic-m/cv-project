{"cells":[{"cell_type":"markdown","metadata":{"id":"uZQ3yrZuZsXx"},"source":["# Initialization"]},{"cell_type":"markdown","metadata":{"id":"yt-OTzpxBk3q"},"source":["### Installation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pqZNQdxgZe3v"},"outputs":[],"source":["%%capture\n","!pip install gdown tqdm\n","!pip install torchvision --quiet\n","!pip install pykan\n","!pip install torchmetrics\n","!pip install scikit-learn"]},{"cell_type":"markdown","metadata":{"id":"ecVBSRTQBmdb"},"source":["### Import of libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ODbC4xF6Zym4"},"outputs":[],"source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","import torchvision.models as models\n","from torchvision import transforms\n","from torchvision.transforms import ToPILImage\n","from torchvision.io import read_image\n","from torch.utils.data import random_split, DataLoader, Dataset\n","\n","from torchmetrics.classification import BinaryAccuracy\n","from torchmetrics.classification import BinaryPrecision\n","from torchmetrics.classification import BinaryRecall\n","from torchmetrics.classification import BinaryAUROC\n","from torchmetrics.classification import BinaryF1Score\n","\n","from kan import KAN\n","#import pymrmr\n","\n","import cv2\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","from sklearn.decomposition import PCA\n","\n","import os\n","import gdown\n","import zipfile\n","import shutil\n","import gc\n","from tqdm.notebook import tqdm\n","import time\n","from google.colab import drive\n","\n","import random\n"]},{"cell_type":"markdown","metadata":{"id":"86RET5RaaNKi"},"source":["### GLOBAL DEFINITIONS\n","\n","Global variables where there are stored hyperparameters for the training and link, paths for the dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YswtWHEPaLP7"},"outputs":[],"source":["global_var = {\n","    # Dataset\n","    'link_download_prefix' : 'https://drive.google.com/uc?export=download&id=YOUR_FILE_ID', # Generic link to download from Drive the file with id \"YOUR_FILE_ID\"\n","    \"link_dataset\": \"https://drive.google.com/file/d/10un_W7teXQy9LOB1uqd0e6VJ46UFNRza/view?usp=drive_link0\", # The ID is \"10un_W7teXQy9LOB1uqd0e6VJ46UFNRza\" of the file\n","    \"path_download_dataset\": \"/content/dataset.zip\", # Path of the runtime directory of Colab with the zipped dataset\n","    \"extract_directory\": \"/content\", # Directory where we want to extract the zipped dataset\n","    \"dataset_name\": \"chest_xray\", # Name of the dataset\n","    \"dataset_name_resized\": \"chest_xray_resized\", # Name of the dataset resized\n","    \"dataset_name_resized_augmented\": \"chest_xray_resized_augmented\", # Name of the dataset resized\n","    \"dataset_name_heatmap\": \"chest_xray_heatmap\",\n","    \"dataset_name_gaussian\": \"chest_xray_gaussian\",\n","\n","    # Preprocessing parameters\n","    \"resizing_dim\": (256,256),\n","    \"colormap_type\": cv2.COLORMAP_TWILIGHT,\n","    \"smoothing_value\": 5,\n","\n","    # Network: KAN module\n","    'input_dim': 10,       # input dimension of KAN\n","    'hidden_dim': [40, 4], # hidden layers of KAN\n","    'output_dim': 1,       # output layer, binary classification\n","\n","    # Train\n","    'batch_size': 16,\n","    \"train_epochs\": 3,\n","\n","    # logger parameters\n","    \"log_directory\": \"/content/log_plots\",\n","\n","    # hyperparamter search\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4IUB3lR18JwU"},"outputs":[],"source":["device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""]},{"cell_type":"markdown","metadata":{"id":"ztmhJrCtndIN"},"source":["### Color Class\n","This class is used for printing text in different color. In particular each attribute in the Color class corresponds to a specific text color or formatting style."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-Q5lxLgy3E-z"},"outputs":[],"source":["class Color:\n","    RED = '\\033[91m'\n","    GREEN = '\\033[92m'\n","    YELLOW = '\\033[93m'\n","    BLUE = '\\033[94m'\n","    MAGENTA = '\\033[95m'\n","    CYAN = '\\033[96m'\n","    WHITE = '\\033[97m'\n","    RESET = '\\033[0m' #RESET: Resets all text formatting and color to default\n","    BOLD = '\\033[1m' #BOLD: Makes the text bold\n","    UNDERLINE = '\\033[4m' #UNDERLINE: Underlines the text"]},{"cell_type":"markdown","metadata":{"id":"Y80hfJRBqsLM"},"source":["Random Seed"]},{"cell_type":"markdown","metadata":{"id":"GgQdo_c94IG5"},"source":["# Dataset Setup"]},{"cell_type":"markdown","metadata":{"id":"09Nrq6xD9viW"},"source":["## Initial setup"]},{"cell_type":"markdown","metadata":{"id":"dBFYnapNrZVx"},"source":["\n","The function, given the link of the dataset in google drive, download it.\n","\n","Args:\n","        \n","1.   **link_dataset** = link to zip files\n","2.   **delete_zip_file** = {True/False}, use to remove the zip file once extracted the content\n","\n","Note: the link my be public or gdown can't download it\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Mk4TdLIY5CkF"},"outputs":[],"source":["def download_dataset(link_dataset, delete_zip_file=False):\n","\n","    # Creating string for download\n","\n","    link_download_base =  global_var['link_download_prefix'] # General link for the download from Drive (with \"YOUR_FILE_ID\")\n","    id_dataset = link_dataset.split(\"/view\")[0].split(\"/\")[-1]\n","    dataset_download_link = link_download_base.replace(\"YOUR_FILE_ID\", id_dataset) # Replace the \"YOUR_FILE_ID\" part with the extracted id_dataset\n","\n","    path_download_dataset = global_var['path_download_dataset'] # Path of the zip file that contain the dataset\n","    dataset_name = global_var['dataset_name'] # Dataset's Name\n","    extract_directory = global_var['extract_directory'] # Directory of the GColab Runtime where we want to extract the dataset\n","\n","    # Downloading zip file\n","    try:\n","        if not( os.path.exists(dataset_name) ): # check is the dataset is already downloaded\n","            gdown.download(\n","                url=dataset_download_link, # Link that we extract with the file ID\n","                output=path_download_dataset, # Path of the runtime Colab to save the zip file\n","                quiet=False # Output to check the download\n","            )\n","            print(f\"{Color.GREEN}\\nZip file downloaded{Color.RESET}\")\n","        else:\n","            print(f\"{Color.GREEN}Zip file already downloaded{Color.RESET} \")\n","    except Exception as error:\n","        print(f\"{Color.RED}Download went wrong!\\nerror:{Color.RESET}\")\n","        print(error)\n","\n","\n","    # Unzipping the file\n","    '''\n","    In this part we want to unzip the zip file contain the dataset\n","    '''\n","    try:\n","        if not( os.path.exists(dataset_name) ):\n","\n","          # This open the file zip in path_download_dataset in 'r' = read mode.\n","          # The file is created from the ZipFile class contained in zipfile module\n","          # and the name of the instance will be zipfile\n","\n","            with zipfile.ZipFile(path_download_dataset, 'r') as zip_file:\n","\n","                n_files = len(zip_file.namelist()) # number of files and directories inside the zip\n","\n","                with tqdm(total=n_files, desc='Unzipping files') as pbar: # It's a bar to track the unzip process, we pass the number of total files\n","                    for file_name in zip_file.namelist():\n","                        zip_file.extract(member=file_name, path=extract_directory) # Extract the file_name (iteration over all the files) in the path extract_directory\n","                        pbar.update(1) # increment the progress bar of 1 unit for each extraction\n","\n","            print(f\"{Color.GREEN}Dataset {dataset_name} unzipped{Color.RESET}\")\n","        else:\n","            print(f\"{Color.GREEN}Dataset {dataset_name} already unzipped{Color.RESET} \")\n","    except Exception as error:\n","        print(f\"{Color.RED}Unzip went wrong!\\nerror:{Color.RESET}\")\n","        print(error)\n","\n","    # Removing zip file\n","    '''\n","    In this part we remove the zip file if the flag\n","    delete_zip_file is True (args of the function)\n","    '''\n","    try:\n","        if delete_zip_file == True:\n","            ! rm {path_download_dataset}\n","            print(f\"{Color.GREEN}File {path_download_dataset} removed{Color.RESET}\")\n","    except Exception as error:\n","        print(f\"{Color.RED}Could not remove zip file, pass!\\nerror:{Color.RESET}\")\n","        print(error)\n","        pass"]},{"cell_type":"markdown","metadata":{"id":"eYJ7AV4CyyF5"},"source":["Given the dataset, it merge and shuffle it."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ASW5hS0Tvg-0"},"outputs":[],"source":["def merge_dataset():\n","\n","    dataset_name = global_var['dataset_name'] # Name of the directory\n","    extract_directory = global_var['extract_directory'] # Directory of destination where we put the shuffled dataset\n","    current_path = os.path.join(extract_directory, dataset_name) # Create the path \"/content/chest_xray\"\n","    origin_dataset = os.path.join(current_path,dataset_name) # Create the path \"/content/chest_xray/chest_xray\"\n","\n","    # Create a list of this type -> ['/content/chest_xray/train', '/content/chest_xray/test', '/content/chest_xray/val']:\n","    sub_directory = [ os.path.join(current_path, s) for s in ['train', 'test', 'val']]\n","    classes = ['PNEUMONIA', 'NORMAL']\n","\n","    try:\n","        for sub_dir in sub_directory:\n","\n","            for c in classes:\n","                sub_dir_c = os.path.join(sub_dir, c) # Create a path of the type '/content/chest_xray/train/PNEUMONIA' and '/content/chest_xray/train/NORMAL' and so on...\n","                counter = 0 # This is used for the name of each image\n","\n","                for img_name in os.listdir(sub_dir_c):\n","                    counter += 1\n","                    # Example of img_name:  IM-0001-0001.jpeg\n","\n","                    # sub_dir.split(\"/\")[-1]: Take the part \"train\", \"test\" or \"val\"\n","                    # c.lower(): Take the name of the class in uppercase and transform in lowercase\n","                    # f\"_{counter:05d}\": put the number counter starting with a zero sequence until 5 numbers sequence\n","                    # current_name.split(\".\")[-1]: put the jpeg word at the end\n","                    # Example of new_name: train_pneumonia_00001.jpeg\n","\n","                    new_name = sub_dir.split(\"/\")[-1] + \"_\" + c.lower() + f\"_{counter:05d}\" + \".\" + img_name.split(\".\")[-1]\n","\n","                    final_path = sub_dir + \"/\" + new_name # Create a path of the type: /content/chest_xray/train/train_pneumonia_00001.jpeg\n","\n","                    # Move the file from '/content/chest_xray/train/PNEUMONIA/IM-0001-0001.jpeg' to '/content/chest_xray/train/train_pneumonia_00001.jpeg'\n","                    # this result in changin the name of the file\n","                    os.rename( os.path.join(sub_dir_c, img_name), final_path )\n","\n","                ! rmdir {sub_dir_c} # Remove the empty directory \"PNEUMONIA\" and \"NORMAL\" after all rename\n","        !rm -rf {origin_dataset} # Remove a useless directory\n","\n","        print(f\"{Color.GREEN}Dataset reordered and labelled{Color.RESET}\")\n","    except Exception as error:\n","        print(f\"Dataset already merged\\n\")\n","        pass"]},{"cell_type":"markdown","metadata":{"id":"kJNfF_ilr6Ey"},"source":["This function is used to setup the GPU (if there is)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PH1ysKhGZ1P_"},"outputs":[],"source":["def setup_device():\n","\n","    \"\"\"\n","    Setup device to be used\n","    \"\"\"\n","\n","    if torch.cuda.is_available():\n","        device = torch.device(\"cuda:0\")\n","        torch.cuda.set_device(device)\n","    else:\n","        device = torch.device(\"cpu\")\n","\n","    print(f\"Current device: {device}\")"]},{"cell_type":"markdown","metadata":{"id":"GaUnYpNGxwXj"},"source":["## Counter Function\n","This function is used in two options:\n","\n","1. **labels** = *True* : in this case the counter will count the number of \"normal\" or \"pneumonia\" images in the \"path_dir\"\n","\n","1. **labels** = *False* : in this case the counter will count simply the number of images in the \"path_dir\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s73kxQ4yIrTV"},"outputs":[],"source":["def counter (path_dir, labels = True):\n","\n","  file_list = os.listdir(path_dir) # Create a list of all files in the dataset directory\n","\n","  if labels == True:\n","\n","    train_count_normal = 0 # Counter of images \"normal\"\n","    train_count_pneumonia = 0 # Counter of images \"pneumonia\"\n","\n","  # Iterate through each file in the directory\n","    for filename in file_list:\n","        if 'normal' in filename:\n","            train_count_normal += 1\n","        elif 'pneumonia' in filename:\n","            train_count_pneumonia += 1\n","\n","    return train_count_normal, train_count_pneumonia\n","\n","  else:\n","\n","    samples_num = len(file_list);\n","    return samples_num"]},{"cell_type":"markdown","metadata":{"id":"vgiZw8VRqNqf"},"source":["# Preprocessing operation on dataset"]},{"cell_type":"markdown","metadata":{"id":"kwqEzOQTzGvx"},"source":["Given the path for an image, extract a [colormap](https://docs.opencv.org/3.4/d3/d50/group__imgproc__colormap.html) to highlight details\n","\n","Args:\n","\n","1. img_originale: images in tensor format\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fRlQn3_lJgg4"},"outputs":[],"source":["def get_heatmap(img_original):\n","\n","    colormap_type = global_var['colormap_type']\n","\n","    # check if the image is on gpu, if yes bring to cpu for use numpy\n","    if isinstance(img_original, str):\n","        img_original = cv2.imread(img_original, cv2.IMREAD_GRAYSCALE)\n","    else:\n","        if img_original.is_cuda:\n","            img_original = img_original.cpu()\n","\n","        img_original = img_original.detach().numpy()\n","        img_original = np.transpose( img_original, (1,2,0) ) # C, H, W -> H, W, C format\n","        img_original = cv2.cvtColor(img_original, cv2.COLOR_RGB2GRAY) # get gray-scale image\n","\n","    img_normalized = cv2.normalize(img_original, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n","    img_colored = cv2.applyColorMap(img_normalized, colormap_type)\n","    img_rgb = cv2.cvtColor(img_colored, cv2.COLOR_BGR2RGB)\n","\n","    return img_rgb"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gYtMMzCCynsW"},"outputs":[],"source":["def get_gaussian(img_original):\n","\n","    img_original = cv2.imread(img_original, cv2.IMREAD_GRAYSCALE)\n","\n","    smoothing_value = global_var['smoothing_value']\n","    img_smoothed = cv2.GaussianBlur( img_original, ( smoothing_value, smoothing_value), 0 )\n","\n","    return img_smoothed"]},{"cell_type":"markdown","metadata":{"id":"6eQMDf30quvq"},"source":["This function will do a \"downsampling\" or \"upsampling\" technique given an image (tensor) in input, depending on the new requested size.\n","\n","Args:\n","\n","1. **mode** : the mode of [interpolation](https://pytorch.org/docs/stable/generated/torch.nn.functional.interpolate.html) like 'nearest', 'bilinear', 'bicubic' ...\n","2. **size** : the new size, like (800x800)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sjRur4spqNYP"},"outputs":[],"source":["def resize_and_save(mode, size):\n","\n","    original_dataset = global_var['dataset_name'] # Name of the original dataset\n","    resized_dataset = global_var['dataset_name_resized'] # Name of the resized dataset\n","\n","    extract_directory = global_var['extract_directory'] # Directory of the GColab Runtime where we want to extract the dataset\n","    original_path = os.path.join(extract_directory, original_dataset) # Create the path \"/content/chest_xray\"\n","    resized_path = os.path.join(extract_directory, resized_dataset) # Create the path \"/content/chest_xray_resized\"\n","\n","    # Create a list of this type -> ['/content/chest_xray/train', '/content/chest_xray/test', '/content/chest_xray/val']:\n","    sub_directory_original = [ os.path.join(original_path, s) for s in ['train', 'test', 'val']]\n","\n","    try:\n","      for sub_dir in sub_directory_original:\n","        category = os.path.basename(sub_dir) # Extract the part 'train', 'test' or 'val'\n","\n","        # Copy from source to destination\n","        source_dir = sub_dir # Source Directory (Ex. '/content/chest_xray/train')\n","        destination_dir = resized_path + '/' + category # Destination Directory ('Ex. /content/chest_xray_resized/train')\n","\n","        if not(os.path.exists(destination_dir)):\n","          # Copy the entire directory\n","          shutil.copytree(source_dir, destination_dir)\n","\n","          # Loop for the resizing and save\n","          img_files = os.listdir(sub_dir)\n","          for img in tqdm(img_files, desc=f\"Resizing {category} images\", unit=\"image\"):\n","\n","            # Take the name and extension of the image\n","            name, extension = os.path.splitext(img)\n","\n","            # Read the image\n","            img_path = sub_dir + '/' + name + extension\n","            img = read_image(img_path) # Tensor Image (Ex. torch.Size([1, 928, 1288]))\n","\n","            # Add a batch dimensione of 1 needed for the nn.functional.interpolate method\n","            img_4d = img.unsqueeze(0) # Tensor Image (Ex. torch.Size([1, 1, 928, 1288]))\n","\n","            # Interpolation method\n","            resized_image = nn.functional.interpolate(img_4d, size=size, mode=mode) # Tensor Image (Ex. torch.Size([1, 1, 800, 800]))\n","\n","            # Remove the batch dimension\n","            resized_image = resized_image.squeeze(0)  # Tensor Image (Ex. torch.Size([1, 800, 800]))\n","\n","            # Permute the dimensions\n","            resized_image = resized_image.permute(1, 2, 0) # Tensor Image (Ex. torch.Size([800, 800, 1]))\n","\n","            # Transform in numpy structure\n","            resized_image = resized_image.numpy() # Numpy Image (Ex. (800, 800, 1))\n","\n","            # Convert to PIL image using torchvision.transforms\n","            to_pil = ToPILImage() # Transformer Numpy -> PIL Image\n","            resized_image_pil = to_pil(resized_image) # PIL Image\n","\n","            # Save the resulting image\n","            path_to_save = destination_dir + '/' + name + extension # Path of the type '/content/chest_xray_resized/train/img_name.jpeg'\n","            resized_image_pil.save(path_to_save) # Function to save the image\n","        else:\n","          print(f\"{Color.GREEN}Data for {category} already resized{Color.RESET} \")\n","\n","    except Exception as error:\n","      print(f\"Problem in resizing\\n\")\n","      print(error)\n","      pass"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"REAcgrBor5cX"},"outputs":[],"source":["def data_augmentation(transform):\n","\n","    tr = transform # Transform that we use for augment the data of the training set\n","    original_dataset = global_var['dataset_name_resized'] # Name of the original dataset\n","    new_dataset = global_var['dataset_name_resized_augmented'] # Name of the resized dataset\n","\n","    extract_directory = global_var['extract_directory'] # Directory of the GColab Runtime where we want to extract the dataset\n","    original_path = os.path.join(extract_directory, original_dataset) # Create the path \"/content/chest_xray_resized\"\n","    new_path = os.path.join(extract_directory, new_dataset) # Create the path \"/content/chest_xray_resized_augmented\"\n","\n","    # Create a list of this type -> ['/content/chest_xray_resized/train', '/content/chest_xray_resized/test', '/content/chest_xray_resized/val']:\n","    sub_directory_original = [ os.path.join(original_path, s) for s in ['train', 'test', 'val']]\n","    # Create a list of this type -> ['/content/chest_xray_resized_augmented/train', '/content/chest_xray_resized_augmented/test', '/content/chest_xray_resized_augmented/val']:\n","    sub_directory_augmented = [ os.path.join(new_path, s) for s in ['train', 'test', 'val']]\n","\n","    try:\n","      for sub_dir in sub_directory_original:\n","        category = os.path.basename(sub_dir) # Extract the part 'train', 'test' or 'val'\n","\n","        # Copy from source to destination\n","        source_dir = sub_dir # Source Directory ('Ex. /content/chest_xray_resized/train')\n","        destination_dir = new_path + '/' + category # Destination Directory ('Ex. /content/chest_xray_resized_augmented/train')\n","\n","        if not(os.path.exists(destination_dir)):\n","          # Copy the entire directory\n","          shutil.copytree(source_dir, destination_dir)\n","        else:\n","          print(f\"{Color.GREEN}Data for {category} already copied{Color.RESET} \")\n","          return\n","\n","    except Exception as error:\n","      print(f\"Problem in copying\\n\")\n","      print(error)\n","      pass\n","\n","    try:\n","      dir_train = sub_directory_augmented[0]                   # '/content/chest_xray_resized_augmented/train'\n","      img_files_train = os.listdir(dir_train)                  # List of all training images\n","      n_normal, n_pneumonia = counter(dir_train, labels=True)  # Count the number of normal and pneumonia images\n","      difference = n_pneumonia - n_normal                      # Desired value of augmented images\n","      sub_path = os.path.join(dir_train, '/augmented')         # Subdirectory where we save the augmented images\n","      !mkdir {dir_train + '/augmented/'}                       # Create the subdirectory\n","      actual = len(os.listdir(dir_train + '/augmented/'))      # Actual value of augmented images\n","\n","      n = 0                                                    # Counter used to name the augmented images\n","\n","      with tqdm(total=difference, desc='Augmenting data') as pbar:\n","        while  actual < difference:\n","          img = random.choice(img_files_train)           # Pick a random image file from the list\n","          if ('normal' in img):\n","            n += 1\n","            # Take the name and extension of the image\n","            name, extension = os.path.splitext(img)\n","\n","            # Read the image as a Tensor\n","            img = read_image(dir_train + '/' + img)\n","\n","            # Transform the image\n","            new_image = transform(img); # Transformed Tensor Image\n","\n","            # Permute the dimensions\n","            new_image = new_image.permute(1, 2, 0) # Tensor Image (Ex. torch.Size([800, 800, 1]))\n","\n","            # Transform in numpy structure\n","            new_image = new_image.numpy() # Numpy Image (Ex. (800, 800, 1))\n","\n","            # Convert to PIL image using torchvision.transforms\n","            to_pil = ToPILImage() # Transformer Numpy -> PIL Image\n","            new_image_pil = to_pil(new_image) # PIL Image\n","\n","            # Save the resulting image:  Path of the type /content/chest_xray_resized_augmented/train/augmented/train_normal_00001_augmented.jpeg\n","            path_to_save = dir_train + '/augmented/' + name.split('_')[0] + '_' + name.split('_')[1] + f\"_{n:05d}\" + '_augmented' + extension\n","            new_image_pil.save(path_to_save) # Function to save the image\n","            actual += 1     # Update of Actual value of agumented images\n","            pbar.update(1)\n","\n","      for img_name in os.listdir(dir_train + '/augmented/'):\n","        final_path = dir_train + '/' # Create a path of the type: /content/chest_xray_resized_augmented/train/\n","        os.rename(dir_train + '/augmented/' + img_name, final_path + img_name)\n","\n","      !rmdir {dir_train + '/augmented/'} # Remove the empty directory \"PNEUMONIA\" and \"NORMAL\" after all rename\n","\n","    except Exception as error:\n","        print(f\"Problem in augmenting\\n\")\n","        print(error)\n","        pass"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uyLjQfGMbkz7"},"outputs":[],"source":["def preprocess_image(image_global_path, mode):\n","\n","    # based of the selected mode, it does a preprocess on image\n","    # returun None is something went wrong\n","\n","    if mode == 'heatmap':\n","        preprocessed_img = get_heatmap( image_global_path )\n","    elif mode == 'gaussian':\n","        preprocessed_img = get_gaussian( image_global_path )\n","    elif mode == 'gaussian_he':\n","        preprocessed_img = get_gaussian_he( image_global_path )\n","    else:\n","        print(f\"{Color.RED}Preprocessing {mode} is not available{Color.RESET}\")\n","        ! rm -rf {new_dataset_path}\n","\n","        preprocessed_img = None\n","\n","    return preprocessed_img"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KCT0M4ovvdhG"},"outputs":[],"source":["def create_preprocessed_dataset( dataset_path, new_dataset_path, mode ):\n","\n","    categories = ['val', 'test', 'train']\n","    mode = mode.lower()\n","\n","    if os.path.exists(new_dataset_path):\n","        print(f\"{Color.GREEN}Dataset {new_dataset_path} images already exists{Color.RESET}\")\n","        return\n","    else:\n","        ! cp -r {dataset_path} {new_dataset_path}   # coping augmented dataset \"/content/chest_xray_{mode}\"\n","\n","    for c in categories:  # iterate on train, test, val\n","\n","        n_files = len( os.listdir( os.path.join(new_dataset_path, c ) ) )   # it is \"/content/chest_xray_{mode}/train,test,val\"\n","\n","        with tqdm(total=n_files, desc='Processing ' + c) as pbar:\n","            for img_path in os.listdir( os.path.join(dataset_path, c) ):   # iterate on train, test, val of heatmap dataset\n","\n","                full_img_path = new_dataset_path + \"/\" + c + \"/\" + img_path # global path to image\n","\n","                preprocessed_img = preprocess_image(full_img_path, mode)    # preprocess the image according the the selected mode\n","\n","                if \"augmented\" in img_path:\n","                    new_name = img_path.replace(\"augmented\", mode)   # renaming the images\n","                else:\n","                    new_name = img_path.replace(\".\", f\"_{mode}.\")\n","\n","                #print(new_name)\n","                new_full_path = full_img_path.replace(img_path, new_name)  # /content/chest_xray_heatmap/train/test_normal_00001_heatmap.jpeg\n","\n","                cv2.imwrite(full_img_path, preprocessed_img) # overwrite the image\n","                os.rename(full_img_path, new_full_path) # renaming the image\n","\n","                pbar.update(1)"]},{"cell_type":"markdown","metadata":{"id":"7XZvgZPRvK13"},"source":["# Dataset and Network classes"]},{"cell_type":"markdown","metadata":{"id":"j4uFWWdnvMeh"},"source":["## ChestRayDataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"05zFupZ3yC5K"},"outputs":[],"source":["class ChestRayDataset(Dataset):\n","\n","    def __init__(self,file_path, transform):\n","        self.file_path = file_path\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(os.listdir(self.file_path))\n","\n","    def __getitem__(self, index):\n","        sample_path = self.file_path + \"/\" + os.listdir(self.file_path)[index]\n","        sample = cv2.imread(sample_path)\n","        sample = cv2.cvtColor(sample, cv2.COLOR_BGR2RGB)\n","\n","        # Output of the class\n","        sample_tensor = self.transform(sample)\n","        label = sample_path.split(\"/\")[-1].split(\"_\")[1]\n","\n","        if label == \"normal\":\n","            label = 0\n","        else:\n","            label = 1\n","\n","        return sample_tensor,label"]},{"cell_type":"markdown","metadata":{"id":"2yOQD6ATspbh"},"source":["## Architecture"]},{"cell_type":"markdown","metadata":{"id":"sGVBhUimtj90"},"source":["Metrics used are\n","\n","*   [BinaryAccuracy](https://lightning.ai/docs/torchmetrics/stable/classification/accuracy.html): to evaluate how much correct predictions the network does\n","*   [BinaryRecall](https://lightning.ai/docs/torchmetrics/stable/classification/recall.html): in medical field it is important to avoid false negative so avoid to classify a radiography with diseased lungs as healty\n","*   [BinaryF1Score](https://lightning.ai/docs/torchmetrics/stable/classification/f1_score.html): used to computed average of precision and recall, used to evaluate overall performance\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"QXSJ3ljOIZyL"},"source":["Riferimento per cnn in [Medical classification](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7778711/)"]},{"cell_type":"markdown","source":["### Blocks"],"metadata":{"id":"d9mzIlG5MXGK"}},{"cell_type":"code","source":["class SeparableConv2D(nn.Module):\n","    def __init__(self, in_channels, out_channels, kernel_size=7, padding='same', bias=True):\n","        super(SeparableConv2D, self).__init__()\n","\n","        self.depthwise = nn.Conv2d(\n","            in_channels=in_channels,\n","            out_channels=out_channels,\n","            kernel_size=kernel_size,\n","            padding=padding,\n","            groups=in_channels,\n","            bias=bias\n","            )\n","\n","        self.pointwise = nn.Conv2d(\n","            in_channels=in_channels,\n","            out_channels=out_channels,\n","            kernel_size=1,\n","            bias=bias\n","            )\n","\n","    def forward(self, x):\n","        x = self.depthwise(x)\n","        x = self.pointwise(x)\n","        return x"],"metadata":{"id":"4mJYzPUhKrW8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class CAM(nn.Module):\n","    def __init__(self, channels, ratio=8, bias=False):\n","        super(CAM,self).__init__()\n","\n","        self.fc1 = nn.Linear(\n","            in_features=channels,\n","            out_features=channels//ratio,\n","            bias=bias\n","            )\n","\n","        self.fc2 = nn.Linear(\n","            in_features=channels//ratio,\n","            out_features=channels,\n","            bias=bias\n","            )\n","\n","        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)\n","        self.global_max_pool = nn.AdaptiveMaxPool2d((1,1))\n","\n","        self.relu = nn.ReLU()\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def forward(self, x):\n","\n","        # Input Tensor: torch.Size([1, 1024, 8, 8])\n","        x1 = torch.mean(x, dim=(2,3)) # GAP: torch.Size([1, 1024])\n","        x1 = self.fc1(x1) # GAP + Dense: torch.Size([1, 128])\n","        x1 = self.relu(x1)\n","        x1 = self.fc2(x1) # GAP + Dense + Dense: torch.Size([1, 1024])\n","\n","\n","        x2 = self.global_max_pool(x) # GMP: torch.Size([1, 1024, 1, 1])\n","        x2 = x2.view(x2.size(0), -1) # view: torch.size([1, 1024])\n","        x2 = self.fc1(x2) # GMP + Dense: torch.Size([1, 128])\n","        x2 = self.relu(x2)\n","        x2 = self.fc2(x2) # GMP + Dense + Dense: torch.Size([1, 1024])\n","\n","        feats = x1 + x2 # Feats shape: torch.Size([1, 1024])\n","        feats = self.sigmoid(feats)\n","        feats = feats.view(feats.size(0), feats.size(1), 1, 1) # feats.shape: torch.Size([1, 1024, 1, 1])\n","        feats = torch.mul(x, feats) # Output shape: torch.Size([1, 1024, 8, 8])\n","        return feats"],"metadata":{"id":"aP5rm2XROYGW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class SAM_avg (nn.Module):\n","    def __init__(self, channels, padding='same'):\n","\n","        super(SAM_avg, self).__init__()\n","\n","        self.sep_conv2d_1 = SeparableConv2D(\n","            in_channels=channels,\n","            out_channels=channels,\n","            kernel_size=1,\n","            padding=padding\n","        )\n","\n","        self.sep_conv2d_2 = SeparableConv2D(\n","            in_channels=channels,\n","            out_channels=channels,\n","            kernel_size=3,\n","            padding=padding\n","        )\n","\n","        self.batchnorm2d = nn.BatchNorm2d(channels)\n","\n","        self.conv2d_3 = nn.Conv2d(\n","            in_channels=1,\n","            out_channels=1,\n","            kernel_size=7,\n","            padding=padding\n","        )\n","\n","        self.sigm= nn.Sigmoid()\n","\n","    def forward(self, x, cam):\n","\n","\n","        # Input tensor: torch.Size([1, 1024, 8, 8])\n","\n","        x = self.sep_conv2d_1(x) # SepConv2d_1 shape: torch.Size([1, 1024, 8, 8])\n","        x = self.sep_conv2d_2(x) # SepConv2d_2 shape: torch.Size([1, 1024, 8, 8])\n","        x = self.batchnorm2d(x) # BatchNorm2d shape: torch.Size([1, 1024, 8, 8])\n","        x = x*cam # X*CAM shape: torch.Size([1, 1024, 8, 8])\n","\n","        x1 = torch.mean(x, dim=1, keepdim=True) # Mean pooling shape: torch.Size([1, 1, 8, 8])\n","\n","        feats = self.conv2d_3(x1) # Conv2d shape: torch.Size([1, 1, 8, 8])\n","        feats = self.sigm(feats)\n","        out = x * feats # Output shape: torch.Size([1, 1024, 8, 8])\n","\n","        return out"],"metadata":{"id":"5eypP1uqnZfo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class SAM_max (nn.Module):\n","    def __init__(self, channels, padding='same'):\n","\n","        super(SAM_max, self).__init__()  # Initialize nn.Module\n","\n","        self.sep_conv2d_1 = SeparableConv2D(\n","            in_channels=channels,\n","            out_channels=channels,\n","            kernel_size=1,\n","            padding=padding\n","        )\n","\n","        self.sep_conv2d_2 = SeparableConv2D(\n","            in_channels=channels,\n","            out_channels=channels,\n","            kernel_size=3,\n","            padding=padding\n","        )\n","\n","        self.batchnorm2d = nn.BatchNorm2d(channels)\n","\n","        self.conv2d_3 = nn.Conv2d(\n","            in_channels=1,\n","            out_channels=1,\n","            kernel_size=7,\n","            padding=padding\n","        )\n","\n","        self.sigm= nn.Sigmoid()\n","\n","    def forward(self, x, cam):\n","\n","        # input: torch.Size([1, 1024, 8, 8])\n","\n","        x = self.sep_conv2d_1(x) # SepConv2d_1 shape: torch.Size([1, 1024, 8, 8])\n","        x = self.sep_conv2d_2(x) # SepConv2d_2 shape: torch.Size([1, 1024, 8, 8])\n","        x = self.batchnorm2d(x) # BatchNorm2d shape: torch.Size([1, 1024, 8, 8])\n","        x = x*cam # X*CAM shape: torch.Size([1, 1024, 8, 8])\n","\n","        # Max Pooling\n","        x2, _ = torch.max(x, dim=1, keepdim=True) # MAX pooling shape: torch.Size([1, 1, 8, 8])\n","\n","        # Convolution and Multiplication\n","        feats = self.conv2d_3(x2) # Conv2d shape: torch.Size([1, 1, 8, 8])\n","        feats = self.sigm(feats)\n","        out = x*feats # Output shape: torch.Size([1, 1024, 8, 8])\n","\n","        return out"],"metadata":{"collapsed":true,"id":"uAj3XIHNh6qc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class RichardsSigmoid(nn.Module):\n","    def __init__(self, units=1, bias=True):\n","        super(RichardsSigmoid, self).__init__()\n","\n","        self.A = nn.Parameter(torch.Tensor(units))\n","        self.Q = nn.Parameter(torch.Tensor(units))\n","        self.mu = nn.Parameter(torch.Tensor(units))\n","\n","        self.reset_parameters()\n","\n","    def reset_parameters(self):\n","        nn.init.zeros_(self.A)\n","        nn.init.zeros_(self.Q)\n","        nn.init.zeros_(self.mu)\n","\n","    def forward(self, x):\n","        # Richards sigmoid function\n","        return 1 / (1 + torch.exp(-self.A * torch.exp(-self.Q * (x - self.mu))))\n","\n","    def compute_output_shape(self, input_shape):\n","        return input_shape[:-1] + (self.units,)"],"metadata":{"id":"U2PP05AUj3Gd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class ChannelDropout(nn.Module):\n","    def __init__(self, channels, drop_ratio=0.2):\n","        super(ChannelDropout, self).__init__()\n","\n","        self.drop_ratio = drop_ratio # Ratio of channels to be dropped.\n","\n","        # Build the Mask with ones\n","        self.mask = nn.Parameter(torch.ones(1, channels, 1 , 1))\n","\n","        # Build the Richard Sigmoid to apply to the mask\n","        self.r_sigm = RichardsSigmoid(units=1)\n","\n","    def forward(self, x):\n","        # Apply the Richard Sigmoid to the mask\n","        mask = self.r_sigm(self.mask) # MASK Shape: torch.Size([1, 3072, 1, 1])\n","\n","        # Repeat the mask as the batch size of the input (to apply to all the features)\n","        batch_size = x.size(0)\n","        mask_replicated = mask.repeat(batch_size, 1, 1, 1)\n","\n","        # Apply the mask to the input\n","        x = x * mask_replicated # Input after Mask: torch.Size([1, 3072, 8, 8])\n","\n","        # New Channel dimension\n","        new_n_channels = int(x.shape[1] * (1 - self.drop_ratio))\n","\n","        # Perform top-k operation along the channels dimension (dim=1)\n","        # Keep the channels with highest values\n","        _, indices = torch.topk(x, k=new_n_channels, dim=1, largest=True)\n","        output = torch.gather(x, dim=1, index=indices)\n","\n","        return output"],"metadata":{"id":"ak3hs7oFj5rf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class CSSAM (nn.Module):\n","    def __init__(self, channels, padding='same', ratio=8, bias=False, drop_ratio=0.2):\n","\n","        super(CSSAM, self).__init__()  # Initialize nn.Module\n","\n","\n","        self.SamAvg = SAM_avg(\n","            channels=channels,\n","            padding=padding\n","            )\n","\n","        self.SamMax = SAM_max(\n","            channels=channels,\n","            padding=padding\n","            )\n","\n","        self.cam = CAM(\n","            channels=channels,\n","            ratio=ratio,\n","            bias=bias\n","            )\n","\n","        self.ch_dropout = ChannelDropout(\n","            channels=channels*3,\n","            drop_ratio=drop_ratio\n","            )\n","\n","    def forward(self, x):\n","\n","        cam = self.cam(x)\n","\n","        # SAM average and SAM max\n","        x_avg = self.SamAvg(x, cam) # SAM_avg shape: torch.Size([1, 1024, 8, 8])\n","        x_max = self.SamMax(x, cam) # SAM_max shape: torch.Size([1, 1024, 8, 8])\n","\n","        # Concatenation\n","        x = torch.cat((x_avg, x_max, cam), dim=1) # Concatenation shape: torch.Size([1, 3072, 8, 8])\n","\n","        # Channel Dropout\n","        out = self.ch_dropout(x) # Channel Dropout shape: torch.Size([1, 1536, 8, 8])\n","\n","        return out"],"metadata":{"id":"WgzwSMXLnj6P"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Bodies"],"metadata":{"id":"DP7tdvSgMaRV"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"DMnIyzqTsy44"},"outputs":[],"source":["class JAM_network(nn.Module):\n","    def __init__(self, channels, padding='same', drop_ratio=0.2, dropout_rate=0.35 ):\n","        super(JAM_network,self).__init__()\n","\n","        self.densenet121 = models.densenet121(pretrained=True).features\n","\n","        self.cssam = CSSAM(\n","            channels=channels,\n","            padding=padding,\n","            drop_ratio=drop_ratio\n","            )\n","\n","        self.dropout = nn.Dropout(\n","            p=dropout_rate\n","            )\n","\n","\n","        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)\n","\n","\n","    def forward(self, x):\n","\n","        # Input Shape: torch.Size([1, 3, 256, 256])\n","\n","        # BACKBONE\n","        x_dense = self.densenet121(x) # x_dense.shape: torch.Size([1, 1024, 8, 8])\n","\n","        # CSSAM\n","        x_cssam = self.cssam(x_dense) # CSSAM shape: torch.Size([1, 2457, 8, 8])\n","\n","        # GAP\n","        feats = self.global_avg_pool(x_cssam) # GAP shape: torch.Size([1, 2457, 1, 1])\n","        feats = torch.flatten(feats, start_dim=1) # Flattening shape: torch.Size([1, 2457])\n","\n","        # DROPOUT\n","        feats = self.dropout(feats) # dropout: torch.Size([1, 2457])\n","\n","        return feats"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"guvsPltcgb0q"},"outputs":[],"source":["class JAM(nn.Module):\n","    def __init__(self, channels, device, model_name, padding='same', drop_ratio=0.2, dropout_rate=0.35):\n","        super().__init__()\n","\n","        self.body = JAM_network(\n","            channels,\n","            padding='same',\n","            drop_ratio=0.2,\n","            dropout_rate=0.35\n","            ).to(device)\n","\n","        self.kan_ch = int(channels * 3 * (1 - drop_ratio))\n","\n","        self.classifier = KAN([ self.kan_ch, 1], device=device)\n","        self.classifier.to(device)\n","\n","        self.sig = nn.Sigmoid()\n","\n","        self.loss_fn = nn.BCELoss()\n","        self.optimizer = torch.optim.Adam(self.parameters(), lr=0.001) # torch.optim.SGD(self.model.parameters(), lr=0.1)\n","\n","        self.accuracy = BinaryAccuracy()\n","        self.precision = BinaryPrecision()\n","        self.recall = BinaryRecall()\n","        self.auroc = BinaryAUROC()\n","        self.f1score = BinaryF1Score()\n","\n","        self.model_name = model_name\n","\n","    def set_training_mode(self):\n","        self.body.train()\n","\n","    def set_eval_mode(self):\n","        self.body.eval()\n","\n","    def forward(self, model_input):\n","        out_body = self.body(model_input)\n","        out_cls = self.classifier(out_body)\n","        out = self.sig(out_cls)\n","        return out\n","\n","    def save(self):\n","        name = self.model_name\n","        torch.save(self.state_dict(), name )\n","\n","    def load(self):\n","        name = self.model_name\n","        try:\n","            self.load_state_dict(torch.load(name) )\n","            print(f\"{Color.MAGENTA}loaded: {name}{Color.RESET}\")\n","        except Exception as e:\n","            print(f\"{Color.RED}Model not loaded{Color.RESET}\")\n","            print(e)\n","\n","jam_try = JAM(1024, device, \"jam_model.pt\")\n","t = torch.rand([2, 3, 256, 256]).to(device)\n","print(f\"input tensor: {t.shape}\")\n","\n","out = jam_try(t)\n","print(f\"out.shape: {out.shape},\\n{out}\")"]},{"cell_type":"markdown","metadata":{"id":"_HmvdEe24KbO"},"source":["# Dataset informations"]},{"cell_type":"markdown","metadata":{"id":"owMu7Tiqzbnc"},"source":["## Download and merge of the dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hLaY4F33bKCc"},"outputs":[],"source":["download_dataset(\n","     link_dataset=global_var['link_dataset'],\n","     delete_zip_file=False\n"," )\n","\n","merge_dataset()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OVeAMibJverg"},"outputs":[],"source":["size = global_var[\"resizing_dim\"] # Choose the size\n","\n","resize_and_save(\n","    mode='bilinear',    # Choose the mode of interpolation ('nearest', 'bilinear', 'bicubic', ...)\n","    size=size\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Lg6MUQReBBLp"},"outputs":[],"source":["probs = 0.15\n","\n","resize_crop_transform = transforms.RandomResizedCrop(size=size, scale=(0.8, 1.0))  # For zoom and size change\n","rotation_transform = transforms.RandomRotation(degrees=(0,20))  # For random rotation\n","horizontal_flip_transform = transforms.RandomHorizontalFlip(p=1.0)  # For horizontal flip\n","\n","\n","tr_augmentation = transforms.Compose([\n","     transforms.RandomApply( [resize_crop_transform], p=probs ),\n","     transforms.RandomApply( [rotation_transform], p=probs ),\n","     transforms.RandomApply( [horizontal_flip_transform], p=probs ),\n","\n","])\n","\n","data_augmentation(tr_augmentation)"]},{"cell_type":"markdown","metadata":{"id":"_sFf8oHnnS8s"},"source":["## Dataset exploration, augmentation and visualization\n","\n","In this part we plot an:\n","\n","1. *Histogram Distribution* : to see if the dataset is balanced or unbalanced\n","\n","2. *Pie Chart* : to visualize the sizes of training, test and validation sets"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"YVgIgejynSRf"},"outputs":[],"source":["# @title Initial statistic of dataset\n","# Definition of the paths for training, test and validation set\n","path_training_set = \"/content/chest_xray/train\"\n","path_test_set = \"/content/chest_xray/test\"\n","path_val_set = \"/content/chest_xray/val\"\n","\n","# Counting the Normal and Pneumonia images in the training set\n","train_norm, train_pneu = counter(path_training_set, labels = True); # Count the number of normal images and pneumonia images\n","\n","train_num = counter(path_training_set, labels = False);\n","test_num = counter(path_test_set, labels = False);\n","val_num = counter(path_val_set, labels = False);\n","\n","# Histogram Data\n","classes_hist = ['Normal', 'Pneumonia']\n","counts_hist = [train_norm, train_pneu]\n","colors_hist = ['aquamarine', 'blue']\n","\n","# Pie Chart Data\n","classes_pie = ['Train', 'Test', 'Validation']\n","counts_pie = [train_num, test_num, val_num]\n","colors_pie = ['gold', 'lightcoral', 'lightskyblue']\n","\n","# Create a figure with specific dimensions and subplots\n","fig, ax = plt.subplots(1, 2, figsize=(8, 4))  # Adjust figsize as needed\n","\n","# Plotting the histogram\n","ax[0].bar(classes_hist, counts_hist, color=colors_hist)\n","ax[0].set_title('Histogram distribution')\n","ax[0].set_xlabel('Class')\n","ax[0].set_ylabel('Count')\n","\n","# Plotting the pie chart\n","ax[1].pie(counts_pie, labels=classes_pie, colors=colors_pie, autopct='%1.1f%%', startangle=90)\n","ax[1].set_title('Pie Chart of training, test and validation sizes')\n","\n","plt.tight_layout()  # This adjusts subplot params so that the subplots fit into the figure area.\n","plt.show()\n","\n","print(\"\\nThe number of training images:\", train_num)\n","print(\"The number of test images:\", test_num)\n","print(\"The number of validation images:\", val_num)\n","\n","train_num_norm, train_num_pn = counter(path_training_set, labels = True);\n","test_num_norm, test_num_pn= counter(path_test_set, labels = True);\n","val_num_norm, val_num_pn = counter(path_val_set, labels = True);\n","\n","print(\"\\nThe number of normal images: \", train_num_norm + test_num_norm + val_num_norm)\n","print(\"The number of pneumonia images: \", train_num_pn + test_num_pn + val_num_pn)"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"mc37Ky-RrrVB"},"outputs":[],"source":["# @title Visualization of two random images\n","# Define a transform element to resize the images as square images and transform in Tensor\n","tr_1 = transforms.ToTensor()                # Convert the image to a tensor\n","\n","# Definition of the paths for training, test and validation set\n","path_training_set = \"/content/chest_xray/train\"\n","path_test_set = \"/content/chest_xray/test\"\n","path_val_set = \"/content/chest_xray/val\"\n","\n","# Instance of classes for the Dataset\n","train_dataset = ChestRayDataset(path_training_set, tr_1) # Create an instance of training class\n","test_dataset = ChestRayDataset(path_test_set, tr_1) # Create an instance of test class\n","val_dataset = ChestRayDataset(path_val_set, tr_1) # Create an instance of validation class\n","\n","# Generate two random numbers to select two random images from training set\n","random_num_1 = np.random.randint(1, len(train_dataset))\n","random_num_2 = np.random.randint(1, len(train_dataset))\n","\n","# Take an image + label randomly\n","img_1, label_1 = train_dataset[random_num_1]\n","img_2, label_2 = train_dataset[random_num_2]\n","\n","# Display the image using matplotlib library\n","plt.figure( figsize=(6,3))\n","plt.subplot(1,2,1)\n","plt.imshow(img_1.permute(1, 2, 0))\n","plt.axis('on')\n","plt.text(0, 0, label_1, fontsize=12, color='black', backgroundcolor='white')\n","\n","plt.subplot(1,2,2)\n","plt.imshow(img_2.permute(1, 2, 0))\n","plt.axis('on')\n","plt.text(0, 0, label_2, fontsize=12, color='black', backgroundcolor='white')\n","\n","plt.show()\n","\n","print (\"The shape of the first image is: \", img_1.permute(1, 2, 0).shape)\n","print (\"The shape of the second image is: \", img_2.permute(1, 2, 0).shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"yrfs0O-G5puA"},"outputs":[],"source":["# @title Squarring\n","\n","# Define a transform element to resize the images as square images and transform in Tensor\n","tr_1 = transforms.ToTensor()                 # Convert the image to a tensor\n","\n","# Definition of the paths for training, test and validation set\n","path_training_set_resized = \"/content/chest_xray_resized/train\"\n","path_test_set_resized = \"/content/chest_xray_resized/test\"\n","path_val_set_resized = \"/content/chest_xra_resized/val\"\n","\n","# Instance of classes for the Dataset\n","train_dataset_resized = ChestRayDataset(path_training_set_resized, tr_1) # Create an instance of training class\n","test_dataset_resized = ChestRayDataset(path_test_set_resized, tr_1) # Create an instance of test class\n","val_dataset_resized = ChestRayDataset(path_val_set_resized, tr_1) # Create an instance of validation class\n","\n","# Take an image + label randomly\n","img_1, label_1 = train_dataset_resized[random_num_1]\n","img_2, label_2 = train_dataset_resized[random_num_2]\n","\n","# Display the image using matplotlib library\n","plt.figure( figsize=(6,3))\n","plt.subplot(1,2,1)\n","plt.imshow(img_1.permute(1, 2, 0))\n","plt.axis('on')\n","plt.text(0, 0, label_1, fontsize=12, color='black', backgroundcolor='white')\n","\n","plt.subplot(1,2,2)\n","plt.imshow(img_2.permute(1, 2, 0))\n","plt.axis('on')\n","plt.text(0, 0, label_2, fontsize=12, color='black', backgroundcolor='white')\n","\n","plt.show()\n","\n","print (\"The shape of the first image is: \", img_1.permute(1, 2, 0).shape)\n","print (\"The shape of the second image is: \", img_2.permute(1, 2, 0).shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"JNNfT9nMIZCR"},"outputs":[],"source":["# @title Statistics after data augmentation\n","# Definition of the paths for training, test and validation set\n","path_training_set_resized_augmented = \"/content/chest_xray_resized_augmented/train\"\n","\n","# Counting the Normal and Pneumonia images in the training set\n","train_norm, train_pneu = counter(path_training_set_resized_augmented, labels = True); # Count the number of normal images and pneumonia images\n","\n","# Histogram Data\n","classes_hist = ['Normal', 'Pneumonia']\n","counts_hist = [train_norm, train_pneu]\n","colors_hist = ['aquamarine', 'blue']\n","\n","\n","plt.figure(figsize=(5, 3))\n","plt.bar(classes_hist, counts_hist, color=colors_hist)\n","plt.title('Histogram distribution')\n","plt.xlabel('Class')\n","plt.ylabel('Count')\n","plt.show()\n","\n","print(\"The number of normal images:\", train_norm)\n","print(\"The number of pneumonia images:\", train_pneu)\n","\n","train_num_norm, train_num_pn = counter(path_training_set, labels = True);\n","test_num_norm, test_num_pn= counter(path_test_set, labels = True);\n","val_num_norm, val_num_pn = counter(path_val_set, labels = True);\n","\n","print(\"\\nThe number of normal images: \", train_num_norm + test_num_norm + val_num_norm)\n","print(\"The number of pneumonia images: \", train_num_pn + test_num_pn + val_num_pn)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"C15kRm31B3aX"},"outputs":[],"source":["# @title Visualization of two sample in augmented dataset\n","# Instance of classes for the Dataset\n","train_dataset_resized_augmented = ChestRayDataset(path_training_set_resized_augmented, tr_1) # Create an instance of training class\n","\n","# Generate two random numbers to select two random images from training set\n","random_num_1 = np.random.randint(1, len(train_dataset_resized_augmented))\n","random_num_2 = np.random.randint(1, len(train_dataset_resized_augmented))\n","\n","# Take an image + label randomly\n","img_1, label_1 = train_dataset_resized_augmented[random_num_1]\n","img_2, label_2 = train_dataset_resized_augmented[random_num_2]\n","\n","# Display the image using matplotlib library\n","plt.figure(figsize=(6,3))\n","plt.subplot(1,2,1)\n","plt.imshow(img_1.permute(1, 2, 0))\n","plt.axis('on')\n","plt.text(0, 0, label_1, fontsize=12, color='black', backgroundcolor='white')\n","\n","plt.subplot(1,2,2)\n","plt.imshow(img_2.permute(1, 2, 0))\n","plt.axis('on')\n","plt.text(0, 0, label_2, fontsize=12, color='black', backgroundcolor='white')\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"tru1XMrbMq6Q"},"source":["# Main"]},{"cell_type":"markdown","metadata":{"id":"PETZ7JYemIzn"},"source":["## Train functions"]},{"cell_type":"markdown","metadata":{"id":"LPug6dfQhrfK"},"source":["Training loop from [documentation](https://pytorch.org/tutorials/beginner/introyt/trainingyt.html)"]},{"cell_type":"code","source":["def my_plot(metrics_dict, x_axis, y_axis, marker='o', linestyle='-', color='b'):\n","\n","    x_values= metrics_dict[x_axis]\n","    y_values= metrics_dict[y_axis]\n","\n","    plt.figure(figsize=(8, 5))\n","    plt.plot(x_values, y_values, marker=marker, linestyle=linestyle, color=color)\n","    plt.xlabel(x_axis)\n","    plt.ylabel(y_axis)\n","    plt.title(y_axis)\n","    plt.show()\n"],"metadata":{"id":"Kiho-pu6Z-W-"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MDqbhMpUtpWU"},"outputs":[],"source":["def calculate_metrics(model, outputs, labels, metrics_dict):\n","\n","    accuracy_value = model.accuracy(outputs, labels)\n","    precision_value = model.precision(outputs, labels)\n","    recall_value = model.recall(outputs, labels)\n","    auroc_value = model.auroc(outputs, labels)\n","    f1score_value = model.f1score(outputs, labels)\n","\n","\n","    metrics_dict['accuracy'].append(accuracy_value)\n","    metrics_dict['precision'].append(precision_value)\n","    metrics_dict['recall'].append(recall_value)\n","    metrics_dict['auroc'].append(auroc_value)\n","    metrics_dict['f1score'].append(f1score_value)\n","\n","\n","    metrics_string = f\"\\n\\taccuracy: {accuracy_value:.2f}, precision: {precision_value:.2f}, recall: {recall_value:.2f}, auroc: {auroc_value:.2f}, f1score: {f1score_value:.2f}\"\n","    print(metrics_string)\n","    return accuracy_value"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KlmSrxH8hIuV"},"outputs":[],"source":["def train_one_epoch(model, epoch_index, training_loader):\n","    running_loss = 0.\n","    last_loss = 0.\n","\n","    # Here, we use enumerate(training_loader) instead of\n","    # iter(training_loader) so that we can track the batch\n","    # index and do some intra-epoch reporting\n","\n","    n_batch = len(training_loader)  # number of batches in training loader\n","\n","    with tqdm(total=n_batch, desc=f'Epoch {epoch_index+1} ') as pbar:\n","        for i, data in enumerate(training_loader):\n","\n","            # Every data instance is an input + label pair\n","            inputs, labels = data\n","\n","            inputs = inputs.to(device)\n","            labels = labels.to(device)\n","\n","            labels = labels.unsqueeze(dim=1)     # reshaping as inputs, from [0, 1] to [[0], [1]]\n","            labels = labels.type(torch.float32)  # preditions of network are floats\n","\n","            # Zero your gradients for every batch!\n","            model.optimizer.zero_grad()\n","\n","            # Make predictions for this batch\n","            outputs = model(inputs)\n","\n","            # Compute the loss and its gradients\n","            loss = model.loss_fn(outputs, labels)\n","            loss.backward()\n","\n","            # Adjust learning weights\n","            model.optimizer.step()\n","\n","            # Gather data and report\n","            running_loss += loss.item()\n","\n","            if i % batch_size == 0 and i>0:\n","                last_loss = running_loss / batch_size #1000 # loss per batch\n","                print(f'\\r\\tbatch {i+1} loss: {last_loss}', end='')\n","                running_loss = 0.\n","\n","                model.save()\n","\n","                del inputs, labels, outputs, loss\n","                gc.collect()\n","\n","                if device != \"cpu\":\n","                    torch.cuda.empty_cache()\n","\n","            pbar.update(1)\n","\n","        # Svuota la cache CUDA dopo ogni epoca\n","        torch.cuda.empty_cache()\n","\n","        pbar.update(labels.shape[0])\n","\n","    return last_loss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hVTkjpNugwlg"},"outputs":[],"source":["def training(model, epochs, training_loader, validation_loader, metrics_train):\n","    model.set_training_mode()\n","\n","    best_vloss = 1000000\n","    epoch_number = 0\n","    current_accuracy = 0.0\n","\n","    for epoch in range(epochs):\n","        #print('EPOCH {}:'.format(epoch_number + 1))\n","\n","        # Make sure gradient tracking is on, and do a pass over the data\n","        model.set_training_mode()\n","        avg_loss = train_one_epoch(model, epoch, training_loader)\n","\n","        metrics_train['loss'].append(avg_loss)\n","        metrics_train['epoch'].append(epoch)\n","\n","        running_vloss = 0.0\n","\n","        model.set_eval_mode()\n","        # Disable gradient computation and reduce memory consumption.\n","        with torch.no_grad():\n","            for i, vdata in enumerate(validation_loader):\n","                vinputs, vlabels = vdata\n","\n","                vinputs = vinputs.to(device)\n","                vlabels = vlabels.to(device)\n","\n","                vlabels = vlabels.unsqueeze(dim=1)    # [0, 1, 0, 1] -> [[0], [1], [0], [1]]\n","                vlabels = vlabels.type(torch.float32)\n","\n","                voutputs = model(vinputs)\n","                vloss = model.loss_fn(voutputs, vlabels)\n","\n","                running_vloss += vloss\n","\n","                current_accuracy = calculate_metrics(model, voutputs, vlabels, metrics_train)\n","\n","\n","        avg_vloss = running_vloss / ( i + 1 )\n","        print(f\"\\taverage loss train: {avg_loss:.4f}, average loss validation: {avg_vloss:.4f}\\n\")\n","\n","        if avg_vloss < best_vloss:\n","            best_vloss = avg_loss\n","            model_path = model.model_name #f'/content/model_{epoch_number}'\n","            torch.save(model.state_dict(), model_path)\n","            print(f\"{Color.MAGENTA}Saved: {model.model_name}{Color.RESET}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-4RYAhHqjHPc"},"outputs":[],"source":["def testing(model, test_loader, metrics_test):\n","\n","    model.set_eval_mode()\n","    running_tloss = 0\n","\n","    n_batch = len(test_loader)  # number of batches in training loader\n","    batch_size = global_var['batch_size']\n","    n_files = n_batch*batch_size\n","\n","    with tqdm(total=n_files, desc=f'Testing ') as pbar:\n","\n","        with torch.no_grad():\n","            for i, tdata in enumerate(test_loader):\n","\n","                tinputs, tlabels = tdata\n","\n","                tinputs = tinputs.to(device)\n","                tlabels = tlabels.to(device)\n","\n","                tlabels = tlabels.unsqueeze(dim=1)    # [0, 1, 0, 1] -> [[0], [1], [0], [1]]\n","                tlabels = tlabels.type(torch.float32)\n","\n","                if i == 0:\n","                    outputs_tensor = torch.empty(0, tlabels.shape[1], device=device)\n","                    labels_tensor = torch.empty(0, tlabels.shape[1], device=device)\n","\n","                toutputs = model(tinputs)\n","                tloss = model.loss_fn(toutputs, tlabels)\n","\n","                metrics_test['loss'].append(tloss.item())\n","                metrics_test['iteration'].append(i)\n","\n","                running_tloss += tloss\n","\n","                outputs_tensor = torch.cat((outputs_tensor, toutputs), dim=0)\n","                labels_tensor = torch.cat((labels_tensor, tlabels), dim=0)\n","\n","                pbar.update(tlabels.shape[0])\n","\n","            _ = calculate_metrics(model, outputs_tensor, labels_tensor, metrics_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Aah1c8VdYl8x"},"outputs":[],"source":["print(device)\n","batch_size = global_var[\"batch_size\"]\n","train_epochs = global_var[\"train_epochs\"]\n","tf_to_tensor = transforms.ToTensor()"]},{"cell_type":"markdown","metadata":{"id":"XW7LJ74xETHq"},"source":["## Main 2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NVhyz6flF_XK"},"outputs":[],"source":["#base_path = global_var['extract_directory'] + \"/\" + global_var['dataset_name_resized_augmented']\n","\n","dt_train_path = global_var['extract_directory'] + \"/\" + global_var['dataset_name_resized_augmented'] + '/train'\n","dt_test_path = global_var['extract_directory'] + \"/\" + global_var['dataset_name_resized_augmented'] + '/test'\n","dt_val_path = global_var['extract_directory'] + \"/\" + global_var['dataset_name_resized_augmented'] + '/val'\n","\n","print(dt_train_path)\n","print(dt_test_path)\n","print(dt_val_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qgDn7HElF_XK"},"outputs":[],"source":["dt_train_dataset = ChestRayDataset(\n","    file_path= dt_train_path,\n","    transform=tf_to_tensor\n",")\n","\n","dt_test_dataset = ChestRayDataset(\n","    file_path= dt_test_path,\n","    transform=tf_to_tensor\n",")\n","\n","dt_val_dataset = ChestRayDataset(\n","    file_path= dt_val_path,\n","    transform=tf_to_tensor\n",")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xgr5hKfRm_gn"},"outputs":[],"source":["dt_train_dataloader = DataLoader(\n","    dt_train_dataset,\n","    batch_size=batch_size,\n","    shuffle=True\n","    )\n","\n","dt_test_dataloader = DataLoader(\n","    dt_test_dataset,\n","    batch_size=batch_size,\n","    shuffle=True\n","    )\n","\n","dt_val_dataloader = DataLoader(\n","    dt_val_dataset,\n","    batch_size=batch_size,\n","    shuffle=True\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eIcYCX14F_XK"},"outputs":[],"source":["jam_network = JAM(\n","    channels=1024,\n","    device=device,\n","    model_name=global_var['extract_directory'] + \"/\" + \"jam_network.pt\"\n","    )\n","\n","jam_network.load()\n","jam_network.to(device)\n","jam_network.set_eval_mode()\n","\n","gc.collect()\n","if device != \"cpu\":\n","    torch.cuda.empty_cache()\n","\n","train_metrics = {}\n","train_metrics['epoch'] = []\n","train_metrics['loss'] = []\n","train_metrics['accuracy'] = []\n","train_metrics['precision'] = []\n","train_metrics['recall'] = []\n","train_metrics['auroc'] = []\n","train_metrics['f1score'] = []\n","\n","\n","training_epochs = 50\n","print(device)\n","training(jam_network, training_epochs, dt_train_dataloader, dt_val_dataloader, train_metrics)\n","'''\n","data_iter = iter(dt_train_dataloader)\n","samples, labels = next(data_iter)  # Get a batch of data\n","samples, labels = samples[:2], labels[:2]  # Select the first 2 samples\n","\n","start_time = time.time()\n","with torch.no_grad():  # Disable gradient calculation for inference\n","    samples = samples.to(device)\n","    predictions = jam_network(samples)\n","end_time = time.time()\n","\n","print(batch_size)\n","# Print the outputs\n","print(\"Inference time: \", end_time - start_time)\n","print(\"Sample inputs shape:\", samples.shape)\n","print(\"Predictions shape:\", predictions.shape)\n","print(\"Predictions:\", predictions)\n","print(\"labels: \", labels)\n","'''"]},{"cell_type":"code","source":["train_metrics"],"metadata":{"id":"tJlnWpA5tz52"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bfwEWFtKF_XL"},"outputs":[],"source":["jam_network.save()"]},{"cell_type":"code","source":["test_metrics = {}\n","test_metrics['iteration'] = []\n","test_metrics['loss'] = []\n","test_metrics['accuracy'] = []\n","test_metrics['precision'] = []\n","test_metrics['recall'] = []\n","test_metrics['auroc'] = []\n","test_metrics['f1score'] = []\n","\n","jam_network.load()\n","testing(jam_network, dt_test_dataloader, test_metrics)"],"metadata":{"id":"hRN3BOTiaR_m"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## PLOT"],"metadata":{"id":"GGrdQ0RRp1oi"}},{"cell_type":"code","source":["my_plot(test_metrics, 'iteration', 'loss', marker='o', linestyle='-', color='b')"],"metadata":{"id":"IB8xj9eFp1MZ"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"collapsed_sections":["uZQ3yrZuZsXx","ecVBSRTQBmdb","86RET5RaaNKi","ztmhJrCtndIN","GgQdo_c94IG5","09Nrq6xD9viW","vgiZw8VRqNqf","j4uFWWdnvMeh","2yOQD6ATspbh","_HmvdEe24KbO","owMu7Tiqzbnc","_sFf8oHnnS8s","tru1XMrbMq6Q","XW7LJ74xETHq","GGrdQ0RRp1oi"],"provenance":[],"machine_shape":"hm","gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}