{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZQ3yrZuZsXx"
      },
      "source": [
        "# Initialization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yt-OTzpxBk3q"
      },
      "source": [
        "### Installation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGd-QE_AX-bs"
      },
      "source": [
        "In this section there are the installation needed for the rest of the code:\n",
        "\n",
        "1. *gdown* : This package is used for downloading files from Google Drive. It's handy for fetching data or models stored on Google Drive links.\n",
        "\n",
        "2. *tqdm* : This package provides a progress bar for iterables (like loops or file downloads) in Python. It makes it easier to track the progress of lengthy operations.\n",
        "\n",
        "3. *pytorch_ligthning* : This is a high-level interface for PyTorch that makes it easier to train complex models. It provides abstractions and utilities for training neural networks.\n",
        "\n",
        "4. *torchvision* : This package provides popular datasets, model architectures, and image transformations for computer vision tasks in PyTorch.\n",
        "\n",
        "5. *wget* : This package is used for programmatically downloading files from the internet using HTTP or FTP protocols.\n",
        "\n",
        "6. *pykan* : This package is needed for the implementation of the KAN networks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "pqZNQdxgZe3v"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install gdown tqdm\n",
        "!pip install pytorch_lightning torchvision --quiet\n",
        "!pip install pykan"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecVBSRTQBmdb"
      },
      "source": [
        "### Import of libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USfihjHDbYXe"
      },
      "source": [
        "In this section there is the libraries import:\n",
        "\n",
        "1. *numpy* : This library is used for numerical computations and array manipulations in Python.\n",
        "\n",
        "\n",
        "2. *torch* : this library is widely used for building and training deep learning models. In particular **torch.nn** is the neural network module used for building and training neural networks\n",
        "\n",
        "3. *torch.utils.data* :\n",
        "\n",
        "*   **random_split**: A utility function from PyTorch for splitting datasets into random subsets, commonly used for creating train/validation/test splits.\n",
        "\n",
        "* **DataLoader**: Used for creating batches of data that can be fed into the neural network during training.\n",
        "\n",
        "* **Dataset**: The base class for all datasets in PyTorch. You can create custom datasets by subclassing Dataset and implementing methods like __len__ and __getitem__.\n",
        "\n",
        "4. *kan* : This is the library used to define the Kolgmorov-Arnold Networks\n",
        "\n",
        "5. *cv2* : Importing OpenCV, a popular library for computer vision tasks such as image processing, video capture, and computer vision algorithms.\n",
        "\n",
        "6. *PIL* : Importing the Python Imaging Library (PIL), which provides support for opening, manipulating, and saving many different image file formats.\n",
        "\n",
        "7. *matplotlib* : Importing matplotlib's pyplot module, commonly used for plotting and visualizing data.\n",
        "\n",
        "8. *os* : Standard library for interacting with the operating system, used here for file and directory operations.\n",
        "\n",
        "9. *google.colab* : Importing a module from Google Colab, a cloud-based Python environment, which allows you to mount Google Drive for accessing files stored there.\n",
        "\n",
        "10. *shutil* : shutil is a module in Python's standard library that provides a collection of high-level operations on files and collections of files. It offers functionalities for copying, moving, and removing files and directories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ODbC4xF6Zym4"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import ToPILImage\n",
        "from torchvision.io import read_image\n",
        "from torch.utils.data import random_split, DataLoader, Dataset\n",
        "\n",
        "from torchmetrics.classification import BinaryF1Score\n",
        "from torchmetrics.classification import BinaryAccuracy\n",
        "from torchmetrics.classification import BinaryRecall\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning  import Trainer\n",
        "from pytorch_lightning .loggers import TensorBoardLogger\n",
        "\n",
        "\n",
        "from kan import KAN\n",
        "\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import os\n",
        "import gdown\n",
        "import zipfile\n",
        "import shutil\n",
        "from tqdm.notebook import tqdm\n",
        "import time\n",
        "from google.colab import drive\n",
        "\n",
        "import random\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86RET5RaaNKi"
      },
      "source": [
        "### GLOBAL DEFINITIONS\n",
        "\n",
        "Global variables where there are stored hyperparameters for the training and link, paths for the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YswtWHEPaLP7"
      },
      "outputs": [],
      "source": [
        "global_var = {\n",
        "    # Dataset\n",
        "    'link_download_prefix' : 'https://drive.google.com/uc?export=download&id=YOUR_FILE_ID', # Generic link to download from Drive the file with id \"YOUR_FILE_ID\"\n",
        "    \"link_dataset\": \"https://drive.google.com/file/d/10un_W7teXQy9LOB1uqd0e6VJ46UFNRza/view?usp=drive_link0\", # The ID is \"10un_W7teXQy9LOB1uqd0e6VJ46UFNRza\" of the file\n",
        "    \"path_download_dataset\": \"/content/dataset.zip\", # Path of the runtime directory of Colab with the zipped dataset\n",
        "    \"extract_directory\": \"/content\", # Directory where we want to extract the zipped dataset\n",
        "    \"dataset_name\": \"chest_xray\", # Name of the dataset\n",
        "    \"dataset_name_resized\": \"chest_xray_resized\", # Name of the dataset resized\n",
        "    \"dataset_name_resized_augmented\": \"chest_xray_resized_augmented\", # Name of the dataset resized\n",
        "    \"dataset_name_heatmap\": \"chest_xray_heatmap\",\n",
        "    \"dataset_name_gaussian_he\": \"chest_xray_gaussian_he\",\n",
        "\n",
        "    # Preprocessing parameters\n",
        "    \"resizing_dim\": 200,\n",
        "    \"colormap_type\": cv2.COLORMAP_TWILIGHT,\n",
        "    \"smoothing_value\": 5,\n",
        "\n",
        "    # Network: KAN module\n",
        "    'input_dim': 10,       # input dimension of KAN\n",
        "    'hidden_dim': [40, 4], # hidden layers of KAN\n",
        "    'output_dim': 1,       # output layer, binary classification\n",
        "\n",
        "    # Train\n",
        "    'batch_size': 16,\n",
        "    'epochs': 10,\n",
        "\n",
        "    # logger parameters\n",
        "    \"log_directory\": \"/content/log_plots\",\n",
        "\n",
        "    # hyperparamter search\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4IUB3lR18JwU"
      },
      "outputs": [],
      "source": [
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztmhJrCtndIN"
      },
      "source": [
        "### Color Class\n",
        "This class is used for printing text in different color. In particular each attribute in the Color class corresponds to a specific text color or formatting style."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Q5lxLgy3E-z"
      },
      "outputs": [],
      "source": [
        "class Color:\n",
        "    RED = '\\033[91m'\n",
        "    GREEN = '\\033[92m'\n",
        "    YELLOW = '\\033[93m'\n",
        "    BLUE = '\\033[94m'\n",
        "    MAGENTA = '\\033[95m'\n",
        "    CYAN = '\\033[96m'\n",
        "    WHITE = '\\033[97m'\n",
        "    RESET = '\\033[0m' #RESET: Resets all text formatting and color to default\n",
        "    BOLD = '\\033[1m' #BOLD: Makes the text bold\n",
        "    UNDERLINE = '\\033[4m' #UNDERLINE: Underlines the text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y80hfJRBqsLM"
      },
      "source": [
        "Random Seed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zi41WSYW3jgC"
      },
      "outputs": [],
      "source": [
        "# Set randomness always the same at each run\n",
        "pl.seed_everything(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgQdo_c94IG5"
      },
      "source": [
        "# Dataset Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09Nrq6xD9viW"
      },
      "source": [
        "## Initial setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBFYnapNrZVx"
      },
      "source": [
        "\n",
        "The function, given the link of the dataset in google drive, download it.\n",
        "\n",
        "Args:\n",
        "        \n",
        "1.   **link_dataset** = link to zip files\n",
        "2.   **delete_zip_file** = {True/False}, use to remove the zip file once extracted the content\n",
        "\n",
        "Note: the link my be public or gdown can't download it\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mk4TdLIY5CkF"
      },
      "outputs": [],
      "source": [
        "def download_dataset(link_dataset, delete_zip_file=False):\n",
        "\n",
        "    # Creating string for download\n",
        "\n",
        "    link_download_base =  global_var['link_download_prefix'] # General link for the download from Drive (with \"YOUR_FILE_ID\")\n",
        "    id_dataset = link_dataset.split(\"/view\")[0].split(\"/\")[-1]\n",
        "    dataset_download_link = link_download_base.replace(\"YOUR_FILE_ID\", id_dataset) # Replace the \"YOUR_FILE_ID\" part with the extracted id_dataset\n",
        "\n",
        "    path_download_dataset = global_var['path_download_dataset'] # Path of the zip file that contain the dataset\n",
        "    dataset_name = global_var['dataset_name'] # Dataset's Name\n",
        "    extract_directory = global_var['extract_directory'] # Directory of the GColab Runtime where we want to extract the dataset\n",
        "\n",
        "    # Downloading zip file\n",
        "    try:\n",
        "        if not( os.path.exists(dataset_name) ): # check is the dataset is already downloaded\n",
        "            gdown.download(\n",
        "                url=dataset_download_link, # Link that we extract with the file ID\n",
        "                output=path_download_dataset, # Path of the runtime Colab to save the zip file\n",
        "                quiet=False # Output to check the download\n",
        "            )\n",
        "            print(f\"{Color.GREEN}\\nZip file downloaded{Color.RESET}\")\n",
        "        else:\n",
        "            print(f\"{Color.GREEN}Zip file already downloaded{Color.RESET} \")\n",
        "    except Exception as error:\n",
        "        print(f\"{Color.RED}Download went wrong!\\nerror:{Color.RESET}\")\n",
        "        print(error)\n",
        "\n",
        "\n",
        "    # Unzipping the file\n",
        "    '''\n",
        "    In this part we want to unzip the zip file contain the dataset\n",
        "    '''\n",
        "    try:\n",
        "        if not( os.path.exists(dataset_name) ):\n",
        "\n",
        "          # This open the file zip in path_download_dataset in 'r' = read mode.\n",
        "          # The file is created from the ZipFile class contained in zipfile module\n",
        "          # and the name of the instance will be zipfile\n",
        "\n",
        "            with zipfile.ZipFile(path_download_dataset, 'r') as zip_file:\n",
        "\n",
        "                n_files = len(zip_file.namelist()) # number of files and directories inside the zip\n",
        "\n",
        "                with tqdm(total=n_files, desc='Unzipping files') as pbar: # It's a bar to track the unzip process, we pass the number of total files\n",
        "                    for file_name in zip_file.namelist():\n",
        "                        zip_file.extract(member=file_name, path=extract_directory) # Extract the file_name (iteration over all the files) in the path extract_directory\n",
        "                        pbar.update(1) # increment the progress bar of 1 unit for each extraction\n",
        "\n",
        "            print(f\"{Color.GREEN}Dataset {dataset_name} unzipped{Color.RESET}\")\n",
        "        else:\n",
        "            print(f\"{Color.GREEN}Dataset {dataset_name} already unzipped{Color.RESET} \")\n",
        "    except Exception as error:\n",
        "        print(f\"{Color.RED}Unzip went wrong!\\nerror:{Color.RESET}\")\n",
        "        print(error)\n",
        "\n",
        "    # Removing zip file\n",
        "    '''\n",
        "    In this part we remove the zip file if the flag\n",
        "    delete_zip_file is True (args of the function)\n",
        "    '''\n",
        "    try:\n",
        "        if delete_zip_file == True:\n",
        "            ! rm {path_download_dataset}\n",
        "            print(f\"{Color.GREEN}File {path_download_dataset} removed{Color.RESET}\")\n",
        "    except Exception as error:\n",
        "        print(f\"{Color.RED}Could not remove zip file, pass!\\nerror:{Color.RESET}\")\n",
        "        print(error)\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYJ7AV4CyyF5"
      },
      "source": [
        "Given the dataset, it merge and shuffle it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ASW5hS0Tvg-0"
      },
      "outputs": [],
      "source": [
        "def merge_dataset():\n",
        "\n",
        "    dataset_name = global_var['dataset_name'] # Name of the directory\n",
        "    extract_directory = global_var['extract_directory'] # Directory of destination where we put the shuffled dataset\n",
        "    current_path = os.path.join(extract_directory, dataset_name) # Create the path \"/content/chest_xray\"\n",
        "    origin_dataset = os.path.join(current_path,dataset_name) # Create the path \"/content/chest_xray/chest_xray\"\n",
        "\n",
        "    # Create a list of this type -> ['/content/chest_xray/train', '/content/chest_xray/test', '/content/chest_xray/val']:\n",
        "    sub_directory = [ os.path.join(current_path, s) for s in ['train', 'test', 'val']]\n",
        "    classes = ['PNEUMONIA', 'NORMAL']\n",
        "\n",
        "    try:\n",
        "        for sub_dir in sub_directory:\n",
        "\n",
        "            for c in classes:\n",
        "                sub_dir_c = os.path.join(sub_dir, c) # Create a path of the type '/content/chest_xray/train/PNEUMONIA' and '/content/chest_xray/train/NORMAL' and so on...\n",
        "                counter = 0 # This is used for the name of each image\n",
        "\n",
        "                for img_name in os.listdir(sub_dir_c):\n",
        "                    counter += 1\n",
        "                    # Example of img_name:  IM-0001-0001.jpeg\n",
        "\n",
        "                    # sub_dir.split(\"/\")[-1]: Take the part \"train\", \"test\" or \"val\"\n",
        "                    # c.lower(): Take the name of the class in uppercase and transform in lowercase\n",
        "                    # f\"_{counter:05d}\": put the number counter starting with a zero sequence until 5 numbers sequence\n",
        "                    # current_name.split(\".\")[-1]: put the jpeg word at the end\n",
        "                    # Example of new_name: train_pneumonia_00001.jpeg\n",
        "\n",
        "                    new_name = sub_dir.split(\"/\")[-1] + \"_\" + c.lower() + f\"_{counter:05d}\" + \".\" + img_name.split(\".\")[-1]\n",
        "\n",
        "                    final_path = sub_dir + \"/\" + new_name # Create a path of the type: /content/chest_xray/train/train_pneumonia_00001.jpeg\n",
        "\n",
        "                    # Move the file from '/content/chest_xray/train/PNEUMONIA/IM-0001-0001.jpeg' to '/content/chest_xray/train/train_pneumonia_00001.jpeg'\n",
        "                    # this result in changin the name of the file\n",
        "                    os.rename( os.path.join(sub_dir_c, img_name), final_path )\n",
        "\n",
        "                ! rmdir {sub_dir_c} # Remove the empty directory \"PNEUMONIA\" and \"NORMAL\" after all rename\n",
        "        !rm -rf {origin_dataset} # Remove a useless directory\n",
        "\n",
        "        print(f\"{Color.GREEN}Dataset reordered and labelled{Color.RESET}\")\n",
        "    except Exception as error:\n",
        "        print(f\"Dataset already merged\\n\")\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJNfF_ilr6Ey"
      },
      "source": [
        "This function is used to setup the GPU (if there is)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PH1ysKhGZ1P_"
      },
      "outputs": [],
      "source": [
        "def setup_device():\n",
        "\n",
        "    \"\"\"\n",
        "    Setup device to be used\n",
        "    \"\"\"\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.device(\"cuda:0\")\n",
        "        torch.cuda.set_device(device)\n",
        "    else:\n",
        "        device = torch.device(\"cpu\")\n",
        "\n",
        "    print(f\"Current device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GaUnYpNGxwXj"
      },
      "source": [
        "## Counter Function\n",
        "This function is used in two options:\n",
        "\n",
        "1. **labels** = *True* : in this case the counter will count the number of \"normal\" or \"pneumonia\" images in the \"path_dir\"\n",
        "\n",
        "1. **labels** = *False* : in this case the counter will count simply the number of images in the \"path_dir\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s73kxQ4yIrTV"
      },
      "outputs": [],
      "source": [
        "def counter (path_dir, labels = True):\n",
        "\n",
        "  file_list = os.listdir(path_dir) # Create a list of all files in the dataset directory\n",
        "\n",
        "  if labels == True:\n",
        "\n",
        "    train_count_normal = 0 # Counter of images \"normal\"\n",
        "    train_count_pneumonia = 0 # Counter of images \"pneumonia\"\n",
        "\n",
        "  # Iterate through each file in the directory\n",
        "    for filename in file_list:\n",
        "        if 'normal' in filename:\n",
        "            train_count_normal += 1\n",
        "        elif 'pneumonia' in filename:\n",
        "            train_count_pneumonia += 1\n",
        "\n",
        "    return train_count_normal, train_count_pneumonia\n",
        "\n",
        "  else:\n",
        "\n",
        "    samples_num = len(file_list);\n",
        "    return samples_num"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgiZw8VRqNqf"
      },
      "source": [
        "# Preprocessing operation on dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwqEzOQTzGvx"
      },
      "source": [
        "Given the path for an image, extract a [colormap](https://docs.opencv.org/3.4/d3/d50/group__imgproc__colormap.html) to highlight details\n",
        "\n",
        "Args:\n",
        "\n",
        "1. img_originale: images in tensor format\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fRlQn3_lJgg4"
      },
      "outputs": [],
      "source": [
        "def get_heatmap(img_original):\n",
        "\n",
        "    colormap_type = global_var['colormap_type']\n",
        "\n",
        "    # check if the image is on gpu, if yes bring to cpu for use numpy\n",
        "    if isinstance(img_original, str):\n",
        "        img_original = cv2.imread(img_original, cv2.IMREAD_GRAYSCALE)\n",
        "    else:\n",
        "        if img_original.is_cuda:\n",
        "            img_original = img_original.cpu()\n",
        "\n",
        "        img_original = img_original.detach().numpy()\n",
        "        img_original = np.transpose( img_original, (1,2,0) ) # C, H, W -> H, W, C format\n",
        "        img_original = cv2.cvtColor(img_original, cv2.COLOR_RGB2GRAY) # get gray-scale image\n",
        "\n",
        "    img_normalized = cv2.normalize(img_original, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
        "    img_colored = cv2.applyColorMap(img_normalized, colormap_type)\n",
        "    img_rgb = cv2.cvtColor(img_colored, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    return img_rgb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gYtMMzCCynsW"
      },
      "outputs": [],
      "source": [
        "def get_gaussian(img_original):\n",
        "\n",
        "    img_original = cv2.imread(img_original, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "    smoothing_value = global_var['smoothing_value']\n",
        "    img_smoothed = cv2.GaussianBlur( img_original, ( smoothing_value, smoothing_value), 0 )\n",
        "\n",
        "    return img_smoother"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xiQWiVcD_D58"
      },
      "outputs": [],
      "source": [
        "def get_gaussian_he(img_original):\n",
        "\n",
        "    img_original = cv2.imread(img_original, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "    smoothing_value = global_var['smoothing_value']\n",
        "    img_smoothed = cv2.GaussianBlur( img_original, ( smoothing_value, smoothing_value), 0 )\n",
        "    img_eq = cv2.equalizeHist(img_smoothed )\n",
        "\n",
        "    return img_eq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6eQMDf30quvq"
      },
      "source": [
        "This function will do a \"downsampling\" or \"upsampling\" technique given an image (tensor) in input, depending on the new requested size.\n",
        "\n",
        "Args:\n",
        "\n",
        "1. **mode** : the mode of [interpolation](https://pytorch.org/docs/stable/generated/torch.nn.functional.interpolate.html) like 'nearest', 'bilinear', 'bicubic' ...\n",
        "2. **size** : the new size, like (800x800)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sjRur4spqNYP"
      },
      "outputs": [],
      "source": [
        "def resize_and_save(mode, size):\n",
        "\n",
        "    original_dataset = global_var['dataset_name'] # Name of the original dataset\n",
        "    resized_dataset = global_var['dataset_name_resized'] # Name of the resized dataset\n",
        "\n",
        "    extract_directory = global_var['extract_directory'] # Directory of the GColab Runtime where we want to extract the dataset\n",
        "    original_path = os.path.join(extract_directory, original_dataset) # Create the path \"/content/chest_xray\"\n",
        "    resized_path = os.path.join(extract_directory, resized_dataset) # Create the path \"/content/chest_xray_resized\"\n",
        "\n",
        "    # Create a list of this type -> ['/content/chest_xray/train', '/content/chest_xray/test', '/content/chest_xray/val']:\n",
        "    sub_directory_original = [ os.path.join(original_path, s) for s in ['train', 'test', 'val']]\n",
        "\n",
        "    try:\n",
        "      for sub_dir in sub_directory_original:\n",
        "        category = os.path.basename(sub_dir) # Extract the part 'train', 'test' or 'val'\n",
        "\n",
        "        # Copy from source to destination\n",
        "        source_dir = sub_dir # Source Directory (Ex. '/content/chest_xray/train')\n",
        "        destination_dir = resized_path + '/' + category # Destination Directory ('Ex. /content/chest_xray_resized/train')\n",
        "\n",
        "        if not(os.path.exists(destination_dir)):\n",
        "          # Copy the entire directory\n",
        "          shutil.copytree(source_dir, destination_dir)\n",
        "\n",
        "          # Loop for the resizing and save\n",
        "          img_files = os.listdir(sub_dir)\n",
        "          for img in tqdm(img_files, desc=f\"Resizing {category} images\", unit=\"image\"):\n",
        "\n",
        "            # Take the name and extension of the image\n",
        "            name, extension = os.path.splitext(img)\n",
        "\n",
        "            # Read the image\n",
        "            img_path = sub_dir + '/' + name + extension\n",
        "            img = read_image(img_path) # Tensor Image (Ex. torch.Size([1, 928, 1288]))\n",
        "\n",
        "            # Add a batch dimensione of 1 needed for the nn.functional.interpolate method\n",
        "            img_4d = img.unsqueeze(0) # Tensor Image (Ex. torch.Size([1, 1, 928, 1288]))\n",
        "\n",
        "            # Interpolation method\n",
        "            resized_image = nn.functional.interpolate(img_4d, size=size, mode=mode) # Tensor Image (Ex. torch.Size([1, 1, 800, 800]))\n",
        "\n",
        "            # Remove the batch dimension\n",
        "            resized_image = resized_image.squeeze(0)  # Tensor Image (Ex. torch.Size([1, 800, 800]))\n",
        "\n",
        "            # Permute the dimensions\n",
        "            resized_image = resized_image.permute(1, 2, 0) # Tensor Image (Ex. torch.Size([800, 800, 1]))\n",
        "\n",
        "            # Transform in numpy structure\n",
        "            resized_image = resized_image.numpy() # Numpy Image (Ex. (800, 800, 1))\n",
        "\n",
        "            # Convert to PIL image using torchvision.transforms\n",
        "            to_pil = ToPILImage() # Transformer Numpy -> PIL Image\n",
        "            resized_image_pil = to_pil(resized_image) # PIL Image\n",
        "\n",
        "            # Save the resulting image\n",
        "            path_to_save = destination_dir + '/' + name + extension # Path of the type '/content/chest_xray_resized/train/img_name.jpeg'\n",
        "            resized_image_pil.save(path_to_save) # Function to save the image\n",
        "        else:\n",
        "          print(f\"{Color.GREEN}Data for {category} already resized{Color.RESET} \")\n",
        "\n",
        "    except Exception as error:\n",
        "      print(f\"Problem in resizing\\n\")\n",
        "      print(error)\n",
        "      pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "REAcgrBor5cX"
      },
      "outputs": [],
      "source": [
        "def data_augmentation(transform):\n",
        "\n",
        "    tr = transform # Transform that we use for augment the data of the training set\n",
        "    original_dataset = global_var['dataset_name_resized'] # Name of the original dataset\n",
        "    new_dataset = global_var['dataset_name_resized_augmented'] # Name of the resized dataset\n",
        "\n",
        "    extract_directory = global_var['extract_directory'] # Directory of the GColab Runtime where we want to extract the dataset\n",
        "    original_path = os.path.join(extract_directory, original_dataset) # Create the path \"/content/chest_xray_resized\"\n",
        "    new_path = os.path.join(extract_directory, new_dataset) # Create the path \"/content/chest_xray_resized_augmented\"\n",
        "\n",
        "    # Create a list of this type -> ['/content/chest_xray_resized/train', '/content/chest_xray_resized/test', '/content/chest_xray_resized/val']:\n",
        "    sub_directory_original = [ os.path.join(original_path, s) for s in ['train', 'test', 'val']]\n",
        "    # Create a list of this type -> ['/content/chest_xray_resized_augmented/train', '/content/chest_xray_resized_augmented/test', '/content/chest_xray_resized_augmented/val']:\n",
        "    sub_directory_augmented = [ os.path.join(new_path, s) for s in ['train', 'test', 'val']]\n",
        "\n",
        "    try:\n",
        "      for sub_dir in sub_directory_original:\n",
        "        category = os.path.basename(sub_dir) # Extract the part 'train', 'test' or 'val'\n",
        "\n",
        "        # Copy from source to destination\n",
        "        source_dir = sub_dir # Source Directory ('Ex. /content/chest_xray_resized/train')\n",
        "        destination_dir = new_path + '/' + category # Destination Directory ('Ex. /content/chest_xray_resized_augmented/train')\n",
        "\n",
        "        if not(os.path.exists(destination_dir)):\n",
        "          # Copy the entire directory\n",
        "          shutil.copytree(source_dir, destination_dir)\n",
        "        else:\n",
        "          print(f\"{Color.GREEN}Data for {category} already copied{Color.RESET} \")\n",
        "          return\n",
        "\n",
        "    except Exception as error:\n",
        "      print(f\"Problem in copying\\n\")\n",
        "      print(error)\n",
        "      pass\n",
        "\n",
        "    try:\n",
        "      dir_train = sub_directory_augmented[0]                   # '/content/chest_xray_resized_augmented/train'\n",
        "      img_files_train = os.listdir(dir_train)                  # List of all training images\n",
        "      n_normal, n_pneumonia = counter(dir_train, labels=True)  # Count the number of normal and pneumonia images\n",
        "      difference = n_pneumonia - n_normal                      # Desired value of augmented images\n",
        "      sub_path = os.path.join(dir_train, '/augmented')         # Subdirectory where we save the augmented images\n",
        "      !mkdir {dir_train + '/augmented/'}                       # Create the subdirectory\n",
        "      actual = len(os.listdir(dir_train + '/augmented/'))      # Actual value of augmented images\n",
        "\n",
        "      n = 0                                                    # Counter used to name the augmented images\n",
        "\n",
        "      with tqdm(total=difference, desc='Augmenting data') as pbar:\n",
        "        while  actual < difference:\n",
        "          img = random.choice(img_files_train)           # Pick a random image file from the list\n",
        "          if ('normal' in img):\n",
        "            n += 1\n",
        "            # Take the name and extension of the image\n",
        "            name, extension = os.path.splitext(img)\n",
        "\n",
        "            # Read the image as a Tensor\n",
        "            img = read_image(dir_train + '/' + img)\n",
        "\n",
        "            # Transform the image\n",
        "            new_image = transform(img); # Transformed Tensor Image\n",
        "\n",
        "            # Permute the dimensions\n",
        "            new_image = new_image.permute(1, 2, 0) # Tensor Image (Ex. torch.Size([800, 800, 1]))\n",
        "\n",
        "            # Transform in numpy structure\n",
        "            new_image = new_image.numpy() # Numpy Image (Ex. (800, 800, 1))\n",
        "\n",
        "            # Convert to PIL image using torchvision.transforms\n",
        "            to_pil = ToPILImage() # Transformer Numpy -> PIL Image\n",
        "            new_image_pil = to_pil(new_image) # PIL Image\n",
        "\n",
        "            # Save the resulting image:  Path of the type /content/chest_xray_resized_augmented/train/augmented/train_normal_00001_augmented.jpeg\n",
        "            path_to_save = dir_train + '/augmented/' + name.split('_')[0] + '_' + name.split('_')[1] + f\"_{n:05d}\" + '_augmented' + extension\n",
        "            new_image_pil.save(path_to_save) # Function to save the image\n",
        "            actual += 1     # Update of Actual value of agumented images\n",
        "            pbar.update(1)\n",
        "\n",
        "      for img_name in os.listdir(dir_train + '/augmented/'):\n",
        "        final_path = dir_train + '/' # Create a path of the type: /content/chest_xray_resized_augmented/train/\n",
        "        os.rename(dir_train + '/augmented/' + img_name, final_path + img_name)\n",
        "\n",
        "      !rmdir {dir_train + '/augmented/'} # Remove the empty directory \"PNEUMONIA\" and \"NORMAL\" after all rename\n",
        "\n",
        "    except Exception as error:\n",
        "        print(f\"Problem in augmenting\\n\")\n",
        "        print(error)\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uyLjQfGMbkz7"
      },
      "outputs": [],
      "source": [
        "def preprocess_image(image_global_path, mode):\n",
        "\n",
        "    # based of the selected mode, it does a preprocess on image\n",
        "    # returun None is something went wrong\n",
        "\n",
        "    if mode == 'heatmap':\n",
        "        preprocessed_img = get_heatmap( image_global_path )\n",
        "    elif mode == 'gaussian':\n",
        "        preprocessed_img = get_gaussian( image_global_path )\n",
        "    elif mode == 'gaussian_he':\n",
        "        preprocessed_img = get_gaussian_he( image_global_path )\n",
        "    else:\n",
        "        print(f\"{Color.RED}Preprocessing {mode} is not available{Color.RESET}\")\n",
        "        ! rm -rf {new_dataset_path}\n",
        "\n",
        "        preprocessed_img = None\n",
        "\n",
        "    return preprocessed_img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KCT0M4ovvdhG"
      },
      "outputs": [],
      "source": [
        "def create_preprocessed_dataset( dataset_path, new_dataset_path, mode ):\n",
        "\n",
        "    categories = ['val', 'test', 'train']\n",
        "    mode = mode.lower()\n",
        "\n",
        "    if os.path.exists(new_dataset_path):\n",
        "        print(f\"{Color.GREEN}Dataset {new_dataset_path} images already exists{Color.RESET}\")\n",
        "        return\n",
        "    else:\n",
        "        ! cp -r {dataset_path} {new_dataset_path}   # coping augmented dataset \"/content/chest_xray_{mode}\"\n",
        "\n",
        "    for c in categories:  # iterate on train, test, val\n",
        "\n",
        "        n_files = len( os.listdir( os.path.join(new_dataset_path, c ) ) )   # it is \"/content/chest_xray_{mode}/train,test,val\"\n",
        "\n",
        "        with tqdm(total=n_files, desc='Processing ' + c) as pbar:\n",
        "            for img_path in os.listdir( os.path.join(dataset_path, c) ):   # iterate on train, test, val of heatmap dataset\n",
        "\n",
        "                full_img_path = new_dataset_path + \"/\" + c + \"/\" + img_path # global path to image\n",
        "\n",
        "                preprocessed_img = preprocess_image(full_img_path, mode)    # preprocess the image according the the selected mode\n",
        "\n",
        "                if \"augmented\" in img_path:\n",
        "                    new_name = img_path.replace(\"augmented\", mode)   # renaming the images\n",
        "                else:\n",
        "                    new_name = img_path.replace(\".\", f\"_{mode}.\")\n",
        "\n",
        "                #print(new_name)\n",
        "                new_full_path = full_img_path.replace(img_path, new_name)  # /content/chest_xray_heatmap/train/test_normal_00001_heatmap.jpeg\n",
        "\n",
        "                cv2.imwrite(full_img_path, preprocessed_img) # overwrite the image\n",
        "                os.rename(full_img_path, new_full_path) # renaming the image\n",
        "\n",
        "                pbar.update(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XZvgZPRvK13"
      },
      "source": [
        "# Dataset and Network classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4uFWWdnvMeh"
      },
      "source": [
        "## ChestRayDataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "05zFupZ3yC5K"
      },
      "outputs": [],
      "source": [
        "class ChestRayDataset(Dataset):\n",
        "\n",
        "    def __init__(self,file_path, transform):\n",
        "        self.file_path = file_path\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(os.listdir(self.file_path))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        sample_path = self.file_path + \"/\" + os.listdir(self.file_path)[index]\n",
        "        sample = cv2.imread(sample_path)\n",
        "        sample = cv2.cvtColor(sample, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Output of the class\n",
        "        sample_tensor = self.transform(sample)\n",
        "        label = sample_path.split(\"/\")[-1].split(\"_\")[1]\n",
        "\n",
        "        if label == \"normal\":\n",
        "            label = 0\n",
        "        else:\n",
        "            label = 1\n",
        "\n",
        "        return sample_tensor,label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yOQD6ATspbh"
      },
      "source": [
        "## Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGVBhUimtj90"
      },
      "source": [
        "Reference: [Pytorch Lightning]( https://lightning.ai/docs/pytorch/stable/common/lightning_module.html),\n",
        "\n",
        "Metrics used are\n",
        "\n",
        "*   [BinaryAccuracy](https://lightning.ai/docs/torchmetrics/stable/classification/accuracy.html): to evaluate how much correct predictions the network does\n",
        "*   [BinaryRecall](https://lightning.ai/docs/torchmetrics/stable/classification/recall.html): in medical field it is important to avoid false negative so avoid to classify a radiography with diseased lungs as healty\n",
        "*   [BinaryF1Score](https://lightning.ai/docs/torchmetrics/stable/classification/f1_score.html): used to computed average of precision and recall, used to evaluate overall performance\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXSJ3ljOIZyL"
      },
      "source": [
        "Riferimento per cnn in [Medical classification](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7778711/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DMnIyzqTsy44"
      },
      "outputs": [],
      "source": [
        "class JAM_network(pl.LightningModule):\n",
        "    def __init__(self):\n",
        "        super(JAM_network,self).__init__()\n",
        "\n",
        "\n",
        "    def forward(self, network_input):\n",
        "\n",
        "        return network_input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "idw78DEotObF"
      },
      "outputs": [],
      "source": [
        "class JAM(pl.LightningModule):\n",
        "    def __init__(self, input_dim, out_channels, kernel_size, bias):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.loss_fn = nn.BCELoss()\n",
        "\n",
        "        self.accuracy = BinaryAccuracy()\n",
        "        self.recall = BinaryRecall()\n",
        "        self.f1score = BinaryF1Score()\n",
        "\n",
        "    def forward(self, model_input):\n",
        "        return self.model(model_input)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "\n",
        "        inputs, targets = batch\n",
        "\n",
        "        inputs = inputs.to(device)\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        targets = targets.unsqueeze(dim=1)\n",
        "        targets = targets.type(torch.float32)\n",
        "\n",
        "        predictions = self.model(inputs)\n",
        "\n",
        "        loss = self.loss_fn(predictions, targets)\n",
        "\n",
        "        # Calculating metrics\n",
        "        accuracy_value = self.accuracy(predictions, targets)\n",
        "        recall_value = self.recall(predictions, targets)\n",
        "        f1score_value = self.f1score(predictions, targets)\n",
        "\n",
        "        # link per visualizzare le metriche: https://lightning.ai/docs/pytorch/stable/visualize/logging_basic.html\n",
        "        # plottiamo queste 3 + la loss\n",
        "\n",
        "        self.log_dict({\"acc\":accuracy_value, \"recall\" :recall_value, \"f1score\": f1score_value})\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch):\n",
        "\n",
        "        inputs, targets = batch\n",
        "        targets = targets.unsqueeze(dim=1)\n",
        "        targets = targets.type(torch.float32)\n",
        "\n",
        "        predictions = self.model(inputs)\n",
        "\n",
        "        loss = self.loss_fn(predictions, targets)\n",
        "\n",
        "        # Calculating metrics\n",
        "        accuracy_value = self.accuracy(predictions, targets)\n",
        "        recall_value = self.recall(predictions, targets)\n",
        "        f1score_value = self.f1score(predictions, targets)\n",
        "\n",
        "        self.log('val_loss', loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
        "        self.log('val_accuracy', accuracy_value, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
        "        self.log('val_recall', recall_value, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
        "        self.log('val_f1score', f1score_value, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
        "\n",
        "        return {'loss': loss, 'accuracy': accuracy_value, 'recall': recall_value, 'f1': f1score_value}\n",
        "\n",
        "    def test_step(self, batch):\n",
        "        inputs, targets = batch\n",
        "        targets = targets.unsqueeze(dim=1)\n",
        "        targets = targets.type(torch.float32)\n",
        "\n",
        "        predictions = self.model(inputs)\n",
        "\n",
        "        loss = self.loss_fn(predictions, targets)\n",
        "\n",
        "        # Calculating metrics\n",
        "        accuracy_value = self.accuracy(predictions, targets)\n",
        "        recall_value = self.recall(predictions, targets)\n",
        "        f1score_value = self.f1score(predictions, targets)\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.SGD(self.model.parameters(), lr=0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HmvdEe24KbO"
      },
      "source": [
        "# Dataset informations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owMu7Tiqzbnc"
      },
      "source": [
        "## Download and merge of the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hLaY4F33bKCc"
      },
      "outputs": [],
      "source": [
        "download_dataset(\n",
        "     link_dataset=global_var['link_dataset'],\n",
        "     delete_zip_file=False\n",
        " )\n",
        "\n",
        "merge_dataset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OVeAMibJverg"
      },
      "outputs": [],
      "source": [
        "size = global_var[\"resizing_dim\"] # Choose the size\n",
        "\n",
        "resize_and_save(\n",
        "    mode='bilinear',    # Choose the mode of interpolation ('nearest', 'bilinear', 'bicubic', ...)\n",
        "    size=(size, size)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lg6MUQReBBLp"
      },
      "outputs": [],
      "source": [
        "tr_augmentation = transforms.Compose([\n",
        "     #transforms.RandomHorizontalFlip(0.8),\n",
        "     #transforms.RandomVerticalFlip(0.2),\n",
        "     transforms.ColorJitter(brightness=0.4),\n",
        "     #transforms.RandomRotation(degrees=(90,90)),\n",
        "])\n",
        "\n",
        "data_augmentation(tr_augmentation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8tUV8CLcv9jR"
      },
      "outputs": [],
      "source": [
        "dataset_directory = global_var[\"extract_directory\"] + \"/\" + global_var[\"dataset_name_resized_augmented\"]\n",
        "heatmap_directory = global_var[\"extract_directory\"] + \"/\" + global_var[\"dataset_name_heatmap\"]\n",
        "\n",
        "create_preprocessed_dataset(\n",
        "    dataset_path = dataset_directory,\n",
        "    new_dataset_path = heatmap_directory,\n",
        "    mode = 'heatmap'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fd1IvlGFAm-F"
      },
      "outputs": [],
      "source": [
        "dataset_directory = global_var[\"extract_directory\"] + \"/\" + global_var[\"dataset_name_resized_augmented\"]\n",
        "gaussian_he_directory = global_var[\"extract_directory\"] + \"/\" + global_var[\"dataset_name_gaussian_he\"]\n",
        "\n",
        "create_preprocessed_dataset(\n",
        "    dataset_path = dataset_directory,\n",
        "    new_dataset_path = gaussian_he_directory,\n",
        "    mode = 'gaussian_he'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_sFf8oHnnS8s"
      },
      "source": [
        "## Dataset exploration, augmentation and visualization\n",
        "\n",
        "In this part we plot an:\n",
        "\n",
        "1. *Histogram Distribution* : to see if the dataset is balanced or unbalanced\n",
        "\n",
        "2. *Pie Chart* : to visualize the sizes of training, test and validation sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "YVgIgejynSRf"
      },
      "outputs": [],
      "source": [
        "# @title Initial statistic of dataset\n",
        "# Definition of the paths for training, test and validation set\n",
        "path_training_set = \"/content/chest_xray/train\"\n",
        "path_test_set = \"/content/chest_xray/test\"\n",
        "path_val_set = \"/content/chest_xray/val\"\n",
        "\n",
        "# Counting the Normal and Pneumonia images in the training set\n",
        "train_norm, train_pneu = counter(path_training_set, labels = True); # Count the number of normal images and pneumonia images\n",
        "\n",
        "train_num = counter(path_training_set, labels = False);\n",
        "test_num = counter(path_test_set, labels = False);\n",
        "val_num = counter(path_val_set, labels = False);\n",
        "\n",
        "# Histogram Data\n",
        "classes_hist = ['Normal', 'Pneumonia']\n",
        "counts_hist = [train_norm, train_pneu]\n",
        "colors_hist = ['aquamarine', 'blue']\n",
        "\n",
        "# Pie Chart Data\n",
        "classes_pie = ['Train', 'Test', 'Validation']\n",
        "counts_pie = [train_num, test_num, val_num]\n",
        "colors_pie = ['gold', 'lightcoral', 'lightskyblue']\n",
        "\n",
        "# Create a figure with specific dimensions and subplots\n",
        "fig, ax = plt.subplots(1, 2, figsize=(8, 4))  # Adjust figsize as needed\n",
        "\n",
        "# Plotting the histogram\n",
        "ax[0].bar(classes_hist, counts_hist, color=colors_hist)\n",
        "ax[0].set_title('Histogram distribution')\n",
        "ax[0].set_xlabel('Class')\n",
        "ax[0].set_ylabel('Count')\n",
        "\n",
        "# Plotting the pie chart\n",
        "ax[1].pie(counts_pie, labels=classes_pie, colors=colors_pie, autopct='%1.1f%%', startangle=90)\n",
        "ax[1].set_title('Pie Chart of training, test and validation sizes')\n",
        "\n",
        "plt.tight_layout()  # This adjusts subplot params so that the subplots fit into the figure area.\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nThe number of training images:\", train_num)\n",
        "print(\"The number of test images:\", test_num)\n",
        "print(\"The number of validation images:\", val_num)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "mc37Ky-RrrVB"
      },
      "outputs": [],
      "source": [
        "# @title Visualization of two random images\n",
        "# Define a transform element to resize the images as square images and transform in Tensor\n",
        "tr_1 = transforms.ToTensor()                # Convert the image to a tensor\n",
        "\n",
        "# Definition of the paths for training, test and validation set\n",
        "path_training_set = \"/content/chest_xray/train\"\n",
        "path_test_set = \"/content/chest_xray/test\"\n",
        "path_val_set = \"/content/chest_xray/val\"\n",
        "\n",
        "# Instance of classes for the Dataset\n",
        "train_dataset = ChestRayDataset(path_training_set, tr_1) # Create an instance of training class\n",
        "test_dataset = ChestRayDataset(path_test_set, tr_1) # Create an instance of test class\n",
        "val_dataset = ChestRayDataset(path_val_set, tr_1) # Create an instance of validation class\n",
        "\n",
        "# Generate two random numbers to select two random images from training set\n",
        "random_num_1 = np.random.randint(1, len(train_dataset))\n",
        "random_num_2 = np.random.randint(1, len(train_dataset))\n",
        "\n",
        "# Take an image + label randomly\n",
        "img_1, label_1 = train_dataset[random_num_1]\n",
        "img_2, label_2 = train_dataset[random_num_2]\n",
        "\n",
        "# Display the image using matplotlib library\n",
        "plt.figure( figsize=(6,3))\n",
        "plt.subplot(1,2,1)\n",
        "plt.imshow(img_1.permute(1, 2, 0))\n",
        "plt.axis('on')\n",
        "plt.text(0, 0, label_1, fontsize=12, color='black', backgroundcolor='white')\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.imshow(img_2.permute(1, 2, 0))\n",
        "plt.axis('on')\n",
        "plt.text(0, 0, label_2, fontsize=12, color='black', backgroundcolor='white')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "print (\"The shape of the first image is: \", img_1.permute(1, 2, 0).shape)\n",
        "print (\"The shape of the second image is: \", img_2.permute(1, 2, 0).shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "yrfs0O-G5puA"
      },
      "outputs": [],
      "source": [
        "# @title Squarring\n",
        "\n",
        "# Define a transform element to resize the images as square images and transform in Tensor\n",
        "tr_1 = transforms.ToTensor()                 # Convert the image to a tensor\n",
        "\n",
        "# Definition of the paths for training, test and validation set\n",
        "path_training_set_resized = \"/content/chest_xray_resized/train\"\n",
        "path_test_set_resized = \"/content/chest_xray_resized/test\"\n",
        "path_val_set_resized = \"/content/chest_xra_resized/val\"\n",
        "\n",
        "# Instance of classes for the Dataset\n",
        "train_dataset_resized = ChestRayDataset(path_training_set_resized, tr_1) # Create an instance of training class\n",
        "test_dataset_resized = ChestRayDataset(path_test_set_resized, tr_1) # Create an instance of test class\n",
        "val_dataset_resized = ChestRayDataset(path_val_set_resized, tr_1) # Create an instance of validation class\n",
        "\n",
        "# Take an image + label randomly\n",
        "img_1, label_1 = train_dataset_resized[random_num_1]\n",
        "img_2, label_2 = train_dataset_resized[random_num_2]\n",
        "\n",
        "# Display the image using matplotlib library\n",
        "plt.figure( figsize=(6,3))\n",
        "plt.subplot(1,2,1)\n",
        "plt.imshow(img_1.permute(1, 2, 0))\n",
        "plt.axis('on')\n",
        "plt.text(0, 0, label_1, fontsize=12, color='black', backgroundcolor='white')\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.imshow(img_2.permute(1, 2, 0))\n",
        "plt.axis('on')\n",
        "plt.text(0, 0, label_2, fontsize=12, color='black', backgroundcolor='white')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "print (\"The shape of the first image is: \", img_1.permute(1, 2, 0).shape)\n",
        "print (\"The shape of the second image is: \", img_2.permute(1, 2, 0).shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "JNNfT9nMIZCR"
      },
      "outputs": [],
      "source": [
        "# @title Statistics after data augmentation\n",
        "# Definition of the paths for training, test and validation set\n",
        "path_training_set_resized_augmented = \"/content/chest_xray_resized_augmented/train\"\n",
        "\n",
        "# Counting the Normal and Pneumonia images in the training set\n",
        "train_norm, train_pneu = counter(path_training_set_resized_augmented, labels = True); # Count the number of normal images and pneumonia images\n",
        "\n",
        "# Histogram Data\n",
        "classes_hist = ['Normal', 'Pneumonia']\n",
        "counts_hist = [train_norm, train_pneu]\n",
        "colors_hist = ['aquamarine', 'blue']\n",
        "\n",
        "\n",
        "plt.figure(figsize=(5, 3))\n",
        "plt.bar(classes_hist, counts_hist, color=colors_hist)\n",
        "plt.title('Histogram distribution')\n",
        "plt.xlabel('Class')\n",
        "plt.ylabel('Count')\n",
        "plt.show()\n",
        "\n",
        "print(\"The number of normal images:\", train_norm)\n",
        "print(\"The number of pneumonia images:\", train_pneu)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "C15kRm31B3aX"
      },
      "outputs": [],
      "source": [
        "# @title Visualization of two sample is augmented dataset\n",
        "# Instance of classes for the Dataset\n",
        "train_dataset_resized_augmented = ChestRayDataset(path_training_set_resized_augmented, tr_1) # Create an instance of training class\n",
        "\n",
        "# Generate two random numbers to select two random images from training set\n",
        "random_num_1 = np.random.randint(1, len(train_dataset_resized_augmented))\n",
        "random_num_2 = np.random.randint(1, len(train_dataset_resized_augmented))\n",
        "\n",
        "# Take an image + label randomly\n",
        "img_1, label_1 = train_dataset_resized_augmented[random_num_1]\n",
        "img_2, label_2 = train_dataset_resized_augmented[random_num_2]\n",
        "\n",
        "# Display the image using matplotlib library\n",
        "plt.figure(figsize=(6,3))\n",
        "plt.subplot(1,2,1)\n",
        "plt.imshow(img_1.permute(1, 2, 0))\n",
        "plt.axis('on')\n",
        "plt.text(0, 0, label_1, fontsize=12, color='black', backgroundcolor='white')\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.imshow(img_2.permute(1, 2, 0))\n",
        "plt.axis('on')\n",
        "plt.text(0, 0, label_2, fontsize=12, color='black', backgroundcolor='white')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AAvCKZVfbV4A"
      },
      "outputs": [],
      "source": [
        "# @title Heatmap visualization - normal\n",
        "path_train_heatmap = global_var[\"extract_directory\"] + \"/\" + global_var[\"dataset_name_heatmap\"] + \"/train\"\n",
        "train_dataset_heatmap = ChestRayDataset(path_train_heatmap, tr_1)\n",
        "\n",
        "normal_img, label_1  = train_dataset_heatmap[110] #\"chest_xray/test/train_normal_00001.jpeg\"\n",
        "print(label_1)\n",
        "print(normal_img.shape)\n",
        "\n",
        "plt.figure( figsize=(6,3))\n",
        "plt.subplot(1,2,1)\n",
        "plt.imshow( np.transpose(normal_img, (1,2,0)), cmap='gray')\n",
        "plt.axis('off')\n",
        "\n",
        "#plt.subplot(1,2,2)\n",
        "#plt.imshow(processed_img)\n",
        "plt.axis('off')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "bqOv_AceeXmq"
      },
      "outputs": [],
      "source": [
        "# @title Heatmap visualization - pneumonia\n",
        "normal_img, label_2  = train_dataset_heatmap[100]\n",
        "print(label_2)\n",
        "print(normal_img.shape)\n",
        "\n",
        "plt.figure( figsize=(6,3))\n",
        "plt.subplot(1,2,1)\n",
        "plt.imshow(np.transpose(normal_img, (1,2,0)), cmap='gray')\n",
        "plt.axis('off')\n",
        "\n",
        "#plt.subplot(1,2,2)\n",
        "#plt.imshow(np.transpose(normal_img, (1,2,0)))\n",
        "plt.axis('off')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tru1XMrbMq6Q"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPug6dfQhrfK"
      },
      "source": [
        "Training loop from [documentation](https://pytorch.org/tutorials/beginner/introyt/trainingyt.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KlmSrxH8hIuV"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(model, epoch_index, training_loader):\n",
        "    running_loss = 0.\n",
        "    last_loss = 0.\n",
        "\n",
        "    # Here, we use enumerate(training_loader) instead of\n",
        "    # iter(training_loader) so that we can track the batch\n",
        "    # index and do some intra-epoch reporting\n",
        "\n",
        "    n_batch = len(training_loader)\n",
        "    batch_size = global_var['batch_size']\n",
        "    n_files = n_batch*batch_size\n",
        "\n",
        "    with tqdm(total=n_files, desc=f'Epoch {epoch_index} ') as pbar:\n",
        "        for i, data in enumerate(training_loader):\n",
        "\n",
        "            # Every data instance is an input + label pair\n",
        "            inputs, labels = data\n",
        "\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            labels = labels.unsqueeze(dim=1)\n",
        "            labels = labels.type(torch.float32)\n",
        "\n",
        "            # Zero your gradients for every batch!\n",
        "            model.optimizer.zero_grad()\n",
        "\n",
        "            # Make predictions for this batch\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Compute the loss and its gradients\n",
        "            loss = model.loss_fn(outputs, labels)\n",
        "            loss.backward()\n",
        "\n",
        "            # Adjust learning weights\n",
        "            model.optimizer.step()\n",
        "\n",
        "            # Gather data and report\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            if i % batch_size == 0:\n",
        "                last_loss = running_loss / 1000 # loss per batch\n",
        "                print(f'\\r\\tbatch {i+1} loss: {last_loss}', end='')\n",
        "                running_loss = 0.\n",
        "\n",
        "            pbar.update(labels.shape[0])\n",
        "\n",
        "            #if(i>21):\n",
        "            #    break\n",
        "\n",
        "    return last_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hVTkjpNugwlg"
      },
      "outputs": [],
      "source": [
        "def training(model, epochs, training_loader, validation_loader):\n",
        "\n",
        "    best_vloss = 1000000\n",
        "    epoch_number = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print('EPOCH {}:'.format(epoch_number + 1))\n",
        "\n",
        "        # Make sure gradient tracking is on, and do a pass over the data\n",
        "        #model.train(True)\n",
        "        avg_loss = train_one_epoch(model, epoch_number, training_loader)\n",
        "\n",
        "        running_vloss = 0.0\n",
        "\n",
        "        # Disable gradient computation and reduce memory consumption.\n",
        "        with torch.no_grad():\n",
        "            for i, vdata in enumerate(validation_loader):\n",
        "                vinputs, vlabels = vdata\n",
        "\n",
        "                vlabels = vlabels.unsqueeze(dim=1)\n",
        "                vlabels = vlabels.type(torch.float32)\n",
        "\n",
        "                voutputs = model(vinputs)\n",
        "                vloss = model.loss_fn(voutputs, vlabels)\n",
        "                running_vloss += vloss\n",
        "\n",
        "        avg_vloss = running_vloss / ( i + 1 )\n",
        "        print(f\"\\n\\tLOSS -> train: {avg_loss}, valid: {avg_vloss}\")\n",
        "\n",
        "        if avg_vloss < best_vloss:\n",
        "            best_vloss = avg_loss\n",
        "            model_path = f'/content/model_{epoch_number}'\n",
        "            torch.save(model.state_dict(), model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECaFyy74EbAe"
      },
      "source": [
        "## Preprocessing: heatmap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "fws7DmaXPg2N",
        "outputId": "462fc67b-a3f3-474c-f4b3-047ef041b503"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'cpu'"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLgtIpyUFmlE"
      },
      "source": [
        "### First step: creation of dataset instance and dataloaders for each step of a training process regarding to a neural network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Szyt1UuERDX"
      },
      "outputs": [],
      "source": [
        "tf_to_tensor = transforms.ToTensor()\n",
        "\n",
        "heatmap_train_path = global_var['extract_directory'] + \"/\" + global_var['dataset_name_heatmap'] + '/train'\n",
        "heatmap_test_path = global_var['extract_directory'] + \"/\" + global_var['dataset_name_heatmap'] + '/test'\n",
        "heatmap_val_path = global_var['extract_directory'] + \"/\" + global_var['dataset_name_heatmap'] + '/val'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "36ZkX1rtGCQ8"
      },
      "outputs": [],
      "source": [
        "heatmap_train_dataset = ChestRayDataset(\n",
        "    file_path= heatmap_train_path,\n",
        "    transform=tf_to_tensor\n",
        ")\n",
        "\n",
        "heatmap_test_dataset = ChestRayDataset(\n",
        "    file_path= heatmap_test_path,\n",
        "    transform=tf_to_tensor\n",
        ")\n",
        "\n",
        "heatmap_val_dataset = ChestRayDataset(\n",
        "    file_path= heatmap_val_path,\n",
        "    transform=tf_to_tensor\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6eFyZC_GGESa"
      },
      "outputs": [],
      "source": [
        "batch_size = global_var[\"batch_size\"]\n",
        "\n",
        "heatmap_train_dataloader = DataLoader(\n",
        "    heatmap_train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True\n",
        "    )\n",
        "\n",
        "heatmap_test_dataloader = DataLoader(\n",
        "    heatmap_test_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True\n",
        "    )\n",
        "\n",
        "heatmap_val_dataloader = DataLoader(\n",
        "    heatmap_val_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sy0c3PlGGCj3"
      },
      "source": [
        "### Second step: instanting logger, network and train it"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWaxfi-8G76f"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6R-ogK6aDVLU"
      },
      "outputs": [],
      "source": [
        "heatmap_log_directory = global_var[\"log_directory\"]\n",
        "\n",
        "if not os.path.exists(heatmap_log_directory):\n",
        "    !mkdir {heatmap_log_directory}\n",
        "\n",
        "logger = TensorBoardLogger(\n",
        "    save_dir=heatmap_log_directory,\n",
        "    name='heatmap_lightning_logs'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3SxK5R3LL7oc",
        "outputId": "17ae0ac4-ff0a-4f2b-ee98-b8db4388f13a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.4399440288543701\n"
          ]
        }
      ],
      "source": [
        "heatmap_jam_network = JAM(\n",
        "    input_dim=3,\n",
        "    out_channels=32,\n",
        "    kernel_size=3,\n",
        "    bias=True\n",
        ")\n",
        "\n",
        "heatmap_jam_network.to(device)\n",
        "\n",
        "sample = heatmap_train_dataset[0][0].unsqueeze(0).to(device)\n",
        "label = heatmap_train_dataset[0][1]\n",
        "\n",
        "t_now = time.time()\n",
        "heatmap_jam_network(sample)\n",
        "print(time.time() - t_now)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520,
          "referenced_widgets": [
            "c64cce89cef945618dcbab2553fcf6dd",
            "5f725faf652549659f1e45f0dbfd76f4",
            "f8ec2e90eb73424ea730f829b7507638",
            "5d0aba6dcdc4415b90179be45ebfd08b",
            "a007def2e13349218c26e6fc50165d43",
            "fc5f9ce0b8f34aa2905408685d3f16e0",
            "5aa42baf2a854616a329cc612a67196d",
            "5871334556d14b19ad5900fe82e12ca7",
            "1b5acedd207b4dc5a60778823012a18d",
            "5700933e96004ca1962cdf45051ea6b0",
            "7b7d9aecaa4644b6afc976c1c0318bae"
          ]
        },
        "id": "w0xntfsZ-t0O",
        "outputId": "77d51638-0c5d-4fb4-a0ca-b36308c9fba6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: False, used: False\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
            "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py:653: Checkpoint directory /content/log_plots/heatmap_lightning_logs/version_1/checkpoints exists and is not empty.\n",
            "INFO:pytorch_lightning.callbacks.model_summary:\n",
            "  | Name     | Type           | Params\n",
            "--------------------------------------------\n",
            "0 | model    | JAM_network    | 62.6 K\n",
            "1 | loss_fnc | BCELoss        | 0     \n",
            "2 | accuracy | BinaryAccuracy | 0     \n",
            "3 | recall   | BinaryRecall   | 0     \n",
            "4 | f1score  | BinaryF1Score  | 0     \n",
            "--------------------------------------------\n",
            "50.2 K    Trainable params\n",
            "12.5 K    Non-trainable params\n",
            "62.6 K    Total params\n",
            "0.251     Total estimated model params size (MB)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c64cce89cef945618dcbab2553fcf6dd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception ignored in: <function WeakSet.__init__.<locals>._remove at 0x7bf1e7107be0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/_weakrefset.py\", line 39, in _remove\n",
            "    def _remove(item, selfref=ref(self)):\n",
            "KeyboardInterrupt: \n",
            "description:   0%|                                                          | 0/100 [33:40<?, ?it/s]\n",
            "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=1` reached.\n"
          ]
        }
      ],
      "source": [
        "heatmap_trainer = pl.Trainer(\n",
        "    logger=logger,\n",
        "    accelerator=device.split(\":\")[0],\n",
        "    max_epochs=1\n",
        "    )\n",
        "\n",
        "\n",
        "heatmap_trainer.fit(\n",
        "    model=heatmap_jam_network,\n",
        "    train_dataloaders=heatmap_train_dataloader,\n",
        "    #val_dataloaders=heatmap_val_dataloader\n",
        "    )\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "huyIYb5qQnuF"
      },
      "source": [
        "## Preprocessing: Gaussian blur + histogram equalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "CDo2KvqUQnuq",
        "outputId": "097f4ab4-92f8-4642-be6e-5dca87054891"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'cpu'"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oP1H6PljQnur"
      },
      "source": [
        "### First step: creation of dataset instance and dataloaders for each step of a training process regarding to a neural network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "vd4I6hDaQnus"
      },
      "outputs": [],
      "source": [
        "tf_to_tensor = transforms.ToTensor()\n",
        "\n",
        "gaussian_he_train_path = global_var['extract_directory'] + \"/\" + global_var['dataset_name_gaussian_he'] + '/train'\n",
        "gaussian_he_test_path = global_var['extract_directory'] + \"/\" + global_var['dataset_name_gaussian_he'] + '/test'\n",
        "gaussian_he_val_path = global_var['extract_directory'] + \"/\" + global_var['dataset_name_gaussian_he'] + '/val'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "HoeTggQXQnut"
      },
      "outputs": [],
      "source": [
        "gaussian_he_train_dataset = ChestRayDataset(\n",
        "    file_path= gaussian_he_train_path,\n",
        "    transform=tf_to_tensor\n",
        ")\n",
        "\n",
        "gaussian_he_test_dataset = ChestRayDataset(\n",
        "    file_path= gaussian_he_test_path,\n",
        "    transform=tf_to_tensor\n",
        ")\n",
        "\n",
        "gaussian_he_val_dataset = ChestRayDataset(\n",
        "    file_path= gaussian_he_val_path,\n",
        "    transform=tf_to_tensor\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "9dH48GhhQnut"
      },
      "outputs": [],
      "source": [
        "batch_size = global_var[\"batch_size\"]\n",
        "\n",
        "gaussian_he_train_dataloader = DataLoader(\n",
        "    gaussian_he_train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True\n",
        "    )\n",
        "\n",
        "gaussian_he_test_dataloader = DataLoader(\n",
        "    gaussian_he_test_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True\n",
        "    )\n",
        "\n",
        "gaussian_he_val_dataloader = DataLoader(\n",
        "    gaussian_he_val_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nj1idUSdQnuu"
      },
      "source": [
        "### Second step: instanting logger, network and train it"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4S_vSzSQQnuv"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "7yB8AjkaQnuw"
      },
      "outputs": [],
      "source": [
        "gaussian_he_log_directory = global_var[\"log_directory\"]\n",
        "\n",
        "if not os.path.exists(gaussian_he_log_directory):\n",
        "    !mkdir {gaussian_he_log_directory}\n",
        "\n",
        "logger_gaussian_he = TensorBoardLogger(\n",
        "    save_dir=gaussian_he_log_directory,\n",
        "    name='gaussian_he_lightning_logs'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "D1l5BtsmQnuw",
        "outputId": "b61ad6dc-b383-4262-f091-43f6df8aa508"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/kan/KAN.py:327: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at ../aten/src/ATen/native/ReduceOps.cpp:1807.)\n",
            "  self.acts_scale_std.append(torch.std(postacts, dim=0))\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "tensor([[0.4795]], grad_fn=<SigmoidBackward0>)"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gaussian_he_jam_network = JAM(\n",
        "    input_dim=3,\n",
        "    out_channels=32,\n",
        "    kernel_size=3,\n",
        "    bias=True\n",
        ")\n",
        "\n",
        "gaussian_he_jam_network.to(device)\n",
        "\n",
        "sample = gaussian_he_train_dataset[0][0].unsqueeze(0).to(device)\n",
        "label = gaussian_he_train_dataset[0][1]\n",
        "\n",
        "gaussian_he_jam_network(sample)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "referenced_widgets": [
            "50cef6c49a0c4f59841ee7bf2392a8b9"
          ]
        },
        "id": "JdSbLHCEQnux",
        "outputId": "a02d79a4-ab1e-432e-c15e-ff2ec01678ae"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: False, used: False\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
            "WARNING:pytorch_lightning.loggers.tensorboard:Missing logger folder: /content/log_plots/gaussian_he_lightning_logs\n",
            "INFO:pytorch_lightning.callbacks.model_summary:\n",
            "  | Name     | Type           | Params\n",
            "--------------------------------------------\n",
            "0 | model    | JAM_network    | 192 K \n",
            "1 | loss_fnc | BCELoss        | 0     \n",
            "2 | accuracy | BinaryAccuracy | 0     \n",
            "3 | recall   | BinaryRecall   | 0     \n",
            "4 | f1score  | BinaryF1Score  | 0     \n",
            "--------------------------------------------\n",
            "142 K     Trainable params\n",
            "49.5 K    Non-trainable params\n",
            "192 K     Total params\n",
            "0.769     Total estimated model params size (MB)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "50cef6c49a0c4f59841ee7bf2392a8b9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "gaussian_he_trainer = pl.Trainer(\n",
        "    logger=logger_gaussian_he,\n",
        "    accelerator=device.split(\":\")[0],\n",
        "    max_epochs=10\n",
        "    )\n",
        "\n",
        "gaussian_he_trainer.fit(\n",
        "    model=gaussian_he_jam_network,\n",
        "    train_dataloaders=gaussian_he_train_dataloader,\n",
        "    #val_dataloaders=heatmap_val_dataloader\n",
        "    )\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "uZQ3yrZuZsXx",
        "yt-OTzpxBk3q",
        "ecVBSRTQBmdb",
        "ztmhJrCtndIN",
        "GgQdo_c94IG5",
        "09Nrq6xD9viW",
        "vgiZw8VRqNqf",
        "j4uFWWdnvMeh",
        "owMu7Tiqzbnc",
        "_sFf8oHnnS8s",
        "ECaFyy74EbAe",
        "cLgtIpyUFmlE"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "06a52677e11c4f16964864bf5562a7c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1cff428f33294fc88021733b0b1b3c29",
            "placeholder": "​",
            "style": "IPY_MODEL_b17eacafd03e4ba18a69441d013692ef",
            "value": " 0/6416 [00:00&lt;?, ?it/s]"
          }
        },
        "168549aca122477fbadea4116f78650c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "191161f3cfed45068fa7d48c795b12d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1b5acedd207b4dc5a60778823012a18d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1cff428f33294fc88021733b0b1b3c29": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2092d0ec2ebf4747bcba92b7ef6b6173": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "40f90ac303524535ab5a4cfe19f892c0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "52902dadb801438dac97cc4180a04023": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5700933e96004ca1962cdf45051ea6b0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5871334556d14b19ad5900fe82e12ca7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5aa42baf2a854616a329cc612a67196d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5b6d59ae848d484aa3125ebd3659b69e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_52902dadb801438dac97cc4180a04023",
            "placeholder": "​",
            "style": "IPY_MODEL_d1085352cf6e41d8ae8c3721c0d8c849",
            "value": " 368/6416 [00:41&lt;11:25,  8.82it/s]"
          }
        },
        "5d0aba6dcdc4415b90179be45ebfd08b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5700933e96004ca1962cdf45051ea6b0",
            "placeholder": "​",
            "style": "IPY_MODEL_7b7d9aecaa4644b6afc976c1c0318bae",
            "value": " 401/401 [13:31&lt;00:00,  0.49it/s, v_num=1]"
          }
        },
        "5f725faf652549659f1e45f0dbfd76f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fc5f9ce0b8f34aa2905408685d3f16e0",
            "placeholder": "​",
            "style": "IPY_MODEL_5aa42baf2a854616a329cc612a67196d",
            "value": "Epoch 0: 100%"
          }
        },
        "629f36e67832481a8333df1db73914e3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "63650f41bb67430c994affd03e0fc7f9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "715be067b84e4100882683842a0a8b1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_63650f41bb67430c994affd03e0fc7f9",
            "placeholder": "​",
            "style": "IPY_MODEL_191161f3cfed45068fa7d48c795b12d7",
            "value": "Epoch 0 :   6%"
          }
        },
        "75483950c5e94171bf7d760f64e403a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_40f90ac303524535ab5a4cfe19f892c0",
            "placeholder": "​",
            "style": "IPY_MODEL_ae48a92751c74c2890351e3052baa9c7",
            "value": "Epoch 0 :   0%"
          }
        },
        "766240c7ce334fb59be92e6cc742466d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a966103f10ed41008bcb5f619bfb9c81",
            "max": 6416,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_be1756e9c0594505a3a54c90034255d1",
            "value": 368
          }
        },
        "7b7d9aecaa4644b6afc976c1c0318bae": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a007def2e13349218c26e6fc50165d43": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "a966103f10ed41008bcb5f619bfb9c81": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ae48a92751c74c2890351e3052baa9c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b17eacafd03e4ba18a69441d013692ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bb5216b854894b658042ef03681edf1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c23e30b5e11b4a688024f6e6f8d779e9",
            "max": 6416,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2092d0ec2ebf4747bcba92b7ef6b6173",
            "value": 16
          }
        },
        "be1756e9c0594505a3a54c90034255d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c23e30b5e11b4a688024f6e6f8d779e9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c64cce89cef945618dcbab2553fcf6dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5f725faf652549659f1e45f0dbfd76f4",
              "IPY_MODEL_f8ec2e90eb73424ea730f829b7507638",
              "IPY_MODEL_5d0aba6dcdc4415b90179be45ebfd08b"
            ],
            "layout": "IPY_MODEL_a007def2e13349218c26e6fc50165d43"
          }
        },
        "d1085352cf6e41d8ae8c3721c0d8c849": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dbc2273768b3417b8cccba10c3ad63bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_715be067b84e4100882683842a0a8b1c",
              "IPY_MODEL_766240c7ce334fb59be92e6cc742466d",
              "IPY_MODEL_5b6d59ae848d484aa3125ebd3659b69e"
            ],
            "layout": "IPY_MODEL_629f36e67832481a8333df1db73914e3"
          }
        },
        "dd0f88b77d1c4b6f8f1dc8ff15f54348": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_75483950c5e94171bf7d760f64e403a9",
              "IPY_MODEL_bb5216b854894b658042ef03681edf1f",
              "IPY_MODEL_06a52677e11c4f16964864bf5562a7c6"
            ],
            "layout": "IPY_MODEL_168549aca122477fbadea4116f78650c"
          }
        },
        "f8ec2e90eb73424ea730f829b7507638": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5871334556d14b19ad5900fe82e12ca7",
            "max": 401,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1b5acedd207b4dc5a60778823012a18d",
            "value": 401
          }
        },
        "fc5f9ce0b8f34aa2905408685d3f16e0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
